{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from transformer_attention import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 1\n",
    "lr = 1e-5\n",
    "warmup_steps = 5\n",
    "decay_factor = 0.65\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 200\n",
    "log_interval = 100  # Print log every `log_interval` batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 256\n",
    "ff_size = 1024\n",
    "n_layers = 3\n",
    "n_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, `batchify` arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "# NOTE: modified to emit data as size [batch_size, seq_len]\n",
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into batches\n",
    "    nbatches = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data.narrow(0, 0, nbatches * batch_size)\n",
    "    # Evenly divide the data across the batches\n",
    "    data = data.view(batch_size, -1).contiguous()\n",
    "    return data\n",
    "\n",
    "# `get_batch` subdivides the source data into chunks of the specified length.\n",
    "# E.g., using the example for the `batchify` function above and a length of 2,\n",
    "# we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the `batchify` function. The chunks are along dimension 0, corresponding\n",
    "# to the `seq_len` dimension in the LSTM.\n",
    "def get_batch(src, i, seq_len, evaluate = False):\n",
    "    seq_len = min(seq_len, src.size(1) - 2 - i)\n",
    "    data = Variable(src[:, i+1 : i+seq_len+1].contiguous(), volatile = evaluate)\n",
    "    prev = Variable(src[:, i : i+seq_len].contiguous(), volatile = evaluate)\n",
    "    targets = Variable(src[:, i+2 : i+seq_len+2].contiguous(), volatile = evaluate)\n",
    "    return data, prev, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing class for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx = None, smoothing = 0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist.add_(self.smoothing / self.size)\n",
    "        if self.padding_idx is not None:\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad = False)) * ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler that sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(h_size, warmup, optimizer):\n",
    "    lrate = lambda e: h_size**(-0.5) * min((e+1)**(-decay_factor), (e+1) * warmup**(-(decay_factor+1)))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2aa0132f940>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aa003a16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.1f:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.1f:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.1f:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.1f:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttentionModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, N = 6,\n",
    "                 embed_size = 512, ff_size = 2048,\n",
    "                 num_attn_heads = 8, dropout = 0.1):\n",
    "        super(TransformerAttentionModel, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.N = N\n",
    "        self.embed_size = embed_size\n",
    "        self.ff_size = ff_size\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_embedding = nn.Embedding(src_vocab, embed_size)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab, embed_size)\n",
    "        self.attn = TransformerAttention(\n",
    "            src_vocab, tgt_vocab, d_model = embed_size,\n",
    "            d_ff = ff_size, h = num_attn_heads, N = N,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.projection = nn.Linear(embed_size, tgt_vocab)\n",
    "        self.log_softmax = nn.LogSoftmax(-1)\n",
    "        \n",
    "        # Tie all embedding/projection weights\n",
    "        self.src_embedding.weight = self.projection.weight\n",
    "        self.tgt_embedding.weight = self.projection.weight\n",
    "        \n",
    "    def init(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "            else:\n",
    "                p.data.fill_(0)\n",
    "                \n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None):\n",
    "        src_embeddings = self.dropout(self.src_embedding(src))\n",
    "        tgt_embeddings = self.dropout(self.tgt_embedding(tgt))\n",
    "        \n",
    "        attentions = self.attn(src_embeddings, tgt_embeddings, src_mask, tgt_mask)\n",
    "        \n",
    "        logits = self.projection(self.dropout(attentions))\n",
    "        # LabelSmoothing loss requires the model output log-softmax\n",
    "        outputs = self.log_softmax(logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerAttentionModel(\n",
    "    ntokens, ntokens, N = n_layers, embed_size = embed_size,\n",
    "    ff_size = ff_size, num_attn_heads = n_heads, dropout = dropout\n",
    ")\n",
    "model.init()\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 8100624\n"
     ]
    }
   ],
   "source": [
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([74, 12562]), torch.Size([10, 7376]), torch.Size([10, 8243]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train():\n",
    "    # Use random length sequences\n",
    "    seq_lens = []\n",
    "    tot_len = 0\n",
    "    jitter = 0.15 * seq_len\n",
    "    num_data = train_data.size(1)\n",
    "    while tot_len < num_data - 2:\n",
    "        if num_data - tot_len - 2 < seq_len + jitter:\n",
    "            slen = num_data - tot_len - 2\n",
    "        else:\n",
    "            slen = int(np.random.normal(seq_len, jitter))\n",
    "            if slen <= 0:\n",
    "                slen = seq_len    # eh\n",
    "            if tot_len + slen >= num_data - jitter - 2:\n",
    "                slen = num_data - tot_len - 2\n",
    "        seq_lens.append(slen)\n",
    "        tot_len += slen\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    # Pre metainfo\n",
    "    total_loss = 0\n",
    "    total_epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(np.cumsum(seq_lens)):\n",
    "        # Get training data\n",
    "        data, prev, targets = get_batch(train_data, i, seq_lens[batch])\n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run the model forward\n",
    "        output = model(data, prev)\n",
    "        if np.isnan(output.data).any():\n",
    "            return 0, total_epoch_loss[0], data, prev, targets\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        if np.isnan(loss.data[0]):\n",
    "            return 1, total_epoch_loss[0], data, prev, targets\n",
    "        # Propagate loss gradient backwards\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # Save gradient statistics before they're changed cuz we'll be logging this batch\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            # Calculate the largest (absolute) gradient of all elements in the model parameters\n",
    "            max_grad = max([p.grad.data.abs().max() for p in parameters])\n",
    "        total_norm = nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        # Scale the batch learning rate so that shorter sequences aren't \"stronger\"\n",
    "        scaled_lr = lr_scheduler.get_lr()[0] * np.sqrt(seq_lens[batch] / seq_len)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = scaled_lr\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metainfo logging\n",
    "        total_loss += loss.data\n",
    "        total_epoch_loss += loss.data * data.size(1)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            print(' b {:3d}/{:3d} >> {:6.1f} ms/b | lr: {:8.2g} | grad norm: {:4.2f} | max abs grad: {:7.3f} | loss: {:4.2f} | perp.: {:6.2f}'.format(\n",
    "                batch, len(seq_lens), elapsed * 1000/log_interval, scaled_lr, total_norm, max_grad, cur_loss, np.exp(cur_loss)\n",
    "            ))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return -1, total_epoch_loss[0] / num_data, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_src):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i in range(0, data_src.size(1) - 2, seq_len):\n",
    "        # Get data\n",
    "        data, prev, targets = get_batch(data_src, i, seq_len, evaluate = True)\n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run model forward\n",
    "        output = model(data, prev)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        total_loss += loss.data * data.size(1)\n",
    "    return total_loss[0] / (data.size(1) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200) lr = 4.391e-08 (warmup)\n",
      "\n",
      "                                                   NaN output\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:.4g}{}'.format(epoch+1, epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, prev, targets = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('Max abs wt: {:5.2f}' % (\n",
    "        max_param\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "0.0\n",
      "[True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    print(max([p.grad.data.abs().max() for p in params]))\n",
    "    \n",
    "    src_embeddings = model.src_embedding(data)\n",
    "    tgt_embeddings = model.tgt_embedding(prev)\n",
    "\n",
    "    attentions = model.attn(src_embeddings, tgt_embeddings, None, None)\n",
    "\n",
    "    logits = model.projection(attentions)\n",
    "    # LabelSmoothing loss requires the model output log-softmax\n",
    "    outputs = model.log_softmax(logits)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [src_embeddings, tgt_embeddings, attentions, logits, outputs]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "\n",
       "( 1 ,.,.) = \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.2355e-02  4.1228e-03  2.2881e-02  ...   1.0479e-02 -1.7622e-02 -9.9456e-04\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "  1.8291e-02  6.5933e-03 -5.7079e-03  ...   2.1550e-02  5.5282e-03 -2.3849e-02\n",
       "\n",
       "( 2 ,.,.) = \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.4070e-02 -8.7339e-03 -2.1584e-02  ...  -4.3088e-03 -8.9790e-03 -1.4042e-02\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "... \n",
       "\n",
       "(71 ,.,.) = \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       "  8.4745e-03 -2.2536e-02  7.4402e-03  ...   9.3998e-03 -2.1390e-02  1.9223e-02\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "\n",
       "(72 ,.,.) = \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       " -1.0046e-02 -2.2341e-02 -1.6297e-02  ...  -8.2538e-03  1.1091e-02 -1.3393e-02\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "\n",
       "(73 ,.,.) = \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "                 ...                   ⋱                   ...                \n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "         nan         nan         nan  ...          nan         nan         nan\n",
       "[torch.FloatTensor of size 74x16x256]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0fdd929f8238>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n\u001b[0;32m      3\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ))\n",
      "\u001b[1;32m<ipython-input-14-623bf367e657>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(data_src)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Run model forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c5a183f08ebb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtgt_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mattentions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Coding\\AI\\attention\\transformer_attention\\transformer_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_stack\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Coding\\AI\\attention\\transformer_attention\\transformer_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;31m# Normalize input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0msrc_attn_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;31m# Position-wise, fully-connected feed-forward output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mff_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Coding\\AI\\attention\\transformer_attention\\transformer_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma_2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(test_data)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Check out the prediction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sequence from the generated data starting from whatever point\n",
    "data, prev, targets = get_batch(test_data, 120, seq_len)\n",
    "# We'll just look at a few batches\n",
    "nb = 4\n",
    "data = data[:nb].contiguous()\n",
    "prev = prev[:nb].contiguous()\n",
    "targets = targets[:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output = model(data, prev)\n",
    "# Convert to probabilities\n",
    "output = output.exp()\n",
    "# Get the max prob for each step\n",
    "output_probs, output_idx = output.max(-1)\n",
    "# Convert these to actual words\n",
    "output_txt = [corpus.dictionary.idx2word[i] for i in output]\n",
    "target_txt = [corpus.dictionary.idx2word[i] for i in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb):\n",
    "    o_txt = [corpus.dictionary.idx2word[i] for i in output_idx[i]]\n",
    "    t_txt = [corpus.dictionary.idx2word[i] for i in targets[i]]\n",
    "    print('Outputs: ', ' '.join(o_txt))\n",
    "    print('Targets: ', ' '.join(t_txt))\n",
    "    # Print the output with the targets as words\n",
    "    # Print the output with the targets as indices\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.modules())\n",
    "list(enumerate(modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_module = modules[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize attention weights for the first sequence:\n",
    "\n",
    "**Encoder attention weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(n_layers):\n",
    "    cols = 4\n",
    "    rows = n_heads//cols\n",
    "    fig, axs = plt.subplots(rows, cols, figsize = (20, 10))\n",
    "    wts = attn_module.encoder_stack[layer].attn.attn.data[:seq_len]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    print(\"\\nEncoder layer %d (mean = %g, min = %g, max = %g)\" % (layer+1, wts_mean, wts_min, wts_max))\n",
    "    for h in range(n_heads):\n",
    "        r = h // cols\n",
    "        c = h % cols\n",
    "        ax = axs[r, c]\n",
    "        ax.imshow(norm[0, h], aspect = 'auto', cmap = 'jet')\n",
    "        # Fix labels\n",
    "        s = list(targets[0].data)\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_xticklabels(s if r == rows-1 else [])\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        ax.set_yticklabels(s if c == 0 else [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder self-attention weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(n_layers):\n",
    "    cols = 4\n",
    "    rows = n_heads//cols\n",
    "    # Self-attention weights\n",
    "    fig, axs = plt.subplots(rows, cols, figsize = (20, 10))\n",
    "    wts = attn_module.decoder_stack[layer].self_attn.attn.data[:seq_len]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    print(\"\\nDecoder self-attention %d (mean = %g, min = %g, max = %g)\" % (layer+1, wts_mean, wts_min, wts_max))\n",
    "    for h in range(n_heads):\n",
    "        r = h // cols\n",
    "        c = h % cols\n",
    "        ax = axs[r, c]\n",
    "        ax.imshow(norm[0, h], aspect = 'auto', cmap = 'jet')\n",
    "        # Fix labels\n",
    "        s = list(targets[0].data)\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_xticklabels(s if r == rows-1 else [])\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        ax.set_yticklabels(s if c == 0 else [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder source-attention weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(n_layers):\n",
    "    cols = 4\n",
    "    rows = n_heads//cols\n",
    "    # Self-attention weights\n",
    "    fig, axs = plt.subplots(rows, cols, figsize = (20, 10))\n",
    "    wts = attn_module.decoder_stack[layer].src_attn.attn.data[:seq_len]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    print(\"\\nDecoder src-attention %d (mean = %g, min = %g, max = %g)\" % (layer+1, wts_mean, wts_min, wts_max))\n",
    "    for h in range(n_heads):\n",
    "        r = h // cols\n",
    "        c = h % cols\n",
    "        ax = axs[r, c]\n",
    "        ax.imshow(norm[0, h], aspect = 'auto', cmap = 'jet')\n",
    "        # Fix labels\n",
    "        s = list(targets[0].data)\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_xticklabels(s if r == rows-1 else [])\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        ax.set_yticklabels(s if c == 0 else [])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
