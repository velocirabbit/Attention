{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from recurrent_attention import RecurrentAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead stuff\n",
    "\n",
    "Helper functions for batching, resetting hidden states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 1\n",
    "lr = 0.01\n",
    "warmup_steps = 5\n",
    "decay_factor = 0.5  # Higher => faster learning rate decay\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 100\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 256\n",
    "encode_size = 128\n",
    "h_size = 128\n",
    "decode_size = 128\n",
    "decode_out_size = 256\n",
    "n_enc_layers = 2\n",
    "attn_rnn_layers = 2\n",
    "n_dec_layers = 2\n",
    "smooth_align = True\n",
    "\n",
    "bidirectional_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, `batchify` arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into batches\n",
    "    nbatches = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data.narrow(0, 0, nbatches * batch_size)\n",
    "    # Evenly divide the data across the batches\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "# Wraps hidden states into new Variables to detach them from their history\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "# `get_batch` subdivides the source data into chunks of the specified length.\n",
    "# E.g., using the example for the `batchify` function above and a length of 2,\n",
    "# we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the `batchify` function. The chunks are along dimension 0, corresponding\n",
    "# to the `seq_len` dimension in the LSTM.\n",
    "def get_batch(source, i, seq_len, evaluate = False):\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = Variable(source[i : i+seq_len], volatile = evaluate)\n",
    "    target = Variable(source[i+1 : i+1+seq_len].view(-1), volatile = evaluate)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing class for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx = None, smoothing = 0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist.add_(self.smoothing / self.size)\n",
    "        if self.padding_idx is not None:\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad = False)) * ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler that sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(h_size, warmup, optimizer):\n",
    "    lrate = lambda e: h_size**(-0.5) * min((e+1)**(-decay_factor), (e+1) * warmup**(-(decay_factor+1)))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2bd8be63550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bd89df8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.1f:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.1f:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.1f:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.1f:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, embed_size, encode_size, h_size,\n",
    "                 decode_size, decode_out_size, n_enc_layers, attn_rnn_layers,\n",
    "                 n_dec_layers, align_location = False, loc_align_size = 1,\n",
    "                 loc_align_kernel = 1,  smooth_align = False, bidirectional_attn = False,\n",
    "                 tie_wts = True, dropout = 0.1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.embed_size = embed_size\n",
    "        self.encode_size = encode_size\n",
    "        self.h_size = h_size\n",
    "        self.decode_size = decode_size\n",
    "        self.decode_out_size = decode_out_size\n",
    "        self.n_enc_layers = n_enc_layers\n",
    "        self.attn_rnn_layers = attn_rnn_layers\n",
    "        self.n_dec_layers = n_dec_layers\n",
    "        self.align_location = align_location\n",
    "        self.loc_align_size = loc_align_size\n",
    "        self.loc_align_kernel = loc_align_kernel\n",
    "        self.smooth_align = smooth_align\n",
    "        self.bidirectional_attn = bidirectional_attn\n",
    "        self.tie_wts = tie_wts\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab, embed_size)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size = embed_size if i == 0 else encode_size,\n",
    "                hidden_size = encode_size, dropout = dropout\n",
    "            ) for i in range(n_enc_layers)\n",
    "        ])\n",
    "        self.attn = RecurrentAttention(\n",
    "            in_size = encode_size, h_size = h_size, out_size = decode_size,\n",
    "            align_location = align_location, loc_align_size = loc_align_size,\n",
    "            loc_align_kernel = loc_align_kernel, smooth_align = smooth_align,\n",
    "            num_rnn_layers = attn_rnn_layers, attn_act_fn = 'ReLU',\n",
    "            dropout = dropout, bidirectional = bidirectional_attn\n",
    "        )\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size = decode_size, dropout = dropout,\n",
    "                hidden_size = decode_size if i < n_dec_layers-1 else decode_out_size,\n",
    "            ) for i in range(n_dec_layers)\n",
    "        ])\n",
    "        self.projection = nn.Linear(decode_out_size, tgt_vocab)\n",
    "        if tie_wts and src_vocab == tgt_vocab and embed_size == decode_out_size:\n",
    "            self.embedding.weight = self.projection.weight\n",
    "        self.log_softmax = nn.LogSoftmax(dim = -1)\n",
    "            \n",
    "        # For visualizations\n",
    "        self.save_wts = False\n",
    "        self.enc_out = None\n",
    "        self.dec_out = None\n",
    "        \n",
    "    def init(self):\n",
    "        for subnet in [self.encoders, self.decoders]:\n",
    "            for layer in subnet:\n",
    "                for p in layer.parameters():\n",
    "                    if p.dim() > 1:\n",
    "                        nn.init.xavier_normal(p)\n",
    "                    else:\n",
    "                        p.data.fill_(0)\n",
    "        for p in self.projection.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "            else:\n",
    "                p.data.fill_(0)\n",
    "        self.attn.init()\n",
    "        \n",
    "    def init_states(self, batch_size):\n",
    "        encoder_states = [\n",
    "            (\n",
    "                Variable(torch.zeros(1, batch_size, self.encode_size)),\n",
    "                Variable(torch.zeros(1, batch_size, self.encode_size))\n",
    "            ) for _ in range(self.n_enc_layers)\n",
    "        ]\n",
    "        attn_states = self.attn.init_rnn_states(batch_size)\n",
    "        decoder_states = [\n",
    "            (\n",
    "                Variable(torch.zeros(\n",
    "                    1, batch_size, self.decode_size if i < self.n_dec_layers-1 else self.decode_out_size\n",
    "                )),\n",
    "                Variable(torch.zeros(\n",
    "                    1, batch_size, self.decode_size if i < self.n_dec_layers-1 else self.decode_out_size\n",
    "                ))\n",
    "            ) for i in range(self.n_dec_layers)\n",
    "        ]\n",
    "        return encoder_states, attn_states, decoder_states\n",
    "    \n",
    "    def forward(self, inputs, states):\n",
    "        enc_states, attn_states, dec_states = states\n",
    "        if self.save_wts:\n",
    "            self.enc_out = []\n",
    "            self.dec_out = []\n",
    "        \n",
    "        embeddings = self.embedding(inputs) * np.sqrt(self.embed_size)\n",
    "        \n",
    "        new_enc_states = []\n",
    "        enc_in = self.drop(self.relu(embeddings))\n",
    "        for states, encoder in zip(enc_states, self.encoders):\n",
    "            enc_out, new_enc_state = encoder(enc_in, states)\n",
    "            new_enc_states.append(new_enc_state)\n",
    "            if self.save_wts:\n",
    "                self.enc_out.append(enc_out.data.clone())\n",
    "            enc_in = enc_out\n",
    "                \n",
    "        attn_out, new_attn_states = self.attn(enc_out, attn_states)\n",
    "        \n",
    "        new_dec_states = []\n",
    "        dec_in = attn_out\n",
    "        for states, decoder in zip(dec_states, self.decoders):\n",
    "            dec_out, new_dec_state = decoder(dec_in, states)\n",
    "            new_dec_states.append(new_dec_state)\n",
    "            if self.save_wts:\n",
    "                self.dec_out.append(dec_out.data.clone())\n",
    "            dec_in = dec_out\n",
    "        \n",
    "        logits = self.projection(dec_out)\n",
    "        output = self.log_softmax(logits)\n",
    "        \n",
    "        return output, (new_enc_states, new_attn_states, new_dec_states)\n",
    "    \n",
    "    def train(self, mode = True, save_wts = False):\n",
    "        super(RNNModel, self).train(mode)\n",
    "        self.attn.save_attn_wts = save_wts\n",
    "        self.save_wts = save_wts\n",
    "        \n",
    "    def eval(self, save_wts = True):\n",
    "        super(RNNModel, self).eval()\n",
    "        self.attn.save_attn_wts = save_wts\n",
    "        self.save_wts = save_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 3806353\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    src_vocab = ntokens, tgt_vocab = ntokens, embed_size = embed_size,\n",
    "    encode_size = encode_size, h_size = h_size, decode_size = decode_size,\n",
    "    decode_out_size = decode_out_size, n_enc_layers = n_enc_layers,\n",
    "    attn_rnn_layers = attn_rnn_layers, n_dec_layers = n_dec_layers,\n",
    "    dropout = dropout, smooth_align = smooth_align\n",
    ")\n",
    "model.init()\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, optimizer)\n",
    "# Reference\n",
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12562, 74]), torch.Size([7376, 10]), torch.Size([8243, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Use random length sequences\n",
    "    seq_lens = []\n",
    "    tot_len = 0\n",
    "    jitter = 0.15 * seq_len\n",
    "    num_data = train_data.size(0)\n",
    "    while tot_len < num_data - 2:\n",
    "        if num_data - tot_len - 2 <= seq_len + jitter:\n",
    "            slen = num_data - tot_len - 2\n",
    "        else:\n",
    "            slen = int(np.random.normal(seq_len, jitter))\n",
    "            if slen <= 0:\n",
    "                slen = seq_len    # eh\n",
    "            if tot_len + slen >= num_data - jitter - 2:\n",
    "                slen = num_data - tot_len - 2\n",
    "        seq_lens.append(slen)\n",
    "        tot_len += slen\n",
    "    # Turn on training mode\n",
    "    model.train(save_wts = False)\n",
    "    # Initialize RNN states\n",
    "    states = model.init_states(batch_size)\n",
    "    # Prep metainfo\n",
    "    total_loss = 0\n",
    "    total_epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(np.cumsum(seq_lens)):\n",
    "        # Get training data\n",
    "        data, targets = get_batch(train_data, i, seq_lens[batch])\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states)\n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run the model forward\n",
    "        output, _states = model(data, states)\n",
    "        if np.isnan(output.data).any():\n",
    "            return 0, total_epoch_loss[0], data, targets, states, _states\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        if np.isnan(loss.data[0]):\n",
    "            return 1, total_epoch_loss[0], data, targets, states, _states\n",
    "        states = _states\n",
    "        # Propagate loss gradient backwards\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # Save gradient statistics before they're changed cuz we'll be logging this batch\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            # Calculate the largest (absolute) gradient of all elements in the model parameters\n",
    "            max_grad = max([p.grad.data.abs().max() for p in parameters])\n",
    "        total_norm = nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        # Scale the batch learning rate so that shorter sequences aren't \"stronger\"\n",
    "        scaled_lr = lr_scheduler.get_lr()[0] * np.sqrt(seq_lens[batch] / seq_len)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = scaled_lr\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get some metainfo\n",
    "        total_loss += loss.data\n",
    "        total_epoch_loss += loss.data * data.size(0)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            print(' b {:3d}/{:3d} >> {:6.1f} ms/b | lr: {:8.2g} | grad norm: {:4.2f} | max abs grad: {:7.3f} | loss: {:4.2f} | perp.: {:6.2f}'.format(\n",
    "                batch, len(seq_lens), elapsed * 1000/log_interval, scaled_lr, total_norm, max_grad, cur_loss, np.exp(cur_loss)\n",
    "            ))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return -1, total_epoch_loss[0] / num_data, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_src, save_wts = True):\n",
    "    model.eval(save_wts = save_wts)\n",
    "    total_loss = 0\n",
    "    states = model.init_states(eval_batch_size)\n",
    "    for i in range(0, data_src.size(0) - 1, seq_len):\n",
    "        # Get data\n",
    "        data, targets = get_batch(data_src, i, seq_len, evaluate = True)\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states)\n",
    "        # Evaluate\n",
    "        output, states = model(data, states)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        total_loss += loss.data * data.size(0)\n",
    "    return total_loss[0] / data_src.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100) lr = 5.59e-05 (warmup)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-947f3de2369d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {:3d}/{:3d}) lr = {:0.4g}{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' (warmup)'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mwarmup_steps\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mstat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCAUSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCAUSES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0b97ac97949e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Propagate loss gradient backwards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m# Clip gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data, save_wts = False)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    enc_states, attn_states, dec_states = states\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoder(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoder(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, save_wts = True)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some random words\n",
    "Just, like, y'know, as a test or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "\n",
    "gstates = model.init_states(1)\n",
    "cur_word = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile = True)\n",
    "gen_text = [int(cur_word)]\n",
    "\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(cur_word, gstates)\n",
    "    word_weights = output.squeeze().exp()\n",
    "    word_idx = torch.multinomial(word_weights, 1).data[0]\n",
    "    gen_text.append(int(word_idx))\n",
    "    cur_word.data.fill_(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the words\n",
    "print(' '.join([corpus.dictionary.idx2word[i] for i in gen_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 4\n",
    "model.eval(save_wts = True)\n",
    "# Get some data from a random point in the test_data set\n",
    "states = model.init_states(nb)\n",
    "data, targets = get_batch(test_data, 120, seq_len, evaluate = True)\n",
    "data = data[:,:nb].contiguous()\n",
    "targets = targets.view(seq_len, -1)[:,:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output, states = model(data, states)\n",
    "# Convert the output log probabilities to normal probabilities\n",
    "output = output.exp()\n",
    "# Get the argmax of each step in the output\n",
    "output_p, output_idx = output.max(dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted output word indices to the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.t()\n",
    "output_idx = output_idx.t()\n",
    "for i in range(nb):\n",
    "    # Print the output with the targets\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "List of modules in the model for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modules = list(model.modules())\n",
    "list(enumerate(modules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic weight heat maps to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_wts = np.array(modules[3].weight.data)\n",
    "embed_norm = (embed_wts - embed_wts.mean()) / (embed_wts.max() - embed_wts.min())\n",
    "plt.imshow(embed_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('dim'); plt.ylabel('word index');\n",
    "plt.title('Embedding layer')\n",
    "plt.colorbar()\n",
    "embed_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = modules[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_wts = np.array(attn.attention.weight.data)\n",
    "attn_norm = (attn_wts - attn_wts.mean()) / (attn_wts.max() - attn_wts.min())\n",
    "plt.imshow(attn_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_input+d_state+d_output'); plt.ylabel('d_output')\n",
    "plt.title('Output attention sublayer (in attention mechanism)')\n",
    "plt.colorbar()\n",
    "attn_wts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence attention visualization by mapping the alignment weights (in the attention mechanism) at each step of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_out.size(), attn.attn_wts.size(), attn.rnn_states.size(), model.dec_out.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
