{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from recurrent_attention import RecurrentAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead stuff\n",
    "\n",
    "Helper functions for batching, resetting hidden states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 1\n",
    "lr = 0.1\n",
    "warmup_steps = 5\n",
    "decay_factor = 0.5\n",
    "smooth_labels = True\n",
    "\n",
    "epochs = 150\n",
    "log_interval = 100  # Print log every `log_interval` batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 256\n",
    "encode_size = 128\n",
    "h_size = 64\n",
    "attn_out_size = 128\n",
    "decode_size = 256\n",
    "n_layers = 2\n",
    "attn_rnn_layers = 1\n",
    "bidirectional_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, `batchify` arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into batches\n",
    "    nbatches = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data.narrow(0, 0, nbatches * batch_size)\n",
    "    # Evenly divide the data across the batches\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "# Wraps hidden states into new Variables to detach them from their history\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "# `get_batch` subdivides the source data into chunks of the specified length.\n",
    "# E.g., using the example for the `batchify` function above and a length of 2,\n",
    "# we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the `batchify` function. The chunks are along dimension 0, corresponding\n",
    "# to the `seq_len` dimension in the LSTM.\n",
    "def get_batch(source, i, seq_len, evaluate = False):\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = Variable(source[i : i+seq_len], volatile = evaluate)\n",
    "    target = Variable(source[i+1 : i+1+seq_len].view(-1), volatile = evaluate)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing class for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx = None, smoothing = 0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        if self.padding_idx is not None:\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler that sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(h_size, warmup, optimizer):\n",
    "    lrate = lambda e: h_size**(-0.5) * min((e+1)**(-decay_factor), (e+1) * warmup**(-(decay_factor+1)))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2f621835a58>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f61f7c22b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps*3),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\"%.1f:%d:%d\", \"%.1f:%d:%d\", \"%.1f:%d:%d\", \"%.1f:%d:%d\" % (\n",
    "    2*lr_factor*lr, embed_size, warmup_steps,\n",
    "    lr_factor*lr, embed_size*2, warmup_steps,\n",
    "    lr_factor*lr, embed_size, warmup_steps*3,\n",
    "    lr_factor*lr, embed_size, warmup_steps,\n",
    ")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab,\n",
    "                 embed_size, encode_size, h_size,\n",
    "                 attn_out_size, decode_size, n_layers,\n",
    "                 attn_rnn_layers, bidirectional_attn,\n",
    "                 tie_wts = True, dropout = 0.1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.embed_size = embed_size\n",
    "        self.encode_size = encode_size\n",
    "        self.h_size = h_size\n",
    "        self.attn_out_size = attn_out_size\n",
    "        self.decode_size = decode_size\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_rnn_layers = attn_rnn_layers\n",
    "        self.bidirectional_attn = bidirectional_attn\n",
    "        self.tie_wts = tie_wts\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab, embed_size)\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size = embed_size, hidden_size = encode_size,\n",
    "            num_layers = n_layers, dropout = dropout\n",
    "        )\n",
    "        self.attn = RecurrentAttention(\n",
    "            in_size = encode_size, h_size = h_size, out_size = attn_out_size,\n",
    "            dropout = dropout, num_rnn_layers = attn_rnn_layers,\n",
    "            attn_act_fn = 'Tanh', bidirectional = bidirectional_attn\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size = attn_out_size, hidden_size = decode_size,\n",
    "            num_layers = n_layers, dropout = dropout\n",
    "        )\n",
    "        self.projection = nn.Linear(decode_size, tgt_vocab)\n",
    "        if tie_wts and src_vocab == tgt_vocab and embed_size == decode_size:\n",
    "            self.embedding.weight = self.projection.weight\n",
    "        \n",
    "    def init(self):\n",
    "        for subnet in [self.encoder, self.decoder]:\n",
    "            for p in subnet.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_normal(p)\n",
    "                else:\n",
    "                    p.data.fill_(0)\n",
    "        for p in self.linear.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "            else:\n",
    "                p.data.fill_(0)\n",
    "        self.attn.init()\n",
    "        \n",
    "    def init_states(self, batch_size):\n",
    "        encoder_states = (\n",
    "            Variable(torch.zeros(self.n_layers, batch_size, self.encode_size)),\n",
    "            Variable(torch.zeros(self.n_layers, batch_size, self.encode_size))\n",
    "        )\n",
    "        attn_states = self.attn.init_rnn_states(batch_size)\n",
    "        decoder_states = (\n",
    "            Variable(torch.zeros(self.n_layers, batch_size, self.decode_size)),\n",
    "            Variable(torch.zeros(self.n_layers, batch_size, self.decode_size))\n",
    "        )\n",
    "        return encoder_states, attn_states, decoder_states\n",
    "    \n",
    "    def forward(self, inputs, states):\n",
    "        enc_states, attn_states, dec_states = states\n",
    "        relu = nn.ReLU()\n",
    "        log_softmax = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "        embeddings = self.embedding(inputs) * np.sqrt(self.embed_size)\n",
    "        enc_out, new_enc_states = self.encoder(self.drop(embeddings))\n",
    "        attn_out, new_attn_states = self.attn(enc_out, attn_states)\n",
    "        dec_out, new_dec_states = self.decoder(relu(attn_out))\n",
    "        output = self.projection(dec_out)\n",
    "        if smooth_labels:\n",
    "            output = log_softmax(output)\n",
    "        return output, (new_enc_states, new_attn_states, new_dec_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    ntokens, ntokens, embed_size, encode_size, h_size,\n",
    "    attn_out_size, decode_size, n_layers, attn_rnn_layers,\n",
    "    bidirectional_attn, dropout = dropout\n",
    ")\n",
    "if smooth_labels:\n",
    "    criterion = LabelSmoothing(ntokens, smoothing = 0.1)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 6422609\n"
     ]
    }
   ],
   "source": [
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Use random length sequences\n",
    "    seq_lens = []\n",
    "    tot_len = 0\n",
    "    jitter = 0.15 * seq_len\n",
    "    while tot_len < train_data.size(0) - 2:\n",
    "        if train_data.size(0) - tot_len - 2 <= seq_len + jitter:\n",
    "            slen = train_data.size(0) - tot_len - 2\n",
    "        else:\n",
    "            slen = int(np.random.normal(seq_len, jitter))\n",
    "            if slen <= 0:\n",
    "                slen = seq_len    # eh\n",
    "            if tot_len + slen >= train_data.size(0) - jitter - 2:\n",
    "                slen = train_data.size(0) - tot_len - 2\n",
    "        seq_lens.append(slen)\n",
    "        tot_len += slen\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    # Initialize RNN states\n",
    "    states = model.init_states(batch_size)\n",
    "    # Prep metainfo\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(np.cumsum(seq_lens)):\n",
    "        # Get training data\n",
    "        data, targets = get_batch(train_data, i, seq_lens[batch])\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states) #model.init_states(batch_size)\n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run the model forward\n",
    "        output, _states = model(data, states)\n",
    "        if np.isnan(output.data).any():\n",
    "            return (0, data, targets, states, _states)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        if np.isnan(loss.data[0]):\n",
    "            return (1, data, targets, states, _states)\n",
    "        states = _states\n",
    "        # Propagate loss gradient backwards\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # Save gradient statistics before they're changed cuz we'll be logging this batch\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            # Calculate the largest (absolute) gradient of all elements in the model parameters\n",
    "            max_grad = max([p.grad.data.abs().max() for p in parameters])\n",
    "        total_norm = nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        # Scale the batch learning rate so that shorter sequences aren't \"stronger\"\n",
    "        scaled_lr = [\n",
    "            r * np.sqrt(seq_lens[batch] / seq_len) for r in lr_scheduler.get_lr()\n",
    "        ]\n",
    "        for param_group, r in zip(optimizer.param_groups, scaled_lr):\n",
    "            param_group['lr'] = r\n",
    "        # Adjust parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get some metainfo\n",
    "        total_loss += loss.data\n",
    "        lr = np.mean(scaled_lr)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            print('b {:3d}/{:3d} >> {:6.1f} ms/b | lr: {:8.2g} | grad norm: {:4.2g} | max abs grad: {:8.2g} | loss: {:5.2f} | perp.: {:7.1f}'.format(\n",
    "                batch, len(seq_lens), elapsed * 1000/log_interval, lr, total_norm, max_grad, cur_loss, np.exp(cur_loss)\n",
    "            ))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return (-1, None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_src):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    states = model.init_states(eval_batch_size)\n",
    "    for i in range(0, data_src.size(0) - 1, seq_len):\n",
    "        # Get data\n",
    "        data, targets = get_batch(data_src, i, seq_len, evaluate = True)\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states) #model.init_states(eval_batch_size)\n",
    "        # Evaluate\n",
    "        output, states = model(data, states)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        total_loss += len(data) * loss.data\n",
    "    return total_loss[0] / len(data_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1) lr = 5e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Andrew\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\ipykernel_launcher.py:56: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  715 batches | 1501.87 ms/batch | lr: 5.278e-10 | loss: 10962.90 | perplexity:      inf\n",
      "  200/  715 batches | 1420.10 ms/batch | lr: 4.444e-10 | loss: 10604.90 | perplexity:      inf\n",
      "  300/  715 batches | 1421.16 ms/batch | lr: 5.833e-10 | loss: 10616.75 | perplexity:      inf\n",
      "  400/  715 batches | 1471.09 ms/batch | lr: 5.556e-10 | loss: 10962.61 | perplexity:      inf\n",
      "  500/  715 batches | 1413.70 ms/batch | lr: 3.611e-10 | loss: 10446.99 | perplexity:      inf\n",
      "  600/  715 batches | 1414.52 ms/batch | lr: 4.722e-10 | loss: 10337.67 | perplexity:      inf\n",
      "  700/  715 batches | 1436.35 ms/batch | lr: 4.444e-10 | loss: 10708.06 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1025.59 s | valid_loss: 1457.22 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   2) lr = 1e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Andrew\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  713 batches | 1450.47 ms/batch | lr: 1.389e-09 | loss: 10896.15 | perplexity:      inf\n",
      "  200/  713 batches | 1442.69 ms/batch | lr: 1.222e-09 | loss: 10483.59 | perplexity:      inf\n",
      "  300/  713 batches | 1487.05 ms/batch | lr: 7.778e-10 | loss: 10859.41 | perplexity:      inf\n",
      "  400/  713 batches | 1445.06 ms/batch | lr: 8.889e-10 | loss: 10744.39 | perplexity:      inf\n",
      "  500/  713 batches | 1414.15 ms/batch | lr: 1e-09 | loss: 10586.44 | perplexity:      inf\n",
      "  600/  713 batches | 1438.91 ms/batch | lr: 8.889e-10 | loss: 10616.74 | perplexity:      inf\n",
      "  700/  713 batches | 1412.28 ms/batch | lr: 8.889e-10 | loss: 10525.93 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1025.70 s | valid_loss: 1457.22 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   3) lr = 1.5e-09\n",
      "  100/  721 batches | 1452.34 ms/batch | lr: 9.167e-10 | loss: 10877.99 | perplexity:      inf\n",
      "  200/  721 batches | 1417.01 ms/batch | lr: 1.333e-09 | loss: 10623.12 | perplexity:      inf\n",
      "  300/  721 batches | 1422.39 ms/batch | lr: 1.25e-09 | loss: 10440.87 | perplexity:      inf\n",
      "  400/  721 batches | 1394.40 ms/batch | lr: 1.5e-09 | loss: 10465.24 | perplexity:      inf\n",
      "  500/  721 batches | 1398.35 ms/batch | lr: 1.667e-09 | loss: 10452.96 | perplexity:      inf\n",
      "  600/  721 batches | 1421.02 ms/batch | lr: 9.167e-10 | loss: 10683.51 | perplexity:      inf\n",
      "  700/  721 batches | 1381.16 ms/batch | lr: 1.667e-09 | loss: 10295.32 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.09 s | valid_loss: 1457.22 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   4) lr = 2e-09\n",
      "  100/  717 batches | 1407.44 ms/batch | lr: 1.889e-09 | loss: 10586.67 | perplexity:      inf\n",
      "  200/  717 batches | 1397.18 ms/batch | lr: 1.778e-09 | loss: 10447.20 | perplexity:      inf\n",
      "  300/  717 batches | 1405.67 ms/batch | lr: 1.667e-09 | loss: 10568.39 | perplexity:      inf\n",
      "  400/  717 batches | 1454.57 ms/batch | lr: 2.111e-09 | loss: 10665.44 | perplexity:      inf\n",
      "  500/  717 batches | 1433.63 ms/batch | lr: 1.667e-09 | loss: 10762.32 | perplexity:      inf\n",
      "  600/  717 batches | 1426.52 ms/batch | lr: 2.111e-09 | loss: 10701.56 | perplexity:      inf\n",
      "  700/  717 batches | 1428.65 ms/batch | lr: 1.667e-09 | loss: 10695.75 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.41 s | valid_loss: 1457.22 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   5) lr = 2.5e-09\n",
      "  100/  717 batches | 1502.10 ms/batch | lr: 1.806e-09 | loss: 10968.80 | perplexity:      inf\n",
      "  200/  717 batches | 1398.93 ms/batch | lr: 2.778e-09 | loss: 10501.85 | perplexity:      inf\n",
      "  300/  717 batches | 1410.91 ms/batch | lr: 2.361e-09 | loss: 10580.36 | perplexity:      inf\n",
      "  400/  717 batches | 1461.91 ms/batch | lr: 2.639e-09 | loss: 10968.75 | perplexity:      inf\n",
      "  500/  717 batches | 1416.96 ms/batch | lr: 2.222e-09 | loss: 10434.72 | perplexity:      inf\n",
      "  600/  717 batches | 1409.41 ms/batch | lr: 3.056e-09 | loss: 10549.93 | perplexity:      inf\n",
      "  700/  717 batches | 1384.04 ms/batch | lr: 2.361e-09 | loss: 10386.41 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.53 s | valid_loss: 1457.21 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   6) lr = 3e-09\n",
      "  100/  718 batches | 1452.14 ms/batch | lr: 3.167e-09 | loss: 10889.95 | perplexity:      inf\n",
      "  200/  718 batches | 1431.27 ms/batch | lr: 3.333e-09 | loss: 10513.82 | perplexity:      inf\n",
      "  300/  718 batches | 1386.79 ms/batch | lr: 3e-09 | loss: 10404.50 | perplexity:      inf\n",
      "  400/  718 batches | 1411.43 ms/batch | lr: 3.5e-09 | loss: 10574.43 | perplexity:      inf\n",
      "  500/  718 batches | 1429.99 ms/batch | lr: 2.833e-09 | loss: 10683.41 | perplexity:      inf\n",
      "  600/  718 batches | 1443.84 ms/batch | lr: 2.5e-09 | loss: 10586.29 | perplexity:      inf\n",
      "  700/  718 batches | 1414.64 ms/batch | lr: 2.167e-09 | loss: 10586.47 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.65 s | valid_loss: 1457.21 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   7) lr = 3.5e-09\n",
      "  100/  715 batches | 1477.84 ms/batch | lr: 3.306e-09 | loss: 11065.83 | perplexity:      inf\n",
      "  200/  715 batches | 1400.00 ms/batch | lr: 3.889e-09 | loss: 10489.76 | perplexity:      inf\n",
      "  300/  715 batches | 1453.62 ms/batch | lr: 3.111e-09 | loss: 10677.36 | perplexity:      inf\n",
      "  400/  715 batches | 1398.13 ms/batch | lr: 4.472e-09 | loss: 10434.92 | perplexity:      inf\n",
      "  500/  715 batches | 1421.05 ms/batch | lr: 3.111e-09 | loss: 10628.84 | perplexity:      inf\n",
      "  600/  715 batches | 1414.04 ms/batch | lr: 2.139e-09 | loss: 10592.32 | perplexity:      inf\n",
      "  700/  715 batches | 1476.29 ms/batch | lr: 3.5e-09 | loss: 10865.43 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1020.01 s | valid_loss: 1457.20 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   8) lr = 4e-09\n",
      "  100/  717 batches | 1460.67 ms/batch | lr: 4.444e-09 | loss: 10920.23 | perplexity:      inf\n",
      "  200/  717 batches | 1413.53 ms/batch | lr: 4.444e-09 | loss: 10617.01 | perplexity:      inf\n",
      "  300/  717 batches | 1432.87 ms/batch | lr: 4e-09 | loss: 10725.86 | perplexity:      inf\n",
      "  400/  717 batches | 1412.38 ms/batch | lr: 3.778e-09 | loss: 10343.90 | perplexity:      inf\n",
      "  500/  717 batches | 1398.81 ms/batch | lr: 3.556e-09 | loss: 10489.28 | perplexity:      inf\n",
      "  600/  717 batches | 1458.30 ms/batch | lr: 4.444e-09 | loss: 10780.37 | perplexity:      inf\n",
      "  700/  717 batches | 1444.80 ms/batch | lr: 3.778e-09 | loss: 10507.68 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1023.34 s | valid_loss: 1457.20 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch   9) lr = 4.5e-09\n",
      "  100/  726 batches | 1436.64 ms/batch | lr: 4.5e-09 | loss: 10616.85 | perplexity:      inf\n",
      "  200/  726 batches | 1443.87 ms/batch | lr: 4.75e-09 | loss: 10701.90 | perplexity:      inf\n",
      "  300/  726 batches | 1398.92 ms/batch | lr: 3.5e-09 | loss: 10483.28 | perplexity:      inf\n",
      "  400/  726 batches | 1406.53 ms/batch | lr: 4.5e-09 | loss: 10556.14 | perplexity:      inf\n",
      "  500/  726 batches | 1421.08 ms/batch | lr: 4.25e-09 | loss: 10434.62 | perplexity:      inf\n",
      "  600/  726 batches | 1368.44 ms/batch | lr: 4.5e-09 | loss: 10282.92 | perplexity:      inf\n",
      "  700/  726 batches | 1400.26 ms/batch | lr: 5e-09 | loss: 10495.24 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.81 s | valid_loss: 1457.19 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  10) lr = 5e-09\n",
      "  100/  721 batches | 1391.24 ms/batch | lr: 4.444e-09 | loss: 10452.97 | perplexity:      inf\n",
      "  200/  721 batches | 1421.69 ms/batch | lr: 4.167e-09 | loss: 10446.90 | perplexity:      inf\n",
      "  300/  721 batches | 1438.92 ms/batch | lr: 5e-09 | loss: 10816.90 | perplexity:      inf\n",
      "  400/  721 batches | 1452.38 ms/batch | lr: 5.556e-09 | loss: 10859.37 | perplexity:      inf\n",
      "  500/  721 batches | 1375.66 ms/batch | lr: 5e-09 | loss: 10355.62 | perplexity:      inf\n",
      "  600/  721 batches | 1432.07 ms/batch | lr: 6.111e-09 | loss: 10495.19 | perplexity:      inf\n",
      "  700/  721 batches | 1411.26 ms/batch | lr: 4.167e-09 | loss: 10574.04 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.34 s | valid_loss: 1457.19 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  11) lr = 5.5e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  727 batches | 1413.84 ms/batch | lr: 5.194e-09 | loss: 10604.69 | perplexity:      inf\n",
      "  200/  727 batches | 1414.06 ms/batch | lr: 5.194e-09 | loss: 10622.82 | perplexity:      inf\n",
      "  300/  727 batches | 1400.74 ms/batch | lr: 4.889e-09 | loss: 10289.05 | perplexity:      inf\n",
      "  400/  727 batches | 1382.90 ms/batch | lr: 5.806e-09 | loss: 10367.82 | perplexity:      inf\n",
      "  500/  727 batches | 1392.61 ms/batch | lr: 4.889e-09 | loss: 10440.63 | perplexity:      inf\n",
      "  600/  727 batches | 1394.99 ms/batch | lr: 5.5e-09 | loss: 10446.56 | perplexity:      inf\n",
      "  700/  727 batches | 1422.25 ms/batch | lr: 5.5e-09 | loss: 10410.31 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.24 s | valid_loss: 1457.18 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  12) lr = 6e-09\n",
      "  100/  716 batches | 1481.98 ms/batch | lr: 6.667e-09 | loss: 11095.92 | perplexity:      inf\n",
      "  200/  716 batches | 1416.45 ms/batch | lr: 4.667e-09 | loss: 10544.01 | perplexity:      inf\n",
      "  300/  716 batches | 1420.28 ms/batch | lr: 5.333e-09 | loss: 10628.56 | perplexity:      inf\n",
      "  400/  716 batches | 1440.40 ms/batch | lr: 4.333e-09 | loss: 10580.22 | perplexity:      inf\n",
      "  500/  716 batches | 1434.07 ms/batch | lr: 6.333e-09 | loss: 10671.00 | perplexity:      inf\n",
      "  600/  716 batches | 1375.86 ms/batch | lr: 5e-09 | loss: 10294.80 | perplexity:      inf\n",
      "  700/  716 batches | 1433.20 ms/batch | lr: 5.333e-09 | loss: 10756.11 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.14 s | valid_loss: 1457.17 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  13) lr = 6.5e-09\n",
      "  100/  724 batches | 1461.96 ms/batch | lr: 7.583e-09 | loss: 10713.65 | perplexity:      inf\n",
      "  200/  724 batches | 1400.20 ms/batch | lr: 4.333e-09 | loss: 10489.27 | perplexity:      inf\n",
      "  300/  724 batches | 1388.38 ms/batch | lr: 6.5e-09 | loss: 10410.18 | perplexity:      inf\n",
      "  400/  724 batches | 1383.92 ms/batch | lr: 6.139e-09 | loss: 10385.90 | perplexity:      inf\n",
      "  500/  724 batches | 1451.93 ms/batch | lr: 7.583e-09 | loss: 10658.88 | perplexity:      inf\n",
      "  600/  724 batches | 1383.46 ms/batch | lr: 5.417e-09 | loss: 10313.05 | perplexity:      inf\n",
      "  700/  724 batches | 1411.24 ms/batch | lr: 6.139e-09 | loss: 10579.98 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1020.23 s | valid_loss: 1457.16 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  14) lr = 7e-09\n",
      "  100/  718 batches | 1404.65 ms/batch | lr: 7.389e-09 | loss: 10543.76 | perplexity:      inf\n",
      "  200/  718 batches | 1471.42 ms/batch | lr: 8.944e-09 | loss: 10792.48 | perplexity:      inf\n",
      "  300/  718 batches | 1422.61 ms/batch | lr: 7e-09 | loss: 10598.20 | perplexity:      inf\n",
      "  400/  718 batches | 1441.24 ms/batch | lr: 5.833e-09 | loss: 10780.27 | perplexity:      inf\n",
      "  500/  718 batches | 1410.73 ms/batch | lr: 7e-09 | loss: 10573.88 | perplexity:      inf\n",
      "  600/  718 batches | 1447.88 ms/batch | lr: 7.778e-09 | loss: 10658.69 | perplexity:      inf\n",
      "  700/  718 batches | 1395.97 ms/batch | lr: 7.389e-09 | loss: 10379.85 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.33 s | valid_loss: 1457.15 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  15) lr = 7.5e-09\n",
      "  100/  722 batches | 1383.85 ms/batch | lr: 8.75e-09 | loss: 10379.96 | perplexity:      inf\n",
      "  200/  722 batches | 1437.98 ms/batch | lr: 5.833e-09 | loss: 10780.31 | perplexity:      inf\n",
      "  300/  722 batches | 1420.91 ms/batch | lr: 5e-09 | loss: 10385.92 | perplexity:      inf\n",
      "  400/  722 batches | 1420.59 ms/batch | lr: 9.167e-09 | loss: 10598.16 | perplexity:      inf\n",
      "  500/  722 batches | 1351.65 ms/batch | lr: 6.25e-09 | loss: 10149.06 | perplexity:      inf\n",
      "  600/  722 batches | 1433.68 ms/batch | lr: 7.5e-09 | loss: 10737.48 | perplexity:      inf\n",
      "  700/  722 batches | 1450.32 ms/batch | lr: 7.917e-09 | loss: 10713.27 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1020.79 s | valid_loss: 1457.14 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  16) lr = 8e-09\n",
      "  100/  717 batches | 1435.56 ms/batch | lr: 7.111e-09 | loss: 10737.74 | perplexity:      inf\n",
      "  200/  717 batches | 1420.73 ms/batch | lr: 6.222e-09 | loss: 10652.83 | perplexity:      inf\n",
      "  300/  717 batches | 1406.49 ms/batch | lr: 8e-09 | loss: 10543.45 | perplexity:      inf\n",
      "  400/  717 batches | 1438.51 ms/batch | lr: 9.778e-09 | loss: 10525.40 | perplexity:      inf\n",
      "  500/  717 batches | 1443.13 ms/batch | lr: 7.556e-09 | loss: 10773.95 | perplexity:      inf\n",
      "  600/  717 batches | 1402.31 ms/batch | lr: 5.778e-09 | loss: 10518.95 | perplexity:      inf\n",
      "  700/  717 batches | 1438.87 ms/batch | lr: 1.067e-08 | loss: 10761.88 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.20 s | valid_loss: 1457.13 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  17) lr = 8.5e-09\n",
      "  100/  715 batches | 1493.53 ms/batch | lr: 9.917e-09 | loss: 10955.99 | perplexity:      inf\n",
      "  200/  715 batches | 1464.07 ms/batch | lr: 8.5e-09 | loss: 10859.04 | perplexity:      inf\n",
      "  300/  715 batches | 1383.68 ms/batch | lr: 9.917e-09 | loss: 10373.34 | perplexity:      inf\n",
      "  400/  715 batches | 1428.22 ms/batch | lr: 7.556e-09 | loss: 10689.05 | perplexity:      inf\n",
      "  500/  715 batches | 1462.74 ms/batch | lr: 7.556e-09 | loss: 10725.27 | perplexity:      inf\n",
      "  600/  715 batches | 1389.86 ms/batch | lr: 9.444e-09 | loss: 10403.69 | perplexity:      inf\n",
      "  700/  715 batches | 1413.40 ms/batch | lr: 8.028e-09 | loss: 10591.95 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.74 s | valid_loss: 1457.12 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  18) lr = 9e-09\n",
      "  100/  717 batches | 1440.25 ms/batch | lr: 1.15e-08 | loss: 10768.01 | perplexity:      inf\n",
      "  200/  717 batches | 1460.48 ms/batch | lr: 1e-08 | loss: 10701.33 | perplexity:      inf\n",
      "  300/  717 batches | 1435.47 ms/batch | lr: 8.5e-09 | loss: 10737.29 | perplexity:      inf\n",
      "  400/  717 batches | 1423.44 ms/batch | lr: 9.5e-09 | loss: 10670.73 | perplexity:      inf\n",
      "  500/  717 batches | 1422.60 ms/batch | lr: 7.5e-09 | loss: 10652.31 | perplexity:      inf\n",
      "  600/  717 batches | 1415.22 ms/batch | lr: 1e-08 | loss: 10342.97 | perplexity:      inf\n",
      "  700/  717 batches | 1411.82 ms/batch | lr: 9.5e-09 | loss: 10561.40 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.44 s | valid_loss: 1457.10 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  19) lr = 9.5e-09\n",
      "  100/  719 batches | 1435.01 ms/batch | lr: 9.5e-09 | loss: 10767.82 | perplexity:      inf\n",
      "  200/  719 batches | 1387.13 ms/batch | lr: 1.108e-08 | loss: 10379.58 | perplexity:      inf\n",
      "  300/  719 batches | 1440.80 ms/batch | lr: 6.861e-09 | loss: 10591.80 | perplexity:      inf\n",
      "  400/  719 batches | 1426.19 ms/batch | lr: 1.003e-08 | loss: 10670.66 | perplexity:      inf\n",
      "  500/  719 batches | 1414.21 ms/batch | lr: 1.003e-08 | loss: 10609.75 | perplexity:      inf\n",
      "  600/  719 batches | 1414.91 ms/batch | lr: 8.444e-09 | loss: 10603.63 | perplexity:      inf\n",
      "  700/  719 batches | 1437.58 ms/batch | lr: 8.444e-09 | loss: 10512.87 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1019.58 s | valid_loss: 1457.09 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  20) lr = 1e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  718 batches | 1446.46 ms/batch | lr: 1.056e-08 | loss: 10828.55 | perplexity:      inf\n",
      "  200/  718 batches | 1420.77 ms/batch | lr: 1.056e-08 | loss: 10658.56 | perplexity:      inf\n",
      "  300/  718 batches | 1425.57 ms/batch | lr: 9.444e-09 | loss: 10640.07 | perplexity:      inf\n",
      "  400/  718 batches | 1434.20 ms/batch | lr: 1.111e-08 | loss: 10530.98 | perplexity:      inf\n",
      "  500/  718 batches | 1393.55 ms/batch | lr: 1.278e-08 | loss: 10452.10 | perplexity:      inf\n",
      "  600/  718 batches | 1408.93 ms/batch | lr: 1e-08 | loss: 10554.99 | perplexity:      inf\n",
      "  700/  718 batches | 1431.03 ms/batch | lr: 8.333e-09 | loss: 10670.54 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.24 s | valid_loss: 1457.08 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  21) lr = 1.05e-08\n",
      "  100/  724 batches | 1448.50 ms/batch | lr: 9.917e-09 | loss: 10603.85 | perplexity:      inf\n",
      "  200/  724 batches | 1378.42 ms/batch | lr: 9.917e-09 | loss: 10312.63 | perplexity:      inf\n",
      "  300/  724 batches | 1383.95 ms/batch | lr: 9.333e-09 | loss: 10385.35 | perplexity:      inf\n",
      "  400/  724 batches | 1421.30 ms/batch | lr: 1.108e-08 | loss: 10573.30 | perplexity:      inf\n",
      "  500/  724 batches | 1477.38 ms/batch | lr: 9.917e-09 | loss: 10773.31 | perplexity:      inf\n",
      "  600/  724 batches | 1426.84 ms/batch | lr: 9.333e-09 | loss: 10633.82 | perplexity:      inf\n",
      "  700/  724 batches | 1395.73 ms/batch | lr: 8.167e-09 | loss: 10397.37 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1023.68 s | valid_loss: 1457.06 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  22) lr = 1.1e-08\n",
      "  100/  713 batches | 1456.80 ms/batch | lr: 1.1e-08 | loss: 10761.38 | perplexity:      inf\n",
      "  200/  713 batches | 1481.36 ms/batch | lr: 1.039e-08 | loss: 10816.06 | perplexity:      inf\n",
      "  300/  713 batches | 1428.06 ms/batch | lr: 9.778e-09 | loss: 10621.60 | perplexity:      inf\n",
      "  400/  713 batches | 1431.40 ms/batch | lr: 1.1e-08 | loss: 10640.00 | perplexity:      inf\n",
      "  500/  713 batches | 1423.03 ms/batch | lr: 1.161e-08 | loss: 10554.87 | perplexity:      inf\n",
      "  600/  713 batches | 1474.89 ms/batch | lr: 1.039e-08 | loss: 10736.77 | perplexity:      inf\n",
      "  700/  713 batches | 1432.61 ms/batch | lr: 9.778e-09 | loss: 10664.23 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1028.67 s | valid_loss: 1457.05 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  23) lr = 1.15e-08\n",
      "  100/  717 batches | 1458.96 ms/batch | lr: 1.086e-08 | loss: 10918.98 | perplexity:      inf\n",
      "  200/  717 batches | 1457.63 ms/batch | lr: 1.342e-08 | loss: 10810.00 | perplexity:      inf\n",
      "  300/  717 batches | 1508.48 ms/batch | lr: 1.278e-08 | loss: 11106.73 | perplexity:      inf\n",
      "  400/  717 batches | 1377.88 ms/batch | lr: 8.944e-09 | loss: 10342.74 | perplexity:      inf\n",
      "  500/  717 batches | 1394.74 ms/batch | lr: 1.086e-08 | loss: 10445.72 | perplexity:      inf\n",
      "  600/  717 batches | 1384.60 ms/batch | lr: 1.022e-08 | loss: 10324.20 | perplexity:      inf\n",
      "  700/  717 batches | 1425.49 ms/batch | lr: 1.022e-08 | loss: 10439.68 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.59 s | valid_loss: 1457.03 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  24) lr = 1.2e-08\n",
      "  100/  711 batches | 1421.58 ms/batch | lr: 1.4e-08 | loss: 10645.95 | perplexity:      inf\n",
      "  200/  711 batches | 1423.39 ms/batch | lr: 1.333e-08 | loss: 10670.27 | perplexity:      inf\n",
      "  300/  711 batches | 1469.06 ms/batch | lr: 1.2e-08 | loss: 10912.60 | perplexity:      inf\n",
      "  400/  711 batches | 1442.90 ms/batch | lr: 1.067e-08 | loss: 10573.06 | perplexity:      inf\n",
      "  500/  711 batches | 1465.91 ms/batch | lr: 1.333e-08 | loss: 10924.66 | perplexity:      inf\n",
      "  600/  711 batches | 1415.98 ms/batch | lr: 1.467e-08 | loss: 10548.49 | perplexity:      inf\n",
      "  700/  711 batches | 1451.35 ms/batch | lr: 1.267e-08 | loss: 10767.10 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.45 s | valid_loss: 1457.01 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  25) lr = 1.25e-08\n",
      "  100/  717 batches | 1428.74 ms/batch | lr: 1.111e-08 | loss: 10591.20 | perplexity:      inf\n",
      "  200/  717 batches | 1437.60 ms/batch | lr: 8.333e-09 | loss: 10712.60 | perplexity:      inf\n",
      "  300/  717 batches | 1426.52 ms/batch | lr: 1.389e-08 | loss: 10651.62 | perplexity:      inf\n",
      "  400/  717 batches | 1419.99 ms/batch | lr: 1.597e-08 | loss: 10500.14 | perplexity:      inf\n",
      "  500/  717 batches | 1464.12 ms/batch | lr: 1.111e-08 | loss: 10754.65 | perplexity:      inf\n",
      "  600/  717 batches | 1427.00 ms/batch | lr: 1.111e-08 | loss: 10651.47 | perplexity:      inf\n",
      "  700/  717 batches | 1389.10 ms/batch | lr: 1.25e-08 | loss: 10421.22 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.59 s | valid_loss: 1457.00 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  26) lr = 1.226e-08\n",
      "  100/  717 batches | 1515.32 ms/batch | lr: 1.021e-08 | loss: 10651.68 | perplexity:      inf\n",
      "  200/  717 batches | 1466.28 ms/batch | lr: 1.226e-08 | loss: 10512.22 | perplexity:      inf\n",
      "  300/  717 batches | 1432.26 ms/batch | lr: 1.226e-08 | loss: 10366.62 | perplexity:      inf\n",
      "  400/  717 batches | 1506.52 ms/batch | lr: 1.43e-08 | loss: 10924.50 | perplexity:      inf\n",
      "  500/  717 batches | 1497.75 ms/batch | lr: 1.09e-08 | loss: 10627.18 | perplexity:      inf\n",
      "  600/  717 batches | 1451.37 ms/batch | lr: 1.362e-08 | loss: 10524.03 | perplexity:      inf\n",
      "  700/  717 batches | 1482.46 ms/batch | lr: 1.021e-08 | loss: 10748.53 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1057.29 s | valid_loss: 1456.98 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  27) lr = 1.203e-08\n",
      "  100/  716 batches | 1480.55 ms/batch | lr: 1.203e-08 | loss: 10675.92 | perplexity:      inf\n",
      "  200/  716 batches | 1473.54 ms/batch | lr: 8.687e-09 | loss: 10560.71 | perplexity:      inf\n",
      "  300/  716 batches | 1455.51 ms/batch | lr: 1.27e-08 | loss: 10560.53 | perplexity:      inf\n",
      "  400/  716 batches | 1438.75 ms/batch | lr: 1.002e-08 | loss: 10317.88 | perplexity:      inf\n",
      "  500/  716 batches | 1476.98 ms/batch | lr: 1.203e-08 | loss: 10754.42 | perplexity:      inf\n",
      "  600/  716 batches | 1511.68 ms/batch | lr: 1.27e-08 | loss: 10754.43 | perplexity:      inf\n",
      "  700/  716 batches | 1483.38 ms/batch | lr: 1.136e-08 | loss: 10742.42 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1053.80 s | valid_loss: 1456.96 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  28) lr = 1.181e-08\n",
      "  100/  717 batches | 1465.07 ms/batch | lr: 1.05e-08 | loss: 10669.66 | perplexity:      inf\n",
      "  200/  717 batches | 1446.25 ms/batch | lr: 8.53e-09 | loss: 10329.99 | perplexity:      inf\n",
      "  300/  717 batches | 1440.64 ms/batch | lr: 1.312e-08 | loss: 10493.58 | perplexity:      inf\n",
      "  400/  717 batches | 1472.69 ms/batch | lr: 1.247e-08 | loss: 10681.79 | perplexity:      inf\n",
      "  500/  717 batches | 1461.62 ms/batch | lr: 8.53e-09 | loss: 10584.53 | perplexity:      inf\n",
      "  600/  717 batches | 1495.51 ms/batch | lr: 1.05e-08 | loss: 10620.95 | perplexity:      inf\n",
      "  700/  717 batches | 1523.98 ms/batch | lr: 1.575e-08 | loss: 10972.90 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1052.90 s | valid_loss: 1456.94 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  29) lr = 1.161e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  719 batches | 1493.94 ms/batch | lr: 1.161e-08 | loss: 10797.08 | perplexity:      inf\n",
      "  200/  719 batches | 1451.16 ms/batch | lr: 1.354e-08 | loss: 10493.76 | perplexity:      inf\n",
      "  300/  719 batches | 1499.64 ms/batch | lr: 8.382e-09 | loss: 10657.20 | perplexity:      inf\n",
      "  400/  719 batches | 1447.45 ms/batch | lr: 1.29e-08 | loss: 10481.50 | perplexity:      inf\n",
      "  500/  719 batches | 1446.37 ms/batch | lr: 1.419e-08 | loss: 10463.01 | perplexity:      inf\n",
      "  600/  719 batches | 1439.09 ms/batch | lr: 1.096e-08 | loss: 10547.99 | perplexity:      inf\n",
      "  700/  719 batches | 1459.59 ms/batch | lr: 8.382e-09 | loss: 10675.37 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1048.58 s | valid_loss: 1456.93 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  30) lr = 1.141e-08\n",
      "  100/  711 batches | 1439.49 ms/batch | lr: 8.875e-09 | loss: 10711.98 | perplexity:      inf\n",
      "  200/  711 batches | 1438.11 ms/batch | lr: 8.875e-09 | loss: 10748.43 | perplexity:      inf\n",
      "  300/  711 batches | 1426.18 ms/batch | lr: 8.241e-09 | loss: 10620.64 | perplexity:      inf\n",
      "  400/  711 batches | 1430.44 ms/batch | lr: 1.078e-08 | loss: 10457.21 | perplexity:      inf\n",
      "  500/  711 batches | 1432.22 ms/batch | lr: 9.509e-09 | loss: 10717.65 | perplexity:      inf\n",
      "  600/  711 batches | 1436.05 ms/batch | lr: 1.078e-08 | loss: 10802.54 | perplexity:      inf\n",
      "  700/  711 batches | 1448.24 ms/batch | lr: 1.204e-08 | loss: 10869.39 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.63 s | valid_loss: 1456.91 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  31) lr = 1.123e-08\n",
      "  100/  714 batches | 1457.13 ms/batch | lr: 8.731e-09 | loss: 10754.20 | perplexity:      inf\n",
      "  200/  714 batches | 1458.74 ms/batch | lr: 1.123e-08 | loss: 10839.28 | perplexity:      inf\n",
      "  300/  714 batches | 1415.83 ms/batch | lr: 1.06e-08 | loss: 10547.82 | perplexity:      inf\n",
      "  400/  714 batches | 1460.47 ms/batch | lr: 1.06e-08 | loss: 10851.19 | perplexity:      inf\n",
      "  500/  714 batches | 1431.11 ms/batch | lr: 1.06e-08 | loss: 10450.86 | perplexity:      inf\n",
      "  600/  714 batches | 1421.72 ms/batch | lr: 1.247e-08 | loss: 10614.32 | perplexity:      inf\n",
      "  700/  714 batches | 1428.18 ms/batch | lr: 1.31e-08 | loss: 10663.19 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1023.94 s | valid_loss: 1456.90 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  32) lr = 1.105e-08\n",
      "  100/  719 batches | 1396.27 ms/batch | lr: 7.98e-09 | loss: 10487.31 | perplexity:      inf\n",
      "  200/  719 batches | 1469.65 ms/batch | lr: 1.289e-08 | loss: 10814.84 | perplexity:      inf\n",
      "  300/  719 batches | 1386.46 ms/batch | lr: 1.043e-08 | loss: 10456.81 | perplexity:      inf\n",
      "  400/  719 batches | 1377.35 ms/batch | lr: 1.105e-08 | loss: 10384.08 | perplexity:      inf\n",
      "  500/  719 batches | 1419.22 ms/batch | lr: 1.289e-08 | loss: 10674.95 | perplexity:      inf\n",
      "  600/  719 batches | 1442.43 ms/batch | lr: 8.593e-09 | loss: 10608.17 | perplexity:      inf\n",
      "  700/  719 batches | 1440.82 ms/batch | lr: 9.821e-09 | loss: 10759.93 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.85 s | valid_loss: 1456.88 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  33) lr = 1.088e-08\n",
      "  100/  720 batches | 1418.27 ms/batch | lr: 1.028e-08 | loss: 10675.22 | perplexity:      inf\n",
      "  200/  720 batches | 1364.63 ms/batch | lr: 1.148e-08 | loss: 10274.86 | perplexity:      inf\n",
      "  300/  720 batches | 1464.81 ms/batch | lr: 9.067e-09 | loss: 10778.16 | perplexity:      inf\n",
      "  400/  720 batches | 1404.83 ms/batch | lr: 8.462e-09 | loss: 10565.92 | perplexity:      inf\n",
      "  500/  720 batches | 1398.07 ms/batch | lr: 9.067e-09 | loss: 10529.37 | perplexity:      inf\n",
      "  600/  720 batches | 1419.46 ms/batch | lr: 1.028e-08 | loss: 10656.70 | perplexity:      inf\n",
      "  700/  720 batches | 1433.73 ms/batch | lr: 1.088e-08 | loss: 10541.49 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.24 s | valid_loss: 1456.86 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  34) lr = 1.072e-08\n",
      "  100/  709 batches | 1452.22 ms/batch | lr: 1.131e-08 | loss: 10905.60 | perplexity:      inf\n",
      "  200/  709 batches | 1399.54 ms/batch | lr: 1.191e-08 | loss: 10535.66 | perplexity:      inf\n",
      "  300/  709 batches | 1417.53 ms/batch | lr: 9.528e-09 | loss: 10656.66 | perplexity:      inf\n",
      "  400/  709 batches | 1448.62 ms/batch | lr: 1.072e-08 | loss: 10674.93 | perplexity:      inf\n",
      "  500/  709 batches | 1455.00 ms/batch | lr: 9.528e-09 | loss: 10893.27 | perplexity:      inf\n",
      "  600/  709 batches | 1411.94 ms/batch | lr: 1.012e-08 | loss: 10601.84 | perplexity:      inf\n",
      "  700/  709 batches | 1450.57 ms/batch | lr: 1.489e-08 | loss: 10899.28 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1014.03 s | valid_loss: 1456.85 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  35) lr = 1.056e-08\n",
      "  100/  718 batches | 1444.45 ms/batch | lr: 7.63e-09 | loss: 10668.83 | perplexity:      inf\n",
      "  200/  718 batches | 1414.36 ms/batch | lr: 9.391e-09 | loss: 10656.86 | perplexity:      inf\n",
      "  300/  718 batches | 1429.06 ms/batch | lr: 9.391e-09 | loss: 10741.52 | perplexity:      inf\n",
      "  400/  718 batches | 1399.57 ms/batch | lr: 1.174e-08 | loss: 10541.40 | perplexity:      inf\n",
      "  500/  718 batches | 1441.64 ms/batch | lr: 9.391e-09 | loss: 10601.92 | perplexity:      inf\n",
      "  600/  718 batches | 1406.46 ms/batch | lr: 7.043e-09 | loss: 10595.70 | perplexity:      inf\n",
      "  700/  718 batches | 1397.53 ms/batch | lr: 9.978e-09 | loss: 10486.78 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.32 s | valid_loss: 1456.84 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  36) lr = 1.042e-08\n",
      "  100/  722 batches | 1431.61 ms/batch | lr: 1.1e-08 | loss: 10668.74 | perplexity:      inf\n",
      "  200/  722 batches | 1384.40 ms/batch | lr: 7.523e-09 | loss: 10286.79 | perplexity:      inf\n",
      "  300/  722 batches | 1424.10 ms/batch | lr: 1.042e-08 | loss: 10717.19 | perplexity:      inf\n",
      "  400/  722 batches | 1386.71 ms/batch | lr: 8.102e-09 | loss: 10419.97 | perplexity:      inf\n",
      "  500/  722 batches | 1395.95 ms/batch | lr: 1.331e-08 | loss: 10504.75 | perplexity:      inf\n",
      "  600/  722 batches | 1431.55 ms/batch | lr: 1.042e-08 | loss: 10522.91 | perplexity:      inf\n",
      "  700/  722 batches | 1449.64 ms/batch | lr: 1.157e-08 | loss: 10886.99 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.27 s | valid_loss: 1456.82 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  37) lr = 1.027e-08\n",
      "  100/  719 batches | 1428.83 ms/batch | lr: 1.142e-08 | loss: 10735.42 | perplexity:      inf\n",
      "  200/  719 batches | 1407.31 ms/batch | lr: 1.199e-08 | loss: 10492.81 | perplexity:      inf\n",
      "  300/  719 batches | 1477.68 ms/batch | lr: 1.256e-08 | loss: 10904.97 | perplexity:      inf\n",
      "  400/  719 batches | 1420.61 ms/batch | lr: 1.085e-08 | loss: 10650.43 | perplexity:      inf\n",
      "  500/  719 batches | 1397.65 ms/batch | lr: 8.562e-09 | loss: 10498.64 | perplexity:      inf\n",
      "  600/  719 batches | 1376.54 ms/batch | lr: 7.992e-09 | loss: 10365.07 | perplexity:      inf\n",
      "  700/  719 batches | 1441.39 ms/batch | lr: 1.085e-08 | loss: 10601.77 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1017.64 s | valid_loss: 1456.81 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  38) lr = 1.014e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  717 batches | 1432.10 ms/batch | lr: 1.239e-08 | loss: 10765.55 | perplexity:      inf\n",
      "  200/  717 batches | 1401.53 ms/batch | lr: 6.759e-09 | loss: 10529.14 | perplexity:      inf\n",
      "  300/  717 batches | 1408.29 ms/batch | lr: 1.014e-08 | loss: 10553.19 | perplexity:      inf\n",
      "  400/  717 batches | 1423.78 ms/batch | lr: 9.576e-09 | loss: 10510.77 | perplexity:      inf\n",
      "  500/  717 batches | 1405.60 ms/batch | lr: 1.07e-08 | loss: 10553.05 | perplexity:      inf\n",
      "  600/  717 batches | 1450.89 ms/batch | lr: 1.183e-08 | loss: 10886.53 | perplexity:      inf\n",
      "  700/  717 batches | 1496.92 ms/batch | lr: 1.239e-08 | loss: 10716.98 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1021.11 s | valid_loss: 1456.79 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  39) lr = 1.001e-08\n",
      "  100/  721 batches | 1458.64 ms/batch | lr: 7.784e-09 | loss: 10686.71 | perplexity:      inf\n",
      "  200/  721 batches | 1406.05 ms/batch | lr: 9.452e-09 | loss: 10601.81 | perplexity:      inf\n",
      "  300/  721 batches | 1442.61 ms/batch | lr: 9.452e-09 | loss: 10765.28 | perplexity:      inf\n",
      "  400/  721 batches | 1448.33 ms/batch | lr: 1.001e-08 | loss: 10771.50 | perplexity:      inf\n",
      "  500/  721 batches | 1404.92 ms/batch | lr: 8.896e-09 | loss: 10437.78 | perplexity:      inf\n",
      "  600/  721 batches | 1421.08 ms/batch | lr: 1.056e-08 | loss: 10607.64 | perplexity:      inf\n",
      "  700/  721 batches | 1359.39 ms/batch | lr: 6.672e-09 | loss: 10225.63 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.84 s | valid_loss: 1456.78 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  40) lr = 9.882e-09\n",
      "  100/  715 batches | 1481.30 ms/batch | lr: 8.235e-09 | loss: 10904.86 | perplexity:      inf\n",
      "  200/  715 batches | 1414.32 ms/batch | lr: 1.318e-08 | loss: 10625.99 | perplexity:      inf\n",
      "  300/  715 batches | 1415.48 ms/batch | lr: 8.235e-09 | loss: 10613.50 | perplexity:      inf\n",
      "  400/  715 batches | 1406.71 ms/batch | lr: 8.784e-09 | loss: 10559.23 | perplexity:      inf\n",
      "  500/  715 batches | 1379.43 ms/batch | lr: 7.137e-09 | loss: 10364.88 | perplexity:      inf\n",
      "  600/  715 batches | 1470.76 ms/batch | lr: 7.137e-09 | loss: 10801.43 | perplexity:      inf\n",
      "  700/  715 batches | 1429.13 ms/batch | lr: 7.137e-09 | loss: 10710.71 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1017.92 s | valid_loss: 1456.76 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  41) lr = 9.761e-09\n",
      "  100/  715 batches | 1411.02 ms/batch | lr: 9.219e-09 | loss: 10650.18 | perplexity:      inf\n",
      "  200/  715 batches | 1390.21 ms/batch | lr: 7.05e-09 | loss: 10425.71 | perplexity:      inf\n",
      "  300/  715 batches | 1485.14 ms/batch | lr: 9.219e-09 | loss: 10922.77 | perplexity:      inf\n",
      "  400/  715 batches | 1418.43 ms/batch | lr: 8.134e-09 | loss: 10668.20 | perplexity:      inf\n",
      "  500/  715 batches | 1428.26 ms/batch | lr: 1.139e-08 | loss: 10752.90 | perplexity:      inf\n",
      "  600/  715 batches | 1407.04 ms/batch | lr: 1.03e-08 | loss: 10583.05 | perplexity:      inf\n",
      "  700/  715 batches | 1451.19 ms/batch | lr: 9.761e-09 | loss: 10680.28 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.78 s | valid_loss: 1456.75 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  42) lr = 9.644e-09\n",
      "  100/  717 batches | 1417.56 ms/batch | lr: 8.037e-09 | loss: 10631.74 | perplexity:      inf\n",
      "  200/  717 batches | 1373.27 ms/batch | lr: 9.644e-09 | loss: 10364.84 | perplexity:      inf\n",
      "  300/  717 batches | 1413.17 ms/batch | lr: 8.037e-09 | loss: 10601.42 | perplexity:      inf\n",
      "  400/  717 batches | 1452.25 ms/batch | lr: 1.072e-08 | loss: 10704.49 | perplexity:      inf\n",
      "  500/  717 batches | 1441.74 ms/batch | lr: 1.286e-08 | loss: 10849.91 | perplexity:      inf\n",
      "  600/  717 batches | 1416.13 ms/batch | lr: 1.018e-08 | loss: 10673.86 | perplexity:      inf\n",
      "  700/  717 batches | 1408.21 ms/batch | lr: 1.018e-08 | loss: 10589.20 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1013.11 s | valid_loss: 1456.74 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  43) lr = 9.531e-09\n",
      "  100/  719 batches | 1469.40 ms/batch | lr: 1.006e-08 | loss: 10856.01 | perplexity:      inf\n",
      "  200/  719 batches | 1395.85 ms/batch | lr: 1.006e-08 | loss: 10516.55 | perplexity:      inf\n",
      "  300/  719 batches | 1407.07 ms/batch | lr: 8.472e-09 | loss: 10588.99 | perplexity:      inf\n",
      "  400/  719 batches | 1462.68 ms/batch | lr: 1.112e-08 | loss: 10959.05 | perplexity:      inf\n",
      "  500/  719 batches | 1436.44 ms/batch | lr: 9.002e-09 | loss: 10601.17 | perplexity:      inf\n",
      "  600/  719 batches | 1371.81 ms/batch | lr: 7.943e-09 | loss: 10358.46 | perplexity:      inf\n",
      "  700/  719 batches | 1367.54 ms/batch | lr: 1.112e-08 | loss: 10273.71 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.37 s | valid_loss: 1456.72 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  44) lr = 9.422e-09\n",
      "  100/  717 batches | 1424.45 ms/batch | lr: 1.099e-08 | loss: 10680.11 | perplexity:      inf\n",
      "  200/  717 batches | 1446.52 ms/batch | lr: 8.375e-09 | loss: 10661.93 | perplexity:      inf\n",
      "  300/  717 batches | 1396.04 ms/batch | lr: 8.375e-09 | loss: 10540.44 | perplexity:      inf\n",
      "  400/  717 batches | 1414.48 ms/batch | lr: 7.852e-09 | loss: 10661.78 | perplexity:      inf\n",
      "  500/  717 batches | 1382.23 ms/batch | lr: 8.899e-09 | loss: 10394.74 | perplexity:      inf\n",
      "  600/  717 batches | 1460.25 ms/batch | lr: 9.422e-09 | loss: 10752.63 | perplexity:      inf\n",
      "  700/  717 batches | 1422.84 ms/batch | lr: 7.852e-09 | loss: 10704.20 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.28 s | valid_loss: 1456.71 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  45) lr = 9.317e-09\n",
      "  100/  713 batches | 1457.19 ms/batch | lr: 8.799e-09 | loss: 10983.23 | perplexity:      inf\n",
      "  200/  713 batches | 1427.87 ms/batch | lr: 7.247e-09 | loss: 10710.25 | perplexity:      inf\n",
      "  300/  713 batches | 1434.50 ms/batch | lr: 8.799e-09 | loss: 10534.26 | perplexity:      inf\n",
      "  400/  713 batches | 1422.71 ms/batch | lr: 1.087e-08 | loss: 10722.43 | perplexity:      inf\n",
      "  500/  713 batches | 1425.10 ms/batch | lr: 7.247e-09 | loss: 10728.27 | perplexity:      inf\n",
      "  600/  713 batches | 1414.32 ms/batch | lr: 1.19e-08 | loss: 10612.93 | perplexity:      inf\n",
      "  700/  713 batches | 1436.26 ms/batch | lr: 9.317e-09 | loss: 10528.19 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.71 s | valid_loss: 1456.70 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  46) lr = 9.215e-09\n",
      "  100/  713 batches | 1422.91 ms/batch | lr: 9.727e-09 | loss: 10734.47 | perplexity:      inf\n",
      "  200/  713 batches | 1397.75 ms/batch | lr: 8.703e-09 | loss: 10540.39 | perplexity:      inf\n",
      "  300/  713 batches | 1393.15 ms/batch | lr: 7.679e-09 | loss: 10473.59 | perplexity:      inf\n",
      "  400/  713 batches | 1475.41 ms/batch | lr: 8.703e-09 | loss: 10831.44 | perplexity:      inf\n",
      "  500/  713 batches | 1417.75 ms/batch | lr: 8.191e-09 | loss: 10679.52 | perplexity:      inf\n",
      "  600/  713 batches | 1441.93 ms/batch | lr: 9.215e-09 | loss: 10843.34 | perplexity:      inf\n",
      "  700/  713 batches | 1425.69 ms/batch | lr: 1.126e-08 | loss: 10716.18 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1012.49 s | valid_loss: 1456.68 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  47) lr = 9.117e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  722 batches | 1460.15 ms/batch | lr: 8.61e-09 | loss: 10782.91 | perplexity:      inf\n",
      "  200/  722 batches | 1414.68 ms/batch | lr: 7.597e-09 | loss: 10643.45 | perplexity:      inf\n",
      "  300/  722 batches | 1400.78 ms/batch | lr: 9.623e-09 | loss: 10552.29 | perplexity:      inf\n",
      "  400/  722 batches | 1410.46 ms/batch | lr: 8.104e-09 | loss: 10595.02 | perplexity:      inf\n",
      "  500/  722 batches | 1417.48 ms/batch | lr: 9.117e-09 | loss: 10418.76 | perplexity:      inf\n",
      "  600/  722 batches | 1386.64 ms/batch | lr: 6.584e-09 | loss: 10436.99 | perplexity:      inf\n",
      "  700/  722 batches | 1375.35 ms/batch | lr: 8.104e-09 | loss: 10364.35 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.08 s | valid_loss: 1456.67 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  48) lr = 9.021e-09\n",
      "  100/  720 batches | 1427.51 ms/batch | lr: 8.019e-09 | loss: 10716.11 | perplexity:      inf\n",
      "  200/  720 batches | 1427.76 ms/batch | lr: 9.021e-09 | loss: 10461.44 | perplexity:      inf\n",
      "  300/  720 batches | 1422.85 ms/batch | lr: 1.103e-08 | loss: 10703.91 | perplexity:      inf\n",
      "  400/  720 batches | 1398.94 ms/batch | lr: 9.522e-09 | loss: 10534.14 | perplexity:      inf\n",
      "  500/  720 batches | 1377.39 ms/batch | lr: 9.021e-09 | loss: 10315.61 | perplexity:      inf\n",
      "  600/  720 batches | 1426.26 ms/batch | lr: 1.002e-08 | loss: 10503.59 | perplexity:      inf\n",
      "  700/  720 batches | 1432.11 ms/batch | lr: 1.002e-08 | loss: 10770.51 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1017.31 s | valid_loss: 1456.66 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  49) lr = 8.929e-09\n",
      "  100/  713 batches | 1414.38 ms/batch | lr: 9.921e-09 | loss: 10649.32 | perplexity:      inf\n",
      "  200/  713 batches | 1429.68 ms/batch | lr: 8.929e-09 | loss: 10709.84 | perplexity:      inf\n",
      "  300/  713 batches | 1450.18 ms/batch | lr: 9.425e-09 | loss: 10673.45 | perplexity:      inf\n",
      "  400/  713 batches | 1414.90 ms/batch | lr: 8.929e-09 | loss: 10618.92 | perplexity:      inf\n",
      "  500/  713 batches | 1422.39 ms/batch | lr: 7.937e-09 | loss: 10721.85 | perplexity:      inf\n",
      "  600/  713 batches | 1470.19 ms/batch | lr: 8.929e-09 | loss: 10976.36 | perplexity:      inf\n",
      "  700/  713 batches | 1426.94 ms/batch | lr: 7.44e-09 | loss: 10509.70 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1017.49 s | valid_loss: 1456.64 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  50) lr = 8.839e-09\n",
      "  100/  723 batches | 1436.65 ms/batch | lr: 8.348e-09 | loss: 10806.86 | perplexity:      inf\n",
      "  200/  723 batches | 1384.49 ms/batch | lr: 7.366e-09 | loss: 10412.76 | perplexity:      inf\n",
      "  300/  723 batches | 1404.13 ms/batch | lr: 1.031e-08 | loss: 10521.85 | perplexity:      inf\n",
      "  400/  723 batches | 1414.53 ms/batch | lr: 9.821e-09 | loss: 10442.95 | perplexity:      inf\n",
      "  500/  723 batches | 1381.24 ms/batch | lr: 8.348e-09 | loss: 10424.62 | perplexity:      inf\n",
      "  600/  723 batches | 1357.39 ms/batch | lr: 6.875e-09 | loss: 10242.75 | perplexity:      inf\n",
      "  700/  723 batches | 1455.23 ms/batch | lr: 9.33e-09 | loss: 10891.59 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1012.70 s | valid_loss: 1456.63 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  51) lr = 8.752e-09\n",
      "  100/  719 batches | 1414.51 ms/batch | lr: 8.752e-09 | loss: 10497.65 | perplexity:      inf\n",
      "  200/  719 batches | 1388.59 ms/batch | lr: 9.238e-09 | loss: 10442.98 | perplexity:      inf\n",
      "  300/  719 batches | 1411.35 ms/batch | lr: 1.07e-08 | loss: 10636.90 | perplexity:      inf\n",
      "  400/  719 batches | 1458.11 ms/batch | lr: 5.348e-09 | loss: 10909.67 | perplexity:      inf\n",
      "  500/  719 batches | 1459.74 ms/batch | lr: 7.293e-09 | loss: 10764.10 | perplexity:      inf\n",
      "  600/  719 batches | 1396.63 ms/batch | lr: 6.321e-09 | loss: 10527.47 | perplexity:      inf\n",
      "  700/  719 batches | 1397.31 ms/batch | lr: 8.266e-09 | loss: 10521.64 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1014.52 s | valid_loss: 1456.62 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  52) lr = 8.667e-09\n",
      "  100/  718 batches | 1396.32 ms/batch | lr: 1.011e-08 | loss: 10430.73 | perplexity:      inf\n",
      "  200/  718 batches | 1422.70 ms/batch | lr: 8.186e-09 | loss: 10552.09 | perplexity:      inf\n",
      "  300/  718 batches | 1435.09 ms/batch | lr: 8.667e-09 | loss: 10800.48 | perplexity:      inf\n",
      "  400/  718 batches | 1452.22 ms/batch | lr: 1.059e-08 | loss: 10915.80 | perplexity:      inf\n",
      "  500/  718 batches | 1408.31 ms/batch | lr: 7.223e-09 | loss: 10570.00 | perplexity:      inf\n",
      "  600/  718 batches | 1397.75 ms/batch | lr: 7.704e-09 | loss: 10284.93 | perplexity:      inf\n",
      "  700/  718 batches | 1434.28 ms/batch | lr: 1.156e-08 | loss: 10745.92 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.82 s | valid_loss: 1456.61 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  53) lr = 8.585e-09\n",
      "  100/  712 batches | 1406.65 ms/batch | lr: 7.631e-09 | loss: 10618.57 | perplexity:      inf\n",
      "  200/  712 batches | 1376.93 ms/batch | lr: 9.539e-09 | loss: 10321.47 | perplexity:      inf\n",
      "  300/  712 batches | 1466.30 ms/batch | lr: 9.062e-09 | loss: 10800.33 | perplexity:      inf\n",
      "  400/  712 batches | 1436.82 ms/batch | lr: 7.631e-09 | loss: 10800.39 | perplexity:      inf\n",
      "  500/  712 batches | 1441.47 ms/batch | lr: 8.585e-09 | loss: 10824.55 | perplexity:      inf\n",
      "  600/  712 batches | 1458.47 ms/batch | lr: 7.631e-09 | loss: 10903.35 | perplexity:      inf\n",
      "  700/  712 batches | 1445.13 ms/batch | lr: 9.062e-09 | loss: 10673.10 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.41 s | valid_loss: 1456.59 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  54) lr = 8.505e-09\n",
      "  100/  725 batches | 1397.91 ms/batch | lr: 6.143e-09 | loss: 10545.80 | perplexity:      inf\n",
      "  200/  725 batches | 1390.65 ms/batch | lr: 7.088e-09 | loss: 10460.91 | perplexity:      inf\n",
      "  300/  725 batches | 1409.12 ms/batch | lr: 8.978e-09 | loss: 10563.91 | perplexity:      inf\n",
      "  400/  725 batches | 1414.68 ms/batch | lr: 7.088e-09 | loss: 10418.40 | perplexity:      inf\n",
      "  500/  725 batches | 1378.60 ms/batch | lr: 8.033e-09 | loss: 10381.83 | perplexity:      inf\n",
      "  600/  725 batches | 1378.80 ms/batch | lr: 9.45e-09 | loss: 10363.65 | perplexity:      inf\n",
      "  700/  725 batches | 1428.46 ms/batch | lr: 6.143e-09 | loss: 10703.16 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1013.30 s | valid_loss: 1456.58 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  55) lr = 8.427e-09\n",
      "  100/  719 batches | 1442.45 ms/batch | lr: 8.896e-09 | loss: 10715.43 | perplexity:      inf\n",
      "  200/  719 batches | 1408.61 ms/batch | lr: 7.959e-09 | loss: 10618.48 | perplexity:      inf\n",
      "  300/  719 batches | 1395.37 ms/batch | lr: 7.491e-09 | loss: 10484.86 | perplexity:      inf\n",
      "  400/  719 batches | 1402.08 ms/batch | lr: 8.896e-09 | loss: 10533.52 | perplexity:      inf\n",
      "  500/  719 batches | 1453.51 ms/batch | lr: 7.023e-09 | loss: 10709.19 | perplexity:      inf\n",
      "  600/  719 batches | 1422.61 ms/batch | lr: 8.896e-09 | loss: 10721.18 | perplexity:      inf\n",
      "  700/  719 batches | 1387.95 ms/batch | lr: 6.555e-09 | loss: 10472.86 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1014.15 s | valid_loss: 1456.57 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  56) lr = 8.352e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  719 batches | 1477.25 ms/batch | lr: 7.888e-09 | loss: 10848.84 | perplexity:      inf\n",
      "  200/  719 batches | 1450.26 ms/batch | lr: 8.816e-09 | loss: 10703.31 | perplexity:      inf\n",
      "  300/  719 batches | 1405.88 ms/batch | lr: 6.96e-09 | loss: 10606.11 | perplexity:      inf\n",
      "  400/  719 batches | 1399.56 ms/batch | lr: 6.96e-09 | loss: 10533.39 | perplexity:      inf\n",
      "  500/  719 batches | 1386.50 ms/batch | lr: 7.424e-09 | loss: 10405.91 | perplexity:      inf\n",
      "  600/  719 batches | 1437.57 ms/batch | lr: 8.352e-09 | loss: 10569.56 | perplexity:      inf\n",
      "  700/  719 batches | 1399.37 ms/batch | lr: 7.424e-09 | loss: 10521.16 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.97 s | valid_loss: 1456.56 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  57) lr = 8.278e-09\n",
      "  100/  716 batches | 1394.11 ms/batch | lr: 5.059e-09 | loss: 10484.92 | perplexity:      inf\n",
      "  200/  716 batches | 1441.64 ms/batch | lr: 5.979e-09 | loss: 10715.30 | perplexity:      inf\n",
      "  300/  716 batches | 1478.96 ms/batch | lr: 6.899e-09 | loss: 10963.80 | perplexity:      inf\n",
      "  400/  716 batches | 1413.17 ms/batch | lr: 5.519e-09 | loss: 10624.32 | perplexity:      inf\n",
      "  500/  716 batches | 1425.46 ms/batch | lr: 7.818e-09 | loss: 10715.04 | perplexity:      inf\n",
      "  600/  716 batches | 1398.66 ms/batch | lr: 9.198e-09 | loss: 10478.50 | perplexity:      inf\n",
      "  700/  716 batches | 1427.89 ms/batch | lr: 8.738e-09 | loss: 10551.60 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1016.97 s | valid_loss: 1456.55 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  58) lr = 8.207e-09\n",
      "  100/  719 batches | 1419.53 ms/batch | lr: 6.839e-09 | loss: 10684.89 | perplexity:      inf\n",
      "  200/  719 batches | 1380.66 ms/batch | lr: 7.295e-09 | loss: 10418.11 | perplexity:      inf\n",
      "  300/  719 batches | 1386.67 ms/batch | lr: 9.119e-09 | loss: 10363.48 | perplexity:      inf\n",
      "  400/  719 batches | 1438.62 ms/batch | lr: 7.295e-09 | loss: 10666.61 | perplexity:      inf\n",
      "  500/  719 batches | 1414.28 ms/batch | lr: 8.207e-09 | loss: 10611.90 | perplexity:      inf\n",
      "  600/  719 batches | 1440.63 ms/batch | lr: 7.751e-09 | loss: 10848.28 | perplexity:      inf\n",
      "  700/  719 batches | 1429.64 ms/batch | lr: 5.471e-09 | loss: 10709.02 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1013.05 s | valid_loss: 1456.54 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  59) lr = 8.137e-09\n",
      "  100/  717 batches | 1420.82 ms/batch | lr: 6.329e-09 | loss: 10666.60 | perplexity:      inf\n",
      "  200/  717 batches | 1448.59 ms/batch | lr: 7.685e-09 | loss: 10903.15 | perplexity:      inf\n",
      "  300/  717 batches | 1374.50 ms/batch | lr: 6.781e-09 | loss: 10381.43 | perplexity:      inf\n",
      "  400/  717 batches | 1416.32 ms/batch | lr: 7.685e-09 | loss: 10569.65 | perplexity:      inf\n",
      "  500/  717 batches | 1414.19 ms/batch | lr: 9.493e-09 | loss: 10490.60 | perplexity:      inf\n",
      "  600/  717 batches | 1451.98 ms/batch | lr: 9.493e-09 | loss: 10866.37 | perplexity:      inf\n",
      "  700/  717 batches | 1394.19 ms/batch | lr: 7.233e-09 | loss: 10514.91 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1012.61 s | valid_loss: 1456.52 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  60) lr = 8.069e-09\n",
      "  100/  723 batches | 1418.61 ms/batch | lr: 8.965e-09 | loss: 10527.07 | perplexity:      inf\n",
      "  200/  723 batches | 1412.79 ms/batch | lr: 8.517e-09 | loss: 10545.28 | perplexity:      inf\n",
      "  300/  723 batches | 1383.65 ms/batch | lr: 8.069e-09 | loss: 10411.88 | perplexity:      inf\n",
      "  400/  723 batches | 1402.60 ms/batch | lr: 8.965e-09 | loss: 10563.38 | perplexity:      inf\n",
      "  500/  723 batches | 1427.03 ms/batch | lr: 6.276e-09 | loss: 10690.51 | perplexity:      inf\n",
      "  600/  723 batches | 1413.36 ms/batch | lr: 8.517e-09 | loss: 10435.84 | perplexity:      inf\n",
      "  700/  723 batches | 1411.85 ms/batch | lr: 6.276e-09 | loss: 10648.16 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.22 s | valid_loss: 1456.51 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  61) lr = 8.002e-09\n",
      "  100/  724 batches | 1400.97 ms/batch | lr: 5.335e-09 | loss: 10569.39 | perplexity:      inf\n",
      "  200/  724 batches | 1396.15 ms/batch | lr: 8.891e-09 | loss: 10375.46 | perplexity:      inf\n",
      "  300/  724 batches | 1448.23 ms/batch | lr: 7.113e-09 | loss: 10823.91 | perplexity:      inf\n",
      "  400/  724 batches | 1365.29 ms/batch | lr: 8.447e-09 | loss: 10290.39 | perplexity:      inf\n",
      "  500/  724 batches | 1400.17 ms/batch | lr: 7.558e-09 | loss: 10551.03 | perplexity:      inf\n",
      "  600/  724 batches | 1369.79 ms/batch | lr: 7.113e-09 | loss: 10296.33 | perplexity:      inf\n",
      "  700/  724 batches | 1454.43 ms/batch | lr: 7.558e-09 | loss: 10648.08 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1015.41 s | valid_loss: 1456.50 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  62) lr = 7.938e-09\n",
      "  100/  717 batches | 1397.37 ms/batch | lr: 8.378e-09 | loss: 10551.19 | perplexity:      inf\n",
      "  200/  717 batches | 1467.24 ms/batch | lr: 6.174e-09 | loss: 11042.39 | perplexity:      inf\n",
      "  300/  717 batches | 1425.96 ms/batch | lr: 8.378e-09 | loss: 10599.41 | perplexity:      inf\n",
      "  400/  717 batches | 1428.73 ms/batch | lr: 7.938e-09 | loss: 10654.29 | perplexity:      inf\n",
      "  500/  717 batches | 1375.14 ms/batch | lr: 7.497e-09 | loss: 10368.95 | perplexity:      inf\n",
      "  600/  717 batches | 1429.03 ms/batch | lr: 5.292e-09 | loss: 10538.73 | perplexity:      inf\n",
      "  700/  717 batches | 1443.27 ms/batch | lr: 6.174e-09 | loss: 10599.66 | perplexity:      inf\n",
      "-------------------------------------------------------------------------------------\n",
      "Elapsed time: 1018.39 s | valid_loss: 1456.49 | valid_perplexity:      inf\n",
      "=====================================================================================\n",
      "\n",
      "\n",
      "Epoch  63) lr = 7.874e-09\n",
      "  100/  718 batches | 1470.13 ms/batch | lr: 8.749e-09 | loss: 10763.31 | perplexity:      inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d2cfeca7f9d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {:3d}) lr = {:0.4g}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-5ec62ce4130b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Propagate loss gradient backwards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Clip gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WIDTH = 114\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}) lr = {:0.4g}{}'.format(epoch+1, np.mean(lr_scheduler.get_lr()), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    (stat, data, targets, states, nstates) = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:5.2f} sec | max abs wt: {:5.2g} | valid_loss: {:5.2f} | valid_perplexity: {:8.2f}'.format(\n",
    "        elapsed, max_param, val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    enc_states, attn_states, dec_states = states\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoder(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoder(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:8.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some random words\n",
    "Just, like, y'know, as a test or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "\n",
    "gstates = model.init_states(1)\n",
    "cur_word = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile = True)\n",
    "gen_text = [int(cur_word)]\n",
    "\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(cur_word, gstates)\n",
    "    word_weights = output.squeeze().exp()\n",
    "    word_idx = torch.multinomial(word_weights, 1).data[0]\n",
    "    gen_text.append(int(word_idx))\n",
    "    cur_word.data.fill_(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the words\n",
    "print(' '.join([corpus.dictionary.idx2word[i] for i in gen_text]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
