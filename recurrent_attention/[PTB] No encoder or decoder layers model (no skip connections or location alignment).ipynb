{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from model import *\n",
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead stuff\n",
    "\n",
    "Helper functions for batching, resetting hidden states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 4\n",
    "lr = 0.02\n",
    "warmup_steps = 10\n",
    "decay_factor = 0.5\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 100\n",
    "log_interval = 150  # Print log every `log_interval` batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 512\n",
    "h_size = 256\n",
    "decode_size = 256\n",
    "n_enc_layers = 0\n",
    "attn_rnn_layers = 1\n",
    "n_dec_layers = 0\n",
    "smooth_align = True\n",
    "align_location = False\n",
    "skip_connections = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate scheduler sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ef92d05358>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef92d7bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps*2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.4g:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps*2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 9003793\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    src_vocab = ntokens, tgt_vocab = ntokens, embed_size = embed_size,\n",
    "    h_size = h_size, decode_size = decode_size, n_enc_layers = n_enc_layers,\n",
    "    attn_rnn_layers = attn_rnn_layers, n_dec_layers = n_dec_layers,\n",
    "    align_location = align_location, skip_connections = skip_connections,\n",
    "    smooth_align = smooth_align, dropout = dropout\n",
    ")\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "eval_criterion = LabelSmoothing(ntokens, smoothing = 0)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, decay_factor, optimizer)\n",
    "# Reference\n",
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12562, 74]), torch.Size([7376, 10]), torch.Size([8243, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100) lr = 2.795e-05 (warmup)\n",
      " b 150/719 >> 1804.5 ms/b | lr:  2.9e-05 | grad norm: 2.90 | max abs grad:   0.167 | loss: 7.46 | perp.: 1728.68\n",
      " b 300/719 >> 1746.0 ms/b | lr:  2.8e-05 | grad norm: 2.38 | max abs grad:   0.137 | loss: 6.27 | perp.: 526.37\n",
      " b 450/719 >> 1804.1 ms/b | lr:  2.6e-05 | grad norm: 2.44 | max abs grad:   0.107 | loss: 6.07 | perp.: 431.83\n",
      " b 600/719 >> 1766.1 ms/b | lr:  2.7e-05 | grad norm: 2.33 | max abs grad:   0.154 | loss: 5.97 | perp.: 389.61\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1275.17 sec | train_loss:  6.34 | train_perp: 568.49 | valid_loss:  5.78 | valid_perp.: 323.43\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   2/100) lr = 5.59e-05 (warmup)\n",
      " b 150/722 >> 1679.3 ms/b | lr:  4.6e-05 | grad norm: 3.45 | max abs grad:   0.238 | loss: 5.79 | perp.: 328.24\n",
      " b 300/722 >> 1785.8 ms/b | lr:  5.9e-05 | grad norm: 2.28 | max abs grad:   0.203 | loss: 5.63 | perp.: 277.52\n",
      " b 450/722 >> 1708.0 ms/b | lr:  6.2e-05 | grad norm: 1.77 | max abs grad:   0.085 | loss: 5.52 | perp.: 250.14\n",
      " b 600/722 >> 1737.0 ms/b | lr:  5.1e-05 | grad norm: 3.22 | max abs grad:   0.311 | loss: 5.44 | perp.: 230.45\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1244.72 sec | train_loss:  5.56 | train_perp: 259.12 | valid_loss:  5.31 | valid_perp.: 202.33\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   3/100) lr = 8.385e-05 (warmup)\n",
      " b 150/713 >> 1794.7 ms/b | lr:  8.4e-05 | grad norm: 2.01 | max abs grad:   0.139 | loss: 5.28 | perp.: 197.03\n",
      " b 300/713 >> 1742.0 ms/b | lr:  8.4e-05 | grad norm: 2.57 | max abs grad:   0.227 | loss: 5.16 | perp.: 174.54\n",
      " b 450/713 >> 1756.1 ms/b | lr:  9.5e-05 | grad norm: 2.54 | max abs grad:   0.237 | loss: 5.07 | perp.: 159.87\n",
      " b 600/713 >> 1733.5 ms/b | lr:  8.1e-05 | grad norm: 2.41 | max abs grad:   0.198 | loss: 5.00 | perp.: 147.89\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1254.84 sec | train_loss:  5.11 | train_perp: 165.13 | valid_loss:  4.92 | valid_perp.: 137.22\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   4/100) lr = 0.0001118 (warmup)\n",
      " b 150/723 >> 1724.9 ms/b | lr:   0.0001 | grad norm: 4.79 | max abs grad:   0.620 | loss: 4.83 | perp.: 125.28\n",
      " b 300/723 >> 1735.8 ms/b | lr:  0.00012 | grad norm: 2.81 | max abs grad:   0.241 | loss: 4.75 | perp.: 116.08\n",
      " b 450/723 >> 1710.8 ms/b | lr:  0.00011 | grad norm: 2.90 | max abs grad:   0.232 | loss: 4.68 | perp.: 107.62\n",
      " b 600/723 >> 1774.7 ms/b | lr:  0.00011 | grad norm: 3.75 | max abs grad:   0.400 | loss: 4.65 | perp.: 104.24\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1250.57 sec | train_loss:  4.71 | train_perp: 111.48 | valid_loss:  4.63 | valid_perp.: 102.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   5/100) lr = 0.0001398 (warmup)\n",
      " b 150/718 >> 1783.6 ms/b | lr:  0.00014 | grad norm: 3.57 | max abs grad:   0.412 | loss: 4.52 | perp.:  92.06\n",
      " b 300/718 >> 1723.1 ms/b | lr:  0.00015 | grad norm: 3.13 | max abs grad:   0.345 | loss: 4.45 | perp.:  85.70\n",
      " b 450/718 >> 1761.3 ms/b | lr:  0.00014 | grad norm: 4.42 | max abs grad:   0.549 | loss: 4.42 | perp.:  83.42\n",
      " b 600/718 >> 1760.3 ms/b | lr:  0.00013 | grad norm: 4.74 | max abs grad:   0.627 | loss: 4.42 | perp.:  82.81\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1251.84 sec | train_loss:  4.45 | train_perp:  85.25 | valid_loss:  4.45 | valid_perp.:  86.00\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   6/100) lr = 0.0001677 (warmup)\n",
      " b 150/721 >> 1759.3 ms/b | lr:  0.00019 | grad norm: 3.21 | max abs grad:   0.339 | loss: 4.31 | perp.:  74.27\n",
      " b 300/721 >> 1737.8 ms/b | lr:  0.00017 | grad norm: 3.54 | max abs grad:   0.336 | loss: 4.26 | perp.:  71.08\n",
      " b 450/721 >> 1753.7 ms/b | lr:  0.00015 | grad norm: 3.91 | max abs grad:   0.413 | loss: 4.25 | perp.:  69.94\n",
      " b 600/721 >> 1702.2 ms/b | lr:  0.00019 | grad norm: 3.31 | max abs grad:   0.317 | loss: 4.22 | perp.:  68.18\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1251.98 sec | train_loss:  4.27 | train_perp:  71.23 | valid_loss:  4.34 | valid_perp.:  76.78\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   7/100) lr = 0.0001957 (warmup)\n",
      " b 150/712 >> 1767.0 ms/b | lr:  0.00019 | grad norm: 3.13 | max abs grad:   0.281 | loss: 4.16 | perp.:  63.85\n",
      " b 300/712 >> 1755.6 ms/b | lr:  0.00018 | grad norm: 3.70 | max abs grad:   0.427 | loss: 4.13 | perp.:  61.87\n",
      " b 450/712 >> 1766.6 ms/b | lr:  0.00018 | grad norm: 5.45 | max abs grad:   0.776 | loss: 4.13 | perp.:  62.13\n",
      " b 600/712 >> 1772.2 ms/b | lr:  0.00015 | grad norm: 10.91 | max abs grad:   1.364 | loss: 4.13 | perp.:  62.14\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1257.27 sec | train_loss:  4.15 | train_perp:  63.24 | valid_loss:  4.26 | valid_perp.:  71.11\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   8/100) lr = 0.0002236 (warmup)\n",
      " b 150/717 >> 1753.0 ms/b | lr:  0.00025 | grad norm: 3.32 | max abs grad:   0.463 | loss: 4.06 | perp.:  57.81\n",
      " b 300/717 >> 1830.4 ms/b | lr:  0.00019 | grad norm: 7.00 | max abs grad:   0.924 | loss: 4.02 | perp.:  55.85\n",
      " b 450/717 >> 1878.9 ms/b | lr:  0.00022 | grad norm: 4.90 | max abs grad:   0.641 | loss: 4.04 | perp.:  56.72\n",
      " b 600/717 >> 1789.7 ms/b | lr:  0.00025 | grad norm: 3.05 | max abs grad:   0.384 | loss: 4.05 | perp.:  57.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1287.37 sec | train_loss:  4.06 | train_perp:  57.69 | valid_loss:  4.20 | valid_perp.:  66.86\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   9/100) lr = 0.0002516 (warmup)\n",
      " b 150/719 >> 1721.2 ms/b | lr:  0.00022 | grad norm: 3.99 | max abs grad:   0.501 | loss: 4.00 | perp.:  54.53\n",
      " b 300/719 >> 1675.0 ms/b | lr:  0.00026 | grad norm: 3.51 | max abs grad:   0.369 | loss: 3.99 | perp.:  53.80\n",
      " b 450/719 >> 1656.9 ms/b | lr:  0.00024 | grad norm: 3.84 | max abs grad:   0.493 | loss: 3.98 | perp.:  53.29\n",
      " b 600/719 >> 1714.3 ms/b | lr:  0.00022 | grad norm: 6.85 | max abs grad:   0.902 | loss: 4.00 | perp.:  54.49\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1218.50 sec | train_loss:  4.00 | train_perp:  54.61 | valid_loss:  4.17 | valid_perp.:  64.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  10/100) lr = 0.0002795 (warmup)\n",
      " b 150/719 >> 1706.6 ms/b | lr:  0.00031 | grad norm: 3.10 | max abs grad:   0.454 | loss: 3.94 | perp.:  51.62\n",
      " b 300/719 >> 1667.5 ms/b | lr:  0.00029 | grad norm: 3.10 | max abs grad:   0.396 | loss: 3.92 | perp.:  50.48\n",
      " b 450/719 >> 1715.8 ms/b | lr:  0.00029 | grad norm: 3.32 | max abs grad:   0.570 | loss: 3.95 | perp.:  52.05\n",
      " b 600/719 >> 1640.5 ms/b | lr:  0.00029 | grad norm: 3.84 | max abs grad:   0.601 | loss: 3.93 | perp.:  51.06\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1207.18 sec | train_loss:  3.95 | train_perp:  52.13 | valid_loss:  4.14 | valid_perp.:  63.03\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  11/100) lr = 0.0002665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/718 >> 1647.2 ms/b | lr:  0.00027 | grad norm: 3.51 | max abs grad:   0.590 | loss: 3.87 | perp.:  48.13\n",
      " b 300/718 >> 1681.3 ms/b | lr:  0.00023 | grad norm: 6.90 | max abs grad:   0.838 | loss: 3.89 | perp.:  49.01\n",
      " b 450/718 >> 1639.3 ms/b | lr:  0.00025 | grad norm: 4.08 | max abs grad:   0.683 | loss: 3.88 | perp.:  48.41\n",
      " b 600/718 >> 1696.6 ms/b | lr:  0.00025 | grad norm: 5.97 | max abs grad:   0.955 | loss: 3.92 | perp.:  50.31\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1195.78 sec | train_loss:  3.90 | train_perp:  49.63 | valid_loss:  4.10 | valid_perp.:  60.49\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  12/100) lr = 0.0002552\n",
      " b 150/713 >> 1704.0 ms/b | lr:  0.00025 | grad norm: 3.94 | max abs grad:   0.640 | loss: 3.86 | perp.:  47.57\n",
      " b 300/713 >> 1649.1 ms/b | lr:  0.00024 | grad norm: 4.42 | max abs grad:   0.839 | loss: 3.88 | perp.:  48.25\n",
      " b 450/713 >> 1696.8 ms/b | lr:  0.00026 | grad norm: 4.24 | max abs grad:   0.857 | loss: 3.86 | perp.:  47.60\n",
      " b 600/713 >> 1709.0 ms/b | lr:  0.00023 | grad norm: 5.92 | max abs grad:   1.085 | loss: 3.88 | perp.:  48.30\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1198.01 sec | train_loss:  3.88 | train_perp:  48.35 | valid_loss:  4.10 | valid_perp.:  60.11\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  13/100) lr = 0.0002451\n",
      " b 150/715 >> 1702.7 ms/b | lr:  0.00026 | grad norm: 4.18 | max abs grad:   1.102 | loss: 3.84 | perp.:  46.39\n",
      " b 300/715 >> 1683.8 ms/b | lr:  0.00025 | grad norm: 4.52 | max abs grad:   0.864 | loss: 3.82 | perp.:  45.62\n",
      " b 450/715 >> 1665.0 ms/b | lr:  0.00022 | grad norm: 8.61 | max abs grad:   1.413 | loss: 3.81 | perp.:  45.15\n",
      " b 600/715 >> 1690.6 ms/b | lr:  0.00025 | grad norm: 5.13 | max abs grad:   1.126 | loss: 3.85 | perp.:  46.77\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1198.50 sec | train_loss:  3.84 | train_perp:  46.60 | valid_loss:  4.06 | valid_perp.:  57.92\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  14/100) lr = 0.0002362\n",
      " b 150/721 >> 1697.6 ms/b | lr:  0.00027 | grad norm: 5.08 | max abs grad:   0.862 | loss: 3.79 | perp.:  44.13\n",
      " b 300/721 >> 1678.2 ms/b | lr:  0.00021 | grad norm: 6.81 | max abs grad:   1.423 | loss: 3.78 | perp.:  43.97\n",
      " b 450/721 >> 1680.4 ms/b | lr:  0.00026 | grad norm: 5.32 | max abs grad:   1.735 | loss: 3.78 | perp.:  43.73\n",
      " b 600/721 >> 1667.2 ms/b | lr:  0.00026 | grad norm: 4.72 | max abs grad:   0.982 | loss: 3.80 | perp.:  44.60\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1205.25 sec | train_loss:  3.80 | train_perp:  44.75 | valid_loss:  4.04 | valid_perp.:  56.92\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  15/100) lr = 0.0002282\n",
      " b 150/726 >> 1740.8 ms/b | lr:  0.00023 | grad norm: 5.92 | max abs grad:   1.346 | loss: 3.79 | perp.:  44.37\n",
      " b 300/726 >> 1658.0 ms/b | lr:  0.00023 | grad norm: 7.17 | max abs grad:   1.841 | loss: 3.80 | perp.:  44.62\n",
      " b 450/726 >> 1623.6 ms/b | lr:   0.0002 | grad norm: 8.86 | max abs grad:   2.153 | loss: 3.74 | perp.:  42.17\n",
      " b 600/726 >> 1724.3 ms/b | lr:  0.00022 | grad norm: 6.00 | max abs grad:   1.079 | loss: 3.75 | perp.:  42.65\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1241.83 sec | train_loss:  3.78 | train_perp:  43.85 | valid_loss:  4.02 | valid_perp.:  55.49\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  16/100) lr = 0.000221\n",
      " b 150/715 >> 1904.9 ms/b | lr:  0.00023 | grad norm: 5.66 | max abs grad:   1.051 | loss: 3.75 | perp.:  42.72\n",
      " b 300/715 >> 1892.5 ms/b | lr:  0.00024 | grad norm: 6.08 | max abs grad:   1.219 | loss: 3.75 | perp.:  42.48\n",
      " b 450/715 >> 1898.8 ms/b | lr:  0.00023 | grad norm: 6.52 | max abs grad:   1.415 | loss: 3.76 | perp.:  42.74\n",
      " b 600/715 >> 1837.1 ms/b | lr:   0.0002 | grad norm: 8.04 | max abs grad:   0.942 | loss: 3.75 | perp.:  42.51\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1347.48 sec | train_loss:  3.77 | train_perp:  43.18 | valid_loss:  4.00 | valid_perp.:  54.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  17/100) lr = 0.0002144\n",
      " b 150/711 >> 1921.3 ms/b | lr:  0.00019 | grad norm: 11.05 | max abs grad:   2.250 | loss: 3.76 | perp.:  42.76\n",
      " b 300/711 >> 1897.4 ms/b | lr:  0.00025 | grad norm: 7.76 | max abs grad:   1.672 | loss: 3.72 | perp.:  41.21\n",
      " b 450/711 >> 1858.3 ms/b | lr:  0.00022 | grad norm: 7.28 | max abs grad:   1.640 | loss: 3.71 | perp.:  40.92\n",
      " b 600/711 >> 1944.8 ms/b | lr:   0.0002 | grad norm: 8.63 | max abs grad:   1.663 | loss: 3.76 | perp.:  42.84\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1346.62 sec | train_loss:  3.75 | train_perp:  42.41 | valid_loss:  3.99 | valid_perp.:  54.07\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  18/100) lr = 0.0002083\n",
      " b 150/717 >> 1923.6 ms/b | lr:  0.00021 | grad norm: 9.73 | max abs grad:   2.596 | loss: 3.73 | perp.:  41.53\n",
      " b 300/717 >> 1866.3 ms/b | lr:  0.00021 | grad norm: 8.91 | max abs grad:   1.815 | loss: 3.71 | perp.:  40.77\n",
      " b 450/717 >> 1858.7 ms/b | lr:  0.00021 | grad norm: 9.27 | max abs grad:   1.443 | loss: 3.70 | perp.:  40.35\n",
      " b 600/717 >> 1867.7 ms/b | lr:  0.00019 | grad norm: 9.39 | max abs grad:   1.712 | loss: 3.72 | perp.:  41.28\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1351.25 sec | train_loss:  3.73 | train_perp:  41.52 | valid_loss:  3.99 | valid_perp.:  54.07\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  19/100) lr = 0.0002028\n",
      " b 150/715 >> 1938.4 ms/b | lr:   0.0002 | grad norm: 7.37 | max abs grad:   1.619 | loss: 3.73 | perp.:  41.56\n",
      " b 300/715 >> 1861.3 ms/b | lr:  0.00021 | grad norm: 6.57 | max abs grad:   1.300 | loss: 3.68 | perp.:  39.55\n",
      " b 450/715 >> 1946.3 ms/b | lr:  0.00021 | grad norm: 8.76 | max abs grad:   2.015 | loss: 3.70 | perp.:  40.46\n",
      " b 600/715 >> 1906.8 ms/b | lr:   0.0002 | grad norm: 8.35 | max abs grad:   2.138 | loss: 3.72 | perp.:  41.25\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1367.57 sec | train_loss:  3.72 | train_perp:  41.22 | valid_loss:  3.98 | valid_perp.:  53.28\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  20/100) lr = 0.0001976\n",
      " b 150/720 >> 1910.0 ms/b | lr:  0.00016 | grad norm: 17.06 | max abs grad:   3.404 | loss: 3.70 | perp.:  40.46\n",
      " b 300/720 >> 1916.5 ms/b | lr:  0.00021 | grad norm: 10.77 | max abs grad:   2.645 | loss: 3.68 | perp.:  39.56\n",
      " b 450/720 >> 1864.3 ms/b | lr:  0.00018 | grad norm: 10.21 | max abs grad:   1.493 | loss: 3.67 | perp.:  39.34\n",
      " b 600/720 >> 1915.1 ms/b | lr:  0.00018 | grad norm: 13.92 | max abs grad:   3.283 | loss: 3.69 | perp.:  40.14\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1371.09 sec | train_loss:  3.70 | train_perp:  40.61 | valid_loss:  3.96 | valid_perp.:  52.43\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  21/100) lr = 0.0001929\n",
      " b 150/721 >> 1964.4 ms/b | lr:  0.00018 | grad norm: 10.28 | max abs grad:   1.743 | loss: 3.68 | perp.:  39.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/721 >> 1885.8 ms/b | lr:  0.00018 | grad norm: 14.96 | max abs grad:   2.655 | loss: 3.66 | perp.:  38.69\n",
      " b 450/721 >> 1932.1 ms/b | lr:  0.00018 | grad norm: 12.66 | max abs grad:   2.582 | loss: 3.67 | perp.:  39.36\n",
      " b 600/721 >> 1881.5 ms/b | lr:  0.00016 | grad norm: 19.68 | max abs grad:   5.018 | loss: 3.66 | perp.:  38.90\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1381.87 sec | train_loss:  3.68 | train_perp:  39.83 | valid_loss:  3.94 | valid_perp.:  51.45\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  22/100) lr = 0.0001884\n",
      " b 150/723 >> 1931.1 ms/b | lr:  0.00017 | grad norm: 15.34 | max abs grad:   3.481 | loss: 3.67 | perp.:  39.44\n",
      " b 300/723 >> 1932.8 ms/b | lr:  0.00017 | grad norm: 12.25 | max abs grad:   2.417 | loss: 3.64 | perp.:  38.20\n",
      " b 450/723 >> 1920.5 ms/b | lr:  0.00022 | grad norm: 11.47 | max abs grad:   2.508 | loss: 3.64 | perp.:  38.04\n",
      " b 600/723 >> 1930.5 ms/b | lr:  0.00019 | grad norm: 10.59 | max abs grad:   1.939 | loss: 3.66 | perp.:  38.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1392.52 sec | train_loss:  3.67 | train_perp:  39.18 | valid_loss:  3.93 | valid_perp.:  50.97\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  23/100) lr = 0.0001843\n",
      " b 150/713 >> 1954.6 ms/b | lr:  0.00019 | grad norm: 9.98 | max abs grad:   2.142 | loss: 3.68 | perp.:  39.67\n",
      " b 300/713 >> 1999.7 ms/b | lr:  0.00018 | grad norm: 11.56 | max abs grad:   2.784 | loss: 3.66 | perp.:  39.01\n",
      " b 450/713 >> 1882.5 ms/b | lr:  0.00018 | grad norm: 10.63 | max abs grad:   1.511 | loss: 3.65 | perp.:  38.41\n",
      " b 600/713 >> 1971.7 ms/b | lr:  0.00019 | grad norm: 11.42 | max abs grad:   2.641 | loss: 3.66 | perp.:  38.94\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1390.99 sec | train_loss:  3.68 | train_perp:  39.63 | valid_loss:  3.93 | valid_perp.:  50.83\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  24/100) lr = 0.0001804\n",
      " b 150/719 >> 2055.5 ms/b | lr:  0.00016 | grad norm: 15.70 | max abs grad:   2.356 | loss: 3.68 | perp.:  39.76\n",
      " b 300/719 >> 1892.5 ms/b | lr:  0.00016 | grad norm: 11.47 | max abs grad:   1.747 | loss: 3.62 | perp.:  37.52\n",
      " b 450/719 >> 1950.9 ms/b | lr:  0.00019 | grad norm: 16.21 | max abs grad:   6.035 | loss: 3.63 | perp.:  37.77\n",
      " b 600/719 >> 1946.2 ms/b | lr:  0.00018 | grad norm: 11.85 | max abs grad:   2.447 | loss: 3.65 | perp.:  38.57\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1404.78 sec | train_loss:  3.66 | train_perp:  38.74 | valid_loss:  3.92 | valid_perp.:  50.26\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  25/100) lr = 0.0001768\n",
      " b 150/718 >> 1980.3 ms/b | lr:  0.00019 | grad norm: 18.17 | max abs grad:   2.329 | loss: 3.65 | perp.:  38.33\n",
      " b 300/718 >> 1966.1 ms/b | lr:  0.00018 | grad norm: 13.82 | max abs grad:   2.856 | loss: 3.62 | perp.:  37.38\n",
      " b 450/718 >> 1961.5 ms/b | lr:  0.00017 | grad norm: 16.54 | max abs grad:   2.936 | loss: 3.63 | perp.:  37.72\n",
      " b 600/718 >> 1966.7 ms/b | lr:  0.00019 | grad norm: 11.09 | max abs grad:   2.227 | loss: 3.64 | perp.:  37.91\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1409.97 sec | train_loss:  3.64 | train_perp:  38.26 | valid_loss:  3.91 | valid_perp.:  49.66\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  26/100) lr = 0.0001733\n",
      " b 150/724 >> 2002.1 ms/b | lr:  0.00016 | grad norm: 13.25 | max abs grad:   1.587 | loss: 3.64 | perp.:  37.98\n",
      " b 300/724 >> 1938.3 ms/b | lr:  0.00019 | grad norm: 16.65 | max abs grad:   2.653 | loss: 3.60 | perp.:  36.62\n",
      " b 450/724 >> 1969.7 ms/b | lr:  0.00017 | grad norm: 14.67 | max abs grad:   2.928 | loss: 3.61 | perp.:  36.81\n",
      " b 600/724 >> 1907.0 ms/b | lr:  0.00019 | grad norm: 14.85 | max abs grad:   1.807 | loss: 3.61 | perp.:  36.94\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1415.19 sec | train_loss:  3.63 | train_perp:  37.60 | valid_loss:  3.90 | valid_perp.:  49.29\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  27/100) lr = 0.0001701\n",
      " b 150/720 >> 1965.8 ms/b | lr:  0.00014 | grad norm: 26.63 | max abs grad:   6.012 | loss: 3.63 | perp.:  37.74\n",
      " b 300/720 >> 2012.3 ms/b | lr:  0.00017 | grad norm: 15.00 | max abs grad:   3.001 | loss: 3.65 | perp.:  38.54\n",
      " b 450/720 >> 1895.6 ms/b | lr:  0.00017 | grad norm: 11.30 | max abs grad:   1.796 | loss: 3.59 | perp.:  36.17\n",
      " b 600/720 >> 2012.6 ms/b | lr:  0.00018 | grad norm: 10.34 | max abs grad:   1.311 | loss: 3.63 | perp.:  37.55\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1419.45 sec | train_loss:  3.63 | train_perp:  37.86 | valid_loss:  3.90 | valid_perp.:  49.28\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  28/100) lr = 0.000167\n",
      " b 150/720 >> 2041.4 ms/b | lr:  0.00015 | grad norm: 20.56 | max abs grad:   3.559 | loss: 3.63 | perp.:  37.65\n",
      " b 300/720 >> 1965.2 ms/b | lr:  0.00017 | grad norm: 15.85 | max abs grad:   2.513 | loss: 3.59 | perp.:  36.09\n",
      " b 450/720 >> 1971.7 ms/b | lr:  0.00018 | grad norm: 12.37 | max abs grad:   3.234 | loss: 3.60 | perp.:  36.53\n",
      " b 600/720 >> 2004.6 ms/b | lr:  0.00016 | grad norm: 13.23 | max abs grad:   2.137 | loss: 3.61 | perp.:  36.95\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1432.16 sec | train_loss:  3.62 | train_perp:  37.27 | valid_loss:  3.89 | valid_perp.:  48.83\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  29/100) lr = 0.0001641\n",
      " b 150/718 >> 2049.4 ms/b | lr:  0.00017 | grad norm: 25.43 | max abs grad:   5.813 | loss: 3.61 | perp.:  37.12\n",
      " b 300/718 >> 1969.9 ms/b | lr:  0.00017 | grad norm: 14.12 | max abs grad:   2.534 | loss: 3.59 | perp.:  36.16\n",
      " b 450/718 >> 2021.9 ms/b | lr:  0.00016 | grad norm: 18.71 | max abs grad:   4.545 | loss: 3.60 | perp.:  36.66\n",
      " b 600/718 >> 1964.0 ms/b | lr:  0.00017 | grad norm: 17.46 | max abs grad:   1.985 | loss: 3.59 | perp.:  36.17\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1435.33 sec | train_loss:  3.61 | train_perp:  36.98 | valid_loss:  3.91 | valid_perp.:  49.88\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  30/100) lr = 0.0001614\n",
      " b 150/721 >> 1903.3 ms/b | lr:  0.00017 | grad norm: 13.81 | max abs grad:   2.568 | loss: 3.59 | perp.:  36.25\n",
      " b 300/721 >> 1938.3 ms/b | lr:  0.00018 | grad norm: 14.63 | max abs grad:   3.707 | loss: 3.56 | perp.:  35.26\n",
      " b 450/721 >> 1934.7 ms/b | lr:  0.00017 | grad norm: 17.64 | max abs grad:   2.741 | loss: 3.59 | perp.:  36.19\n",
      " b 600/721 >> 1899.6 ms/b | lr:  0.00016 | grad norm: 14.55 | max abs grad:   2.809 | loss: 3.58 | perp.:  35.91\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1376.54 sec | train_loss:  3.59 | train_perp:  36.39 | valid_loss:  3.87 | valid_perp.:  48.09\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  31/100) lr = 0.0001588\n",
      " b 150/720 >> 1964.4 ms/b | lr:  0.00018 | grad norm: 14.93 | max abs grad:   2.608 | loss: 3.60 | perp.:  36.60\n",
      " b 300/720 >> 1887.9 ms/b | lr:  0.00017 | grad norm: 13.84 | max abs grad:   1.528 | loss: 3.57 | perp.:  35.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 450/720 >> 1910.6 ms/b | lr:  0.00014 | grad norm: 20.03 | max abs grad:   7.344 | loss: 3.57 | perp.:  35.48\n",
      " b 600/720 >> 1859.2 ms/b | lr:  0.00015 | grad norm: 16.27 | max abs grad:   4.163 | loss: 3.58 | perp.:  35.78\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1362.82 sec | train_loss:  3.59 | train_perp:  36.34 | valid_loss:  3.87 | valid_perp.:  47.73\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  32/100) lr = 0.0001563\n",
      " b 150/720 >> 1909.7 ms/b | lr:  0.00015 | grad norm: 16.67 | max abs grad:   4.062 | loss: 3.60 | perp.:  36.42\n",
      " b 300/720 >> 1912.2 ms/b | lr:  0.00014 | grad norm: 19.46 | max abs grad:   4.515 | loss: 3.56 | perp.:  35.04\n",
      " b 450/720 >> 1833.5 ms/b | lr:  0.00015 | grad norm: 21.92 | max abs grad:   4.879 | loss: 3.54 | perp.:  34.58\n",
      " b 600/720 >> 1939.0 ms/b | lr:  0.00015 | grad norm: 14.22 | max abs grad:   2.160 | loss: 3.57 | perp.:  35.58\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1367.89 sec | train_loss:  3.58 | train_perp:  35.98 | valid_loss:  3.88 | valid_perp.:  48.29\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  33/100) lr = 0.0001539\n",
      " b 150/716 >> 1928.5 ms/b | lr:  0.00015 | grad norm: 13.44 | max abs grad:   2.136 | loss: 3.59 | perp.:  36.06\n",
      " b 300/716 >> 1905.5 ms/b | lr:  0.00014 | grad norm: 27.56 | max abs grad:   3.371 | loss: 3.56 | perp.:  35.10\n",
      " b 450/716 >> 1959.4 ms/b | lr:  0.00015 | grad norm: 19.57 | max abs grad:   2.967 | loss: 3.58 | perp.:  35.92\n",
      " b 600/716 >> 1913.7 ms/b | lr:  0.00016 | grad norm: 17.24 | max abs grad:   3.708 | loss: 3.55 | perp.:  34.82\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1380.46 sec | train_loss:  3.58 | train_perp:  36.03 | valid_loss:  3.85 | valid_perp.:  47.11\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  34/100) lr = 0.0001516\n",
      " b 150/711 >> 1953.3 ms/b | lr:  0.00015 | grad norm: 14.74 | max abs grad:   2.540 | loss: 3.57 | perp.:  35.55\n",
      " b 300/711 >> 1928.3 ms/b | lr:  0.00014 | grad norm: 23.22 | max abs grad:   2.864 | loss: 3.55 | perp.:  34.70\n",
      " b 450/711 >> 1957.7 ms/b | lr:  0.00013 | grad norm: 39.53 | max abs grad:   4.162 | loss: 3.56 | perp.:  35.25\n",
      " b 600/711 >> 1950.9 ms/b | lr:  0.00012 | grad norm: 40.27 | max abs grad:   4.039 | loss: 3.56 | perp.:  35.33\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1387.48 sec | train_loss:  3.58 | train_perp:  35.75 | valid_loss:  3.85 | valid_perp.:  46.85\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  35/100) lr = 0.0001494\n",
      " b 150/721 >> 1933.1 ms/b | lr:  0.00016 | grad norm: 14.25 | max abs grad:   3.711 | loss: 3.56 | perp.:  35.30\n",
      " b 300/721 >> 1920.0 ms/b | lr:  0.00015 | grad norm: 19.71 | max abs grad:   3.281 | loss: 3.54 | perp.:  34.43\n",
      " b 450/721 >> 1906.3 ms/b | lr:  0.00014 | grad norm: 17.63 | max abs grad:   2.606 | loss: 3.55 | perp.:  34.93\n",
      " b 600/721 >> 1935.8 ms/b | lr:  0.00013 | grad norm: 20.73 | max abs grad:   4.117 | loss: 3.55 | perp.:  34.80\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1387.01 sec | train_loss:  3.56 | train_perp:  35.31 | valid_loss:  3.87 | valid_perp.:  47.79\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  36/100) lr = 0.0001473\n",
      " b 150/710 >> 1960.3 ms/b | lr:  0.00014 | grad norm: 15.85 | max abs grad:   2.230 | loss: 3.58 | perp.:  35.70\n",
      " b 300/710 >> 1958.5 ms/b | lr:  0.00016 | grad norm: 31.49 | max abs grad:   3.444 | loss: 3.55 | perp.:  34.84\n",
      " b 450/710 >> 1985.9 ms/b | lr:  0.00015 | grad norm: 17.22 | max abs grad:   3.193 | loss: 3.55 | perp.:  34.68\n",
      " b 600/710 >> 2006.9 ms/b | lr:  0.00013 | grad norm: 19.86 | max abs grad:   2.828 | loss: 3.55 | perp.:  34.95\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1398.52 sec | train_loss:  3.57 | train_perp:  35.39 | valid_loss:  3.83 | valid_perp.:  46.25\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  37/100) lr = 0.0001453\n",
      " b 150/719 >> 2001.2 ms/b | lr:  0.00016 | grad norm: 25.31 | max abs grad:   4.776 | loss: 3.59 | perp.:  36.34\n",
      " b 300/719 >> 1945.8 ms/b | lr:  0.00015 | grad norm: 15.71 | max abs grad:   2.924 | loss: 3.53 | perp.:  34.12\n",
      " b 450/719 >> 1943.8 ms/b | lr:  0.00013 | grad norm: 23.42 | max abs grad:   3.810 | loss: 3.53 | perp.:  34.08\n",
      " b 600/719 >> 1912.4 ms/b | lr:  0.00015 | grad norm: 17.13 | max abs grad:   3.837 | loss: 3.53 | perp.:  33.97\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1402.90 sec | train_loss:  3.55 | train_perp:  34.90 | valid_loss:  3.84 | valid_perp.:  46.56\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  38/100) lr = 0.0001434\n",
      " b 150/715 >> 1983.9 ms/b | lr:  0.00014 | grad norm: 19.85 | max abs grad:   2.746 | loss: 3.55 | perp.:  34.88\n",
      " b 300/715 >> 1962.5 ms/b | lr:  0.00015 | grad norm: 17.48 | max abs grad:   3.130 | loss: 3.54 | perp.:  34.60\n",
      " b 450/715 >> 1951.9 ms/b | lr:  0.00015 | grad norm: 16.38 | max abs grad:   2.710 | loss: 3.53 | perp.:  34.00\n",
      " b 600/715 >> 1974.8 ms/b | lr:  0.00013 | grad norm: 20.99 | max abs grad:   2.339 | loss: 3.54 | perp.:  34.40\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1407.17 sec | train_loss:  3.55 | train_perp:  34.91 | valid_loss:  3.84 | valid_perp.:  46.39\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  39/100) lr = 0.0001415\n",
      " b 150/716 >> 1997.7 ms/b | lr:  0.00015 | grad norm: 16.00 | max abs grad:   2.478 | loss: 3.53 | perp.:  34.12\n",
      " b 300/716 >> 1945.5 ms/b | lr:  0.00016 | grad norm: 41.69 | max abs grad:   3.399 | loss: 3.52 | perp.:  33.70\n",
      " b 450/716 >> 1950.3 ms/b | lr:  0.00013 | grad norm: 20.00 | max abs grad:   3.404 | loss: 3.51 | perp.:  33.52\n",
      " b 600/716 >> 1998.5 ms/b | lr:  0.00015 | grad norm: 32.54 | max abs grad:   3.135 | loss: 3.54 | perp.:  34.59\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1413.38 sec | train_loss:  3.54 | train_perp:  34.47 | valid_loss:  3.83 | valid_perp.:  46.03\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  40/100) lr = 0.0001398\n",
      " b 150/715 >> 2033.5 ms/b | lr:  0.00015 | grad norm: 29.24 | max abs grad:   5.247 | loss: 3.55 | perp.:  34.82\n",
      " b 300/715 >> 1933.8 ms/b | lr:  0.00016 | grad norm: 15.28 | max abs grad:   2.534 | loss: 3.51 | perp.:  33.36\n",
      " b 450/715 >> 2027.7 ms/b | lr:  0.00014 | grad norm: 17.21 | max abs grad:   2.384 | loss: 3.53 | perp.:  34.23\n",
      " b 600/715 >> 1966.0 ms/b | lr:  0.00014 | grad norm: 20.18 | max abs grad:   3.981 | loss: 3.51 | perp.:  33.47\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1416.48 sec | train_loss:  3.54 | train_perp:  34.45 | valid_loss:  3.82 | valid_perp.:  45.72\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  41/100) lr = 0.000138\n",
      " b 150/720 >> 1973.7 ms/b | lr:  0.00013 | grad norm: 20.66 | max abs grad:   3.147 | loss: 3.52 | perp.:  33.95\n",
      " b 300/720 >> 1958.6 ms/b | lr:  0.00014 | grad norm: 21.78 | max abs grad:   2.946 | loss: 3.51 | perp.:  33.50\n",
      " b 450/720 >> 1958.8 ms/b | lr:  0.00013 | grad norm: 20.24 | max abs grad:   3.451 | loss: 3.54 | perp.:  34.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 600/720 >> 1968.4 ms/b | lr:  0.00013 | grad norm: 19.02 | max abs grad:   4.158 | loss: 3.51 | perp.:  33.54\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1419.43 sec | train_loss:  3.53 | train_perp:  34.16 | valid_loss:  3.82 | valid_perp.:  45.79\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  42/100) lr = 0.0001364\n",
      " b 150/716 >> 1998.5 ms/b | lr:  0.00015 | grad norm: 33.07 | max abs grad:   3.021 | loss: 3.53 | perp.:  34.04\n",
      " b 300/716 >> 1999.4 ms/b | lr:  0.00014 | grad norm: 21.95 | max abs grad:   2.511 | loss: 3.51 | perp.:  33.59\n",
      " b 450/716 >> 1949.7 ms/b | lr:  0.00014 | grad norm: 17.24 | max abs grad:   2.291 | loss: 3.50 | perp.:  33.00\n",
      " b 600/716 >> 2009.7 ms/b | lr:  0.00015 | grad norm: 34.90 | max abs grad:   4.300 | loss: 3.52 | perp.:  33.78\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1420.66 sec | train_loss:  3.53 | train_perp:  34.02 | valid_loss:  3.82 | valid_perp.:  45.63\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  43/100) lr = 0.0001348\n",
      " b 150/713 >> 2053.1 ms/b | lr:  0.00013 | grad norm: 19.85 | max abs grad:   2.997 | loss: 3.54 | perp.:  34.32\n",
      " b 300/713 >> 1968.2 ms/b | lr:  0.00014 | grad norm: 23.35 | max abs grad:   3.610 | loss: 3.52 | perp.:  33.77\n",
      " b 450/713 >> 1954.8 ms/b | lr:  0.00013 | grad norm: 29.89 | max abs grad:   4.503 | loss: 3.49 | perp.:  32.91\n",
      " b 600/713 >> 2007.1 ms/b | lr:  0.00012 | grad norm: 24.15 | max abs grad:   4.951 | loss: 3.51 | perp.:  33.58\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1423.22 sec | train_loss:  3.53 | train_perp:  33.97 | valid_loss:  3.81 | valid_perp.:  45.34\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  44/100) lr = 0.0001333\n",
      " b 150/714 >> 2010.2 ms/b | lr:  0.00014 | grad norm: 19.75 | max abs grad:   2.354 | loss: 3.51 | perp.:  33.49\n",
      " b 300/714 >> 1989.5 ms/b | lr:  0.00014 | grad norm: 33.75 | max abs grad:   3.173 | loss: 3.51 | perp.:  33.48\n",
      " b 450/714 >> 1966.3 ms/b | lr:  0.00011 | grad norm: 30.94 | max abs grad:   3.353 | loss: 3.49 | perp.:  32.84\n",
      " b 600/714 >> 2022.6 ms/b | lr:  0.00013 | grad norm: 23.31 | max abs grad:   2.574 | loss: 3.51 | perp.:  33.50\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1424.63 sec | train_loss:  3.51 | train_perp:  33.61 | valid_loss:  3.81 | valid_perp.:  45.34\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  45/100) lr = 0.0001318\n",
      " b 150/712 >> 1974.5 ms/b | lr:  0.00015 | grad norm: 15.47 | max abs grad:   1.873 | loss: 3.53 | perp.:  34.13\n",
      " b 300/712 >> 2020.9 ms/b | lr:  0.00014 | grad norm: 17.35 | max abs grad:   3.056 | loss: 3.49 | perp.:  32.92\n",
      " b 450/712 >> 2003.9 ms/b | lr:  0.00014 | grad norm: 23.73 | max abs grad:   3.194 | loss: 3.51 | perp.:  33.41\n",
      " b 600/712 >> 2009.3 ms/b | lr:  9.8e-05 | grad norm: 54.13 | max abs grad:   9.208 | loss: 3.50 | perp.:  33.07\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1423.20 sec | train_loss:  3.52 | train_perp:  33.79 | valid_loss:  3.81 | valid_perp.:  45.16\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  46/100) lr = 0.0001303\n",
      " b 150/723 >> 2023.5 ms/b | lr:  0.00011 | grad norm: 27.68 | max abs grad:   6.376 | loss: 3.51 | perp.:  33.61\n",
      " b 300/723 >> 1931.3 ms/b | lr:  0.00013 | grad norm: 22.99 | max abs grad:   3.196 | loss: 3.48 | perp.:  32.31\n",
      " b 450/723 >> 1963.2 ms/b | lr:  0.00015 | grad norm: 19.50 | max abs grad:   2.379 | loss: 3.48 | perp.:  32.41\n",
      " b 600/723 >> 1929.0 ms/b | lr:  0.00013 | grad norm: 23.91 | max abs grad:   3.577 | loss: 3.50 | perp.:  33.23\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1417.76 sec | train_loss:  3.51 | train_perp:  33.40 | valid_loss:  3.81 | valid_perp.:  45.03\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  47/100) lr = 0.0001289\n",
      " b 150/726 >> 1943.4 ms/b | lr:  0.00013 | grad norm: 21.01 | max abs grad:   3.640 | loss: 3.50 | perp.:  33.02\n",
      " b 300/726 >> 1959.4 ms/b | lr:  0.00013 | grad norm: 18.26 | max abs grad:   3.971 | loss: 3.48 | perp.:  32.60\n",
      " b 450/726 >> 1929.4 ms/b | lr:  0.00012 | grad norm: 18.27 | max abs grad:   2.843 | loss: 3.49 | perp.:  32.65\n",
      " b 600/726 >> 1960.7 ms/b | lr:  0.00012 | grad norm: 23.13 | max abs grad:   2.777 | loss: 3.48 | perp.:  32.62\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1408.12 sec | train_loss:  3.50 | train_perp:  33.08 | valid_loss:  3.82 | valid_perp.:  45.56\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  48/100) lr = 0.0001276\n",
      " b 150/714 >> 1970.4 ms/b | lr:  0.00012 | grad norm: 26.22 | max abs grad:   3.155 | loss: 3.50 | perp.:  33.13\n",
      " b 300/714 >> 1944.3 ms/b | lr:  0.00012 | grad norm: 25.72 | max abs grad:   4.990 | loss: 3.47 | perp.:  32.14\n",
      " b 450/714 >> 2012.1 ms/b | lr:  0.00012 | grad norm: 23.21 | max abs grad:   2.675 | loss: 3.50 | perp.:  33.18\n",
      " b 600/714 >> 1959.9 ms/b | lr:  0.00013 | grad norm: 17.02 | max abs grad:   2.615 | loss: 3.49 | perp.:  32.78\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1410.08 sec | train_loss:  3.51 | train_perp:  33.33 | valid_loss:  3.80 | valid_perp.:  44.57\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  49/100) lr = 0.0001263\n",
      " b 150/715 >> 1965.1 ms/b | lr:  0.00012 | grad norm: 23.97 | max abs grad:   4.294 | loss: 3.51 | perp.:  33.35\n",
      " b 300/715 >> 1998.1 ms/b | lr:  0.00012 | grad norm: 33.19 | max abs grad:   3.608 | loss: 3.49 | perp.:  32.74\n",
      " b 450/715 >> 1981.7 ms/b | lr:  0.00012 | grad norm: 23.87 | max abs grad:   3.896 | loss: 3.50 | perp.:  33.10\n",
      " b 600/715 >> 1954.1 ms/b | lr:  0.00012 | grad norm: 36.21 | max abs grad:   5.432 | loss: 3.47 | perp.:  32.26\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1411.43 sec | train_loss:  3.51 | train_perp:  33.29 | valid_loss:  3.80 | valid_perp.:  44.57\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  50/100) lr = 0.000125\n",
      " b 150/719 >> 1984.1 ms/b | lr:  0.00012 | grad norm: 41.39 | max abs grad:   4.997 | loss: 3.49 | perp.:  32.69\n",
      " b 300/719 >> 1930.9 ms/b | lr:  0.00013 | grad norm: 20.15 | max abs grad:   3.763 | loss: 3.46 | perp.:  31.89\n",
      " b 450/719 >> 1963.1 ms/b | lr:  0.00014 | grad norm: 51.72 | max abs grad:   4.315 | loss: 3.48 | perp.:  32.40\n",
      " b 600/719 >> 1953.6 ms/b | lr:  0.00012 | grad norm: 25.78 | max abs grad:   2.985 | loss: 3.52 | perp.:  33.84\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1406.89 sec | train_loss:  3.49 | train_perp:  32.82 | valid_loss:  3.79 | valid_perp.:  44.32\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  51/100) lr = 0.0001238\n",
      " b 150/718 >> 1962.1 ms/b | lr:  0.00013 | grad norm: 40.14 | max abs grad:   4.303 | loss: 3.48 | perp.:  32.59\n",
      " b 300/718 >> 2023.0 ms/b | lr:  0.00011 | grad norm: 25.52 | max abs grad:   2.624 | loss: 3.50 | perp.:  33.20\n",
      " b 450/718 >> 1979.7 ms/b | lr:  0.00013 | grad norm: 35.70 | max abs grad:   3.793 | loss: 3.46 | perp.:  31.85\n",
      " b 600/718 >> 1923.5 ms/b | lr:  0.00013 | grad norm: 20.43 | max abs grad:   3.101 | loss: 3.47 | perp.:  32.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1419.17 sec | train_loss:  3.50 | train_perp:  33.01 | valid_loss:  3.81 | valid_perp.:  44.93\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  52/100) lr = 0.0001226\n",
      " b 150/724 >> 1973.9 ms/b | lr:  0.00012 | grad norm: 22.95 | max abs grad:   3.206 | loss: 3.49 | perp.:  32.95\n",
      " b 300/724 >> 1950.2 ms/b | lr:  0.00013 | grad norm: 29.94 | max abs grad:   3.302 | loss: 3.45 | perp.:  31.53\n",
      " b 450/724 >> 1952.4 ms/b | lr:  0.00013 | grad norm: 65.46 | max abs grad:   5.400 | loss: 3.47 | perp.:  32.01\n",
      " b 600/724 >> 1984.4 ms/b | lr:  0.00014 | grad norm: 26.73 | max abs grad:   3.318 | loss: 3.48 | perp.:  32.45\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1413.50 sec | train_loss:  3.48 | train_perp:  32.49 | valid_loss:  3.79 | valid_perp.:  44.10\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  53/100) lr = 0.0001214\n",
      " b 150/716 >> 2001.9 ms/b | lr:  0.00011 | grad norm: 30.26 | max abs grad:   3.869 | loss: 3.49 | perp.:  32.92\n",
      " b 300/716 >> 1950.5 ms/b | lr:  0.00012 | grad norm: 29.53 | max abs grad:   3.376 | loss: 3.47 | perp.:  32.06\n",
      " b 450/716 >> 1964.3 ms/b | lr:   0.0001 | grad norm: 38.90 | max abs grad:   4.538 | loss: 3.45 | perp.:  31.57\n",
      " b 600/716 >> 2014.9 ms/b | lr:  0.00012 | grad norm: 27.35 | max abs grad:   3.749 | loss: 3.48 | perp.:  32.62\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1421.83 sec | train_loss:  3.49 | train_perp:  32.68 | valid_loss:  3.78 | valid_perp.:  44.00\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  54/100) lr = 0.0001203\n",
      " b 150/714 >> 1958.9 ms/b | lr:  0.00011 | grad norm: 29.68 | max abs grad:   3.384 | loss: 3.48 | perp.:  32.46\n",
      " b 300/714 >> 2019.7 ms/b | lr:   0.0001 | grad norm: 44.69 | max abs grad:   4.757 | loss: 3.46 | perp.:  31.94\n",
      " b 450/714 >> 1981.1 ms/b | lr:  0.00013 | grad norm: 20.63 | max abs grad:   4.175 | loss: 3.46 | perp.:  31.97\n",
      " b 600/714 >> 1998.9 ms/b | lr:  0.00011 | grad norm: 26.70 | max abs grad:   3.654 | loss: 3.47 | perp.:  32.02\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1419.07 sec | train_loss:  3.48 | train_perp:  32.53 | valid_loss:  3.80 | valid_perp.:  44.53\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  55/100) lr = 0.0001192\n",
      " b 150/714 >> 1992.8 ms/b | lr:  0.00013 | grad norm: 53.82 | max abs grad:   4.816 | loss: 3.49 | perp.:  32.75\n",
      " b 300/714 >> 1975.8 ms/b | lr:  0.00012 | grad norm: 23.72 | max abs grad:   3.594 | loss: 3.47 | perp.:  32.15\n",
      " b 450/714 >> 2015.0 ms/b | lr:  0.00012 | grad norm: 29.08 | max abs grad:   3.666 | loss: 3.47 | perp.:  32.03\n",
      " b 600/714 >> 2022.0 ms/b | lr:  0.00012 | grad norm: 40.30 | max abs grad:   6.260 | loss: 3.48 | perp.:  32.47\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1422.91 sec | train_loss:  3.49 | train_perp:  32.74 | valid_loss:  3.81 | valid_perp.:  45.05\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  56/100) lr = 0.0001181\n",
      " b 150/727 >> 1940.9 ms/b | lr:  0.00011 | grad norm: 21.36 | max abs grad:   3.079 | loss: 3.46 | perp.:  31.81\n",
      " b 300/727 >> 1946.9 ms/b | lr:  0.00012 | grad norm: 19.55 | max abs grad:   2.597 | loss: 3.45 | perp.:  31.46\n",
      " b 450/727 >> 1899.6 ms/b | lr:  0.00011 | grad norm: 27.23 | max abs grad:   3.394 | loss: 3.43 | perp.:  30.91\n",
      " b 600/727 >> 1987.0 ms/b | lr:  0.00013 | grad norm: 33.07 | max abs grad:   3.699 | loss: 3.47 | perp.:  31.99\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1417.88 sec | train_loss:  3.47 | train_perp:  32.11 | valid_loss:  3.77 | valid_perp.:  43.57\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  57/100) lr = 0.0001171\n",
      " b 150/726 >> 1951.1 ms/b | lr:  0.00011 | grad norm: 26.85 | max abs grad:   3.943 | loss: 3.46 | perp.:  31.93\n",
      " b 300/726 >> 1950.4 ms/b | lr:  0.00012 | grad norm: 22.47 | max abs grad:   3.431 | loss: 3.44 | perp.:  31.16\n",
      " b 450/726 >> 1967.2 ms/b | lr:   0.0001 | grad norm: 30.88 | max abs grad:   6.205 | loss: 3.46 | perp.:  31.88\n",
      " b 600/726 >> 1966.6 ms/b | lr:  0.00011 | grad norm: 31.10 | max abs grad:   6.451 | loss: 3.45 | perp.:  31.64\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1419.96 sec | train_loss:  3.47 | train_perp:  32.13 | valid_loss:  3.78 | valid_perp.:  43.60\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  58/100) lr = 0.0001161\n",
      " b 150/712 >> 2015.4 ms/b | lr:  0.00011 | grad norm: 25.94 | max abs grad:   4.065 | loss: 3.48 | perp.:  32.40\n",
      " b 300/712 >> 1982.6 ms/b | lr:  0.00011 | grad norm: 45.90 | max abs grad:   4.874 | loss: 3.45 | perp.:  31.44\n",
      " b 450/712 >> 2015.2 ms/b | lr:  0.00012 | grad norm: 24.02 | max abs grad:   4.179 | loss: 3.47 | perp.:  32.09\n",
      " b 600/712 >> 2006.9 ms/b | lr:  0.00011 | grad norm: 36.34 | max abs grad:   5.346 | loss: 3.48 | perp.:  32.31\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1423.78 sec | train_loss:  3.48 | train_perp:  32.40 | valid_loss:  3.78 | valid_perp.:  43.70\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  59/100) lr = 0.0001151\n",
      " b 150/713 >> 2026.9 ms/b | lr:  0.00013 | grad norm: 22.95 | max abs grad:   3.331 | loss: 3.49 | perp.:  32.68\n",
      " b 300/713 >> 1998.2 ms/b | lr:  0.00011 | grad norm: 34.54 | max abs grad:   5.862 | loss: 3.49 | perp.:  32.90\n",
      " b 450/713 >> 1953.7 ms/b | lr:  0.00011 | grad norm: 25.62 | max abs grad:   3.219 | loss: 3.45 | perp.:  31.62\n",
      " b 600/713 >> 1999.3 ms/b | lr:  9.8e-05 | grad norm: 28.67 | max abs grad:   4.252 | loss: 3.47 | perp.:  32.11\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1422.58 sec | train_loss:  3.48 | train_perp:  32.43 | valid_loss:  3.80 | valid_perp.:  44.67\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  60/100) lr = 0.0001141\n",
      " b 150/724 >> 1990.1 ms/b | lr:  0.00011 | grad norm: 22.53 | max abs grad:   3.217 | loss: 3.48 | perp.:  32.33\n",
      " b 300/724 >> 1944.4 ms/b | lr:  0.00013 | grad norm: 45.85 | max abs grad:   4.995 | loss: 3.43 | perp.:  30.92\n",
      " b 450/724 >> 1919.9 ms/b | lr:  9.7e-05 | grad norm: 41.24 | max abs grad:   4.709 | loss: 3.43 | perp.:  30.73\n",
      " b 600/724 >> 1976.8 ms/b | lr:  0.00011 | grad norm: 37.88 | max abs grad:   4.900 | loss: 3.44 | perp.:  31.24\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1423.44 sec | train_loss:  3.46 | train_perp:  31.74 | valid_loss:  3.78 | valid_perp.:  43.67\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  61/100) lr = 0.0001132\n",
      " b 150/712 >> 1970.3 ms/b | lr:   0.0001 | grad norm: 36.57 | max abs grad:   4.294 | loss: 3.48 | perp.:  32.38\n",
      " b 300/712 >> 2015.8 ms/b | lr:   0.0001 | grad norm: 30.81 | max abs grad:   4.385 | loss: 3.44 | perp.:  31.28\n",
      " b 450/712 >> 1935.8 ms/b | lr:  9.6e-05 | grad norm: 44.98 | max abs grad:   5.511 | loss: 3.45 | perp.:  31.40\n",
      " b 600/712 >> 2024.0 ms/b | lr:  0.00013 | grad norm: 26.64 | max abs grad:   3.811 | loss: 3.46 | perp.:  31.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1418.37 sec | train_loss:  3.47 | train_perp:  32.18 | valid_loss:  3.77 | valid_perp.:  43.28\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  62/100) lr = 0.0001123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/719 >> 1999.4 ms/b | lr:  8.4e-05 | grad norm: 69.01 | max abs grad:   9.719 | loss: 3.47 | perp.:  31.99\n",
      " b 300/719 >> 2014.6 ms/b | lr:  0.00011 | grad norm: 31.25 | max abs grad:   5.117 | loss: 3.44 | perp.:  31.17\n",
      " b 450/719 >> 2085.8 ms/b | lr:  0.00011 | grad norm: 24.87 | max abs grad:   3.007 | loss: 3.43 | perp.:  30.95\n",
      " b 600/719 >> 2723.2 ms/b | lr:  0.00011 | grad norm: 35.69 | max abs grad:   4.287 | loss: 3.45 | perp.:  31.45\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1637.44 sec | train_loss:  3.46 | train_perp:  31.91 | valid_loss:  3.77 | valid_perp.:  43.36\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  63/100) lr = 0.0001114\n",
      " b 150/717 >> 2532.1 ms/b | lr:  0.00012 | grad norm: 33.92 | max abs grad:   7.026 | loss: 3.45 | perp.:  31.62\n",
      " b 300/717 >> 2578.8 ms/b | lr:   0.0001 | grad norm: 27.34 | max abs grad:   3.191 | loss: 3.48 | perp.:  32.33\n",
      " b 450/717 >> 2536.2 ms/b | lr:  9.8e-05 | grad norm: 34.28 | max abs grad:   4.120 | loss: 3.44 | perp.:  31.21\n",
      " b 600/717 >> 2546.0 ms/b | lr:   0.0001 | grad norm: 73.87 | max abs grad:   7.357 | loss: 3.45 | perp.:  31.54\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1818.50 sec | train_loss:  3.47 | train_perp:  32.03 | valid_loss:  3.76 | valid_perp.:  43.12\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  64/100) lr = 0.0001105\n",
      " b 150/714 >> 2514.5 ms/b | lr:  0.00012 | grad norm: 73.85 | max abs grad:   6.405 | loss: 3.44 | perp.:  31.31\n",
      " b 300/714 >> 2552.3 ms/b | lr:  0.00011 | grad norm: 22.25 | max abs grad:   2.542 | loss: 3.44 | perp.:  31.24\n",
      " b 450/714 >> 2569.3 ms/b | lr:  0.00011 | grad norm: 23.94 | max abs grad:   3.191 | loss: 3.44 | perp.:  31.29\n",
      " b 600/714 >> 2592.7 ms/b | lr:  9.7e-05 | grad norm: 44.75 | max abs grad:   6.593 | loss: 3.45 | perp.:  31.65\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1825.54 sec | train_loss:  3.46 | train_perp:  31.75 | valid_loss:  3.76 | valid_perp.:  42.86\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  65/100) lr = 0.0001096\n",
      " b 150/716 >> 2655.9 ms/b | lr:  0.00011 | grad norm: 22.58 | max abs grad:   3.331 | loss: 3.45 | perp.:  31.64\n",
      " b 300/716 >> 2613.4 ms/b | lr:   0.0001 | grad norm: 37.93 | max abs grad:   4.420 | loss: 3.46 | perp.:  31.95\n",
      " b 450/716 >> 2580.4 ms/b | lr:  8.6e-05 | grad norm: 35.56 | max abs grad:   3.720 | loss: 3.45 | perp.:  31.56\n",
      " b 600/716 >> 2569.2 ms/b | lr:  0.00012 | grad norm: 72.73 | max abs grad:   6.610 | loss: 3.45 | perp.:  31.50\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1847.18 sec | train_loss:  3.47 | train_perp:  32.06 | valid_loss:  3.76 | valid_perp.:  42.76\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  66/100) lr = 0.0001088\n",
      " b 150/723 >> 2475.6 ms/b | lr:  0.00011 | grad norm: 38.86 | max abs grad:   4.015 | loss: 3.43 | perp.:  30.98\n",
      " b 300/723 >> 2496.4 ms/b | lr:  0.00011 | grad norm: 24.98 | max abs grad:   4.027 | loss: 3.42 | perp.:  30.68\n",
      " b 450/723 >> 2518.2 ms/b | lr:  9.9e-05 | grad norm: 34.04 | max abs grad:   3.779 | loss: 3.44 | perp.:  31.20\n",
      " b 600/723 >> 2548.0 ms/b | lr:  0.00011 | grad norm: 35.68 | max abs grad:   5.005 | loss: 3.44 | perp.:  31.16\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1813.21 sec | train_loss:  3.45 | train_perp:  31.40 | valid_loss:  3.76 | valid_perp.:  43.07\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  67/100) lr = 0.000108\n",
      " b 150/715 >> 2506.8 ms/b | lr:  0.00011 | grad norm: 45.42 | max abs grad:   5.238 | loss: 3.44 | perp.:  31.16\n",
      " b 300/715 >> 2617.4 ms/b | lr:  0.00011 | grad norm: 32.41 | max abs grad:   4.199 | loss: 3.45 | perp.:  31.54\n",
      " b 450/715 >> 2572.4 ms/b | lr:   0.0001 | grad norm: 34.18 | max abs grad:   4.047 | loss: 3.45 | perp.:  31.65\n",
      " b 600/715 >> 2494.4 ms/b | lr:  0.00011 | grad norm: 24.10 | max abs grad:   3.439 | loss: 3.41 | perp.:  30.41\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1819.16 sec | train_loss:  3.46 | train_perp:  31.69 | valid_loss:  3.76 | valid_perp.:  42.80\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  68/100) lr = 0.0001072\n",
      " b 150/716 >> 2683.5 ms/b | lr:  0.00011 | grad norm: 28.04 | max abs grad:   5.303 | loss: 3.49 | perp.:  32.90\n",
      " b 300/716 >> 2746.9 ms/b | lr:  0.00011 | grad norm: 25.10 | max abs grad:   4.704 | loss: 3.45 | perp.:  31.46\n",
      " b 450/716 >> 2601.4 ms/b | lr:  0.00011 | grad norm: 22.76 | max abs grad:   3.479 | loss: 3.44 | perp.:  31.06\n",
      " b 600/716 >> 2574.1 ms/b | lr:  0.00012 | grad norm: 32.62 | max abs grad:   3.516 | loss: 3.46 | perp.:  31.72\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1878.92 sec | train_loss:  3.45 | train_perp:  31.62 | valid_loss:  3.74 | valid_perp.:  42.24\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  69/100) lr = 0.0001064\n",
      " b 150/714 >> 2663.4 ms/b | lr:  0.00011 | grad norm: 88.56 | max abs grad:   8.515 | loss: 3.47 | perp.:  32.00\n",
      " b 300/714 >> 2638.7 ms/b | lr:  9.7e-05 | grad norm: 32.38 | max abs grad:   3.727 | loss: 3.43 | perp.:  30.73\n",
      " b 450/714 >> 2820.0 ms/b | lr:  0.00012 | grad norm: 53.72 | max abs grad:   5.558 | loss: 3.43 | perp.:  30.94\n",
      " b 600/714 >> 2766.6 ms/b | lr:  0.00011 | grad norm: 49.05 | max abs grad:   5.359 | loss: 3.44 | perp.:  31.16\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1936.53 sec | train_loss:  3.45 | train_perp:  31.60 | valid_loss:  3.75 | valid_perp.:  42.32\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  70/100) lr = 0.0001056\n",
      " b 150/719 >> 2626.7 ms/b | lr:    9e-05 | grad norm: 42.51 | max abs grad:   4.802 | loss: 3.45 | perp.:  31.60\n",
      " b 300/719 >> 2561.3 ms/b | lr:  9.3e-05 | grad norm: 31.78 | max abs grad:   3.695 | loss: 3.44 | perp.:  31.14\n",
      " b 450/719 >> 2558.7 ms/b | lr:  9.6e-05 | grad norm: 62.11 | max abs grad:   9.842 | loss: 3.44 | perp.:  31.10\n",
      " b 600/719 >> 2490.6 ms/b | lr:  9.3e-05 | grad norm: 39.28 | max abs grad:   4.678 | loss: 3.42 | perp.:  30.52\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1839.83 sec | train_loss:  3.45 | train_perp:  31.52 | valid_loss:  3.75 | valid_perp.:  42.49\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  71/100) lr = 0.0001049\n",
      " b 150/717 >> 2673.6 ms/b | lr:  9.3e-05 | grad norm: 33.38 | max abs grad:   5.668 | loss: 3.46 | perp.:  31.88\n",
      " b 300/717 >> 2586.4 ms/b | lr:  0.00012 | grad norm: 105.12 | max abs grad:   9.050 | loss: 3.42 | perp.:  30.69\n",
      " b 450/717 >> 2528.1 ms/b | lr:  9.3e-05 | grad norm: 57.52 | max abs grad:   6.531 | loss: 3.42 | perp.:  30.65\n",
      " b 600/717 >> 2600.2 ms/b | lr:   0.0001 | grad norm: 23.65 | max abs grad:   2.456 | loss: 3.44 | perp.:  31.12\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1844.24 sec | train_loss:  3.44 | train_perp:  31.33 | valid_loss:  3.77 | valid_perp.:  43.26\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  72/100) lr = 0.0001042\n",
      " b 150/719 >> 2552.6 ms/b | lr:  9.8e-05 | grad norm: 37.96 | max abs grad:   4.474 | loss: 3.45 | perp.:  31.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/719 >> 2628.7 ms/b | lr:  9.8e-05 | grad norm: 35.34 | max abs grad:   4.530 | loss: 3.42 | perp.:  30.49\n",
      " b 450/719 >> 2710.6 ms/b | lr:  0.00011 | grad norm: 103.80 | max abs grad:   8.209 | loss: 3.44 | perp.:  31.28\n",
      " b 600/719 >> 2653.3 ms/b | lr:  0.00011 | grad norm: 40.65 | max abs grad:   4.294 | loss: 3.41 | perp.:  30.34\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1926.74 sec | train_loss:  3.44 | train_perp:  31.25 | valid_loss:  3.74 | valid_perp.:  42.22\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  73/100) lr = 0.0001035\n",
      " b 150/722 >> 2787.8 ms/b | lr:   0.0001 | grad norm: 42.63 | max abs grad:   4.481 | loss: 3.45 | perp.:  31.49\n",
      " b 300/722 >> 2803.8 ms/b | lr:   0.0001 | grad norm: 27.91 | max abs grad:   4.104 | loss: 3.43 | perp.:  30.79\n",
      " b 450/722 >> 2617.2 ms/b | lr:  9.8e-05 | grad norm: 26.41 | max abs grad:   2.832 | loss: 3.41 | perp.:  30.23\n",
      " b 600/722 >> 2512.3 ms/b | lr:  0.00011 | grad norm: 28.77 | max abs grad:   5.141 | loss: 3.40 | perp.:  30.09\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1929.22 sec | train_loss:  3.44 | train_perp:  31.16 | valid_loss:  3.77 | valid_perp.:  43.25\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  74/100) lr = 0.0001027\n",
      " b 150/716 >> 2669.7 ms/b | lr:  9.4e-05 | grad norm: 30.97 | max abs grad:   4.598 | loss: 3.46 | perp.:  31.68\n",
      " b 300/716 >> 2566.0 ms/b | lr:   0.0001 | grad norm: 31.98 | max abs grad:   3.513 | loss: 3.41 | perp.:  30.21\n",
      " b 450/716 >> 2580.7 ms/b | lr:  8.7e-05 | grad norm: 50.07 | max abs grad:   6.828 | loss: 3.40 | perp.:  30.08\n",
      " b 600/716 >> 2665.7 ms/b | lr:  9.7e-05 | grad norm: 57.67 | max abs grad:   5.904 | loss: 3.43 | perp.:  30.99\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1872.58 sec | train_loss:  3.44 | train_perp:  31.24 | valid_loss:  3.75 | valid_perp.:  42.59\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  75/100) lr = 0.0001021\n",
      " b 150/722 >> 2635.2 ms/b | lr:  9.6e-05 | grad norm: 36.69 | max abs grad:   4.850 | loss: 3.43 | perp.:  30.79\n",
      " b 300/722 >> 2521.2 ms/b | lr:   0.0001 | grad norm: 26.32 | max abs grad:   3.871 | loss: 3.41 | perp.:  30.23\n",
      " b 450/722 >> 2592.3 ms/b | lr:  0.00011 | grad norm: 52.18 | max abs grad:   6.502 | loss: 3.42 | perp.:  30.65\n",
      " b 600/722 >> 2595.1 ms/b | lr:  0.00011 | grad norm: 53.33 | max abs grad:   5.226 | loss: 3.41 | perp.:  30.24\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1867.52 sec | train_loss:  3.43 | train_perp:  31.02 | valid_loss:  3.74 | valid_perp.:  42.31\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  76/100) lr = 0.0001014\n",
      " b 150/721 >> 2636.5 ms/b | lr:  8.3e-05 | grad norm: 41.92 | max abs grad:   4.703 | loss: 3.44 | perp.:  31.21\n",
      " b 300/721 >> 2595.3 ms/b | lr:   0.0001 | grad norm: 50.22 | max abs grad:   5.497 | loss: 3.42 | perp.:  30.54\n",
      " b 450/721 >> 2569.7 ms/b | lr:   0.0001 | grad norm: 42.43 | max abs grad:   4.605 | loss: 3.42 | perp.:  30.52\n",
      " b 600/721 >> 2556.3 ms/b | lr:  9.9e-05 | grad norm: 26.65 | max abs grad:   3.055 | loss: 3.40 | perp.:  30.05\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1869.02 sec | train_loss:  3.44 | train_perp:  31.03 | valid_loss:  3.74 | valid_perp.:  42.21\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  77/100) lr = 0.0001007\n",
      " b 150/717 >> 2595.3 ms/b | lr:  9.5e-05 | grad norm: 29.57 | max abs grad:   4.680 | loss: 3.44 | perp.:  31.14\n",
      " b 300/717 >> 2583.4 ms/b | lr:  9.5e-05 | grad norm: 28.65 | max abs grad:   4.061 | loss: 3.42 | perp.:  30.57\n",
      " b 450/717 >> 2627.0 ms/b | lr:  9.8e-05 | grad norm: 26.96 | max abs grad:   3.434 | loss: 3.40 | perp.:  30.05\n",
      " b 600/717 >> 2584.9 ms/b | lr:   0.0001 | grad norm: 35.21 | max abs grad:   4.478 | loss: 3.46 | perp.:  31.84\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1870.52 sec | train_loss:  3.44 | train_perp:  31.04 | valid_loss:  3.74 | valid_perp.:  42.22\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  78/100) lr = 0.0001001\n",
      " b 150/716 >> 2623.8 ms/b | lr:  0.00011 | grad norm: 30.33 | max abs grad:   3.761 | loss: 3.44 | perp.:  31.21\n",
      " b 300/716 >> 2658.0 ms/b | lr:  0.00011 | grad norm: 39.27 | max abs grad:   4.280 | loss: 3.43 | perp.:  30.93\n",
      " b 450/716 >> 2617.6 ms/b | lr:  9.1e-05 | grad norm: 40.39 | max abs grad:   4.364 | loss: 3.41 | perp.:  30.37\n",
      " b 600/716 >> 2625.6 ms/b | lr:   0.0001 | grad norm: 27.35 | max abs grad:   3.341 | loss: 3.42 | perp.:  30.71\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1875.89 sec | train_loss:  3.44 | train_perp:  31.17 | valid_loss:  3.75 | valid_perp.:  42.56\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  79/100) lr = 9.944e-05\n",
      " b 150/715 >> 2688.1 ms/b | lr:  0.00011 | grad norm: 33.60 | max abs grad:   3.788 | loss: 3.46 | perp.:  31.66\n",
      " b 300/715 >> 2567.6 ms/b | lr:  9.4e-05 | grad norm: 32.07 | max abs grad:   5.168 | loss: 3.41 | perp.:  30.18\n",
      " b 450/715 >> 2690.3 ms/b | lr:  9.4e-05 | grad norm: 31.68 | max abs grad:   3.833 | loss: 3.43 | perp.:  30.84\n",
      " b 600/715 >> 2648.9 ms/b | lr:  9.7e-05 | grad norm: 28.77 | max abs grad:   3.700 | loss: 3.42 | perp.:  30.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1882.91 sec | train_loss:  3.44 | train_perp:  31.08 | valid_loss:  3.74 | valid_perp.:  42.22\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  80/100) lr = 9.882e-05\n",
      " b 150/715 >> 2647.9 ms/b | lr:  9.9e-05 | grad norm: 52.88 | max abs grad:   5.853 | loss: 3.44 | perp.:  31.19\n",
      " b 300/715 >> 2637.9 ms/b | lr:  0.00011 | grad norm: 72.88 | max abs grad:   6.064 | loss: 3.40 | perp.:  30.11\n",
      " b 450/715 >> 2626.1 ms/b | lr:  9.6e-05 | grad norm: 28.63 | max abs grad:   3.356 | loss: 3.42 | perp.:  30.49\n",
      " b 600/715 >> 2829.0 ms/b | lr:  9.6e-05 | grad norm: 30.07 | max abs grad:   3.770 | loss: 3.43 | perp.:  30.91\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1917.59 sec | train_loss:  3.44 | train_perp:  31.04 | valid_loss:  3.74 | valid_perp.:  42.15\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  81/100) lr = 9.821e-05\n",
      " b 150/720 >> 2625.5 ms/b | lr:  9.3e-05 | grad norm: 25.11 | max abs grad:   4.611 | loss: 3.43 | perp.:  30.94\n",
      " b 300/720 >> 2604.0 ms/b | lr:  9.3e-05 | grad norm: 33.56 | max abs grad:   4.074 | loss: 3.42 | perp.:  30.68\n",
      " b 450/720 >> 2504.6 ms/b | lr:    9e-05 | grad norm: 31.12 | max abs grad:   3.381 | loss: 3.39 | perp.:  29.62\n",
      " b 600/720 >> 2574.0 ms/b | lr:  9.5e-05 | grad norm: 38.59 | max abs grad:   3.992 | loss: 3.42 | perp.:  30.45\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1854.57 sec | train_loss:  3.43 | train_perp:  30.80 | valid_loss:  3.74 | valid_perp.:  42.23\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  82/100) lr = 9.761e-05\n",
      " b 150/720 >> 2613.7 ms/b | lr:  9.2e-05 | grad norm: 37.35 | max abs grad:   3.376 | loss: 3.44 | perp.:  31.18\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train(\n",
    "        model, train_data, batch_size, seq_len, ntokens,\n",
    "        criterion, optimizer, lr_scheduler, clip, log_interval\n",
    "    )\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(\n",
    "        model, val_data, eval_batch_size, \n",
    "        seq_len, ntokens, eval_criterion,\n",
    "        save_wts = False\n",
    "    )\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    enc_states, attn_states, dec_states = states\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoder(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoder(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, save_wts = True)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 4\n",
    "model.eval(save_wts = True)\n",
    "# Get some data from a random point in the test_data set\n",
    "states = model.init_states(nb)\n",
    "data, targets = get_batch(test_data, 120, seq_len, evaluate = True)\n",
    "data = data[:,:nb].contiguous()\n",
    "targets = targets.view(seq_len, -1)[:,:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output, states = model(data, states)\n",
    "# Convert the output log probabilities to normal probabilities\n",
    "output = output.exp()\n",
    "# Get the argmax of each step in the output\n",
    "output_p, output_idx = output.max(dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted output word indices to the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.t()\n",
    "output_idx = output_idx.t()\n",
    "for i in range(nb):\n",
    "    # Print the output with the targets\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Some basic weight heat maps to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_wts = np.array(model.embedding.weight.data)\n",
    "embed_norm = (embed_wts - embed_wts.mean()) / (embed_wts.max() - embed_wts.min())\n",
    "plt.imshow(embed_wts, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('dim'); plt.ylabel('word index');\n",
    "plt.title('Embedding layer')\n",
    "plt.colorbar()\n",
    "embed_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_wts = np.array(attn.attention.weight.data)\n",
    "attn_norm = (attn_wts - attn_wts.mean()) / (attn_wts.max() - attn_wts.min())\n",
    "plt.imshow(attn_wts, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_input+d_state'); plt.ylabel('d_output')\n",
    "plt.title('Output attention sublayer (in attention mechanism)')\n",
    "plt.colorbar()\n",
    "attn_wts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence attention visualization by mapping the alignment weights (in the attention mechanism) at each step of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = nb//cols\n",
    "fig, axs = plt.subplots(rows, cols, figsize = (30, 20))\n",
    "for b in range(nb):\n",
    "    wts = attn.attn_wts[:,b,:]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    r = b // cols\n",
    "    c = b % cols\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(wts, aspect = 'auto', cmap = 'jet')\n",
    "    # Fix labels\n",
    "    xlabels = list(targets[b].data)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_xlabel('Targets')\n",
    "    ylabels = list(data[:,b].data)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_ylabel('Inputs')\n",
    "    ax.set_title('Example %d' % b)\n",
    "    fig.colorbar(im, ax = ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
