{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from model import *\n",
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead stuff\n",
    "\n",
    "Helper functions for batching, resetting hidden states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 4\n",
    "lr = 0.02\n",
    "warmup_steps = 10\n",
    "decay_factor = 0.5\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 100\n",
    "log_interval = 150  # Print log every `log_interval` batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 512\n",
    "encode_size = 256\n",
    "h_size = 256\n",
    "decode_size = 256\n",
    "n_enc_layers = 1\n",
    "attn_rnn_layers = 1\n",
    "n_dec_layers = 0\n",
    "smooth_align = True\n",
    "align_location = False\n",
    "skip_connections = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate scheduler sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a241b07f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xd8lEX+wPHPbDbJpvdeSKWFEiA0ARVRQVSwIGLlPDwbnIqi6Hnq7+xdvAMreAIWVFRABTmagqBA6ISaBikk2fSyqZv5/bFJSM8GNg3m/XrlRfLsPPPME2C/O8/MfEdIKVEURVGUlmi6ugGKoihK96YChaIoitIqFSgURVGUVqlAoSiKorRKBQpFURSlVSpQKIqiKK1SgUJRFEVplQoUiqIoSqtUoFAURVFape3qBliCp6enDAkJ6epmKIqi9Ch79uzJllJ6tVXugggUISEhxMbGdnUzFEVRehQhxClzyqlHT4qiKEqrVKBQFEVRWqUChaIoitKqC2KMQlGU7qGyspLU1FTKysq6uilKPTqdjsDAQKytrc/pfBUoFEWxmNTUVJycnAgJCUEI0dXNUQApJTk5OaSmphIaGnpOdahHT4qiWExZWRkeHh4qSHQjQgg8PDzOq5enAoWiKBalgkT3c75/JypQdKI1B9LJKlLPbhVF6VlUoOgk2cXlPPzVPt7beLKrm6IoF7xffvmFPn36EBERwWuvvdbk9fLycm699VYiIiIYOXIkycnJDV4/ffo0jo6OvPXWW83W/5e//IXQ0FCio6OJjo5m//79ABw7dozRo0dja2vb4NyUlBTGjx9Pv379iIqK4r333mu23pbON+eeOpIKFJ0kIasYgPVxmRirZRe3RlEuXEajkdmzZ7Nu3TqOHDnCV199xZEjRxqUWbJkCW5ubsTHxzN37lzmz5/f4PW5c+dyzTXXtHqdN998k/3797N//36io6MBcHd359///jfz5s1rUFar1fL2229z9OhR/vzzTxYtWtSkTa2db849dSQVKDpJYnYJYOpZxCbndnFrFOXCtWvXLiIiIggLC8PGxoYZM2awevXqBmVWr17NzJkzAZg2bRqbNm1CStMHuFWrVhEWFkZUVFS7r+3t7c3w4cObTEP18/Nj6NChADg5OdGvXz/S0tLMPt+ce+pIanpsJ0nUF2Oj1SCAdYczGBnm0dVNUpQO9a8f4ziSXmjROvv7O/P89a2/gaelpREUFFT3c2BgIDt37myxjFarxcXFhZycHOzs7Hj99dfZsGFDk0c/kydPZvHixfj7+wPwzDPP8MILLzBhwgRee+01bG1tzbqH5ORk9u3bx8iRIwH48MMPAXjggQfO6546kupRdJJEfQlhng5c3seLdYfPUK0ePylKh6jtGdTXeNZPS2Wef/555s6di6OjY5PX165dWxckXn31VY4dO8bu3bvJzc3l9ddfN6ttxcXF3HzzzSxYsABnZ2fAFCBaCxLm3lNHUj2KTpKYXUI/PycmRvmyPi6TfSl5DOvl3tXNUpQO09Yn/44SGBhISkpK3c+pqal1b/CNywQGBlJVVUVBQQHu7u7s3LmTlStX8uSTT5Kfn49Go0Gn0zFnzpwG5/v5+QFga2vLPffc0+Kgd32VlZXcfPPN3HHHHdx0000Wv6eOpHoUnaCiqprTuQbCPB25oq83NlYa1h7K6OpmKcoFafjw4Zw8eZKkpCQqKipYsWIFU6ZMaVBmypQpLF26FICVK1dyxRVXIIRg27ZtJCcnk5yczKOPPso//vGPJkEC4MyZM4Dpk/6qVasYMGBAq22SUjJr1iz69evHY4891iH31JFUj6ITnM41YKyWhHk54KSzZlykJ+sOneGf1/ZTi5MUxcK0Wi0LFy5k4sSJGI1G/vrXvxIVFcVzzz1HTEwMU6ZMYdasWdx1111ERETg7u7OihUr2qy3/hjFHXfcgV6vR0pJdHR03ThDRkYGMTExFBYWotFoWLBgAUeOHOHgwYMsX76cgQMH1s2QeuWVV5g8eXKDMYqWznd2dm72njqLaO7ZV08TExMju/PGRf+Ly+C+5XtYNXsM0UGurNyTyrxvD9T9rCgXiqNHj9KvX7+ubobSjOb+boQQe6SUMW2dqx49dYLaqbFhXg4AXNXPB2srwU8H0ruyWYqiKGZRgaITJGQV4+Vki7PONDfaxd6a8X28WbU/jUpjdRe3TlEUpXUqUHSCxGzT1Nj6pg0LJLu4gt+O67uoVYqiKOZRgaITJOqLCfNqOC97fF9vPBxsWLkntYtapSiKYh4VKDpYXkkFeYZKwr0a9iisrTTcMCSATccyyS2p6KLWKYqitE0Fig6WmG1KBhjWKFCA6fFTpVGyen/TnC+KoijdhQoUHSxBXzPjybNpSoB+fs4MCHBWj58UxcLONc34hg0bGDZsGAMHDmTYsGFs3ry52fq//fZboqKi0Gg0NJ6a/+qrrxIREUGfPn1Yv349YF6a8bfeegshBNnZ2c1ec9KkSbi6unLdddc1OJ6UlMTIkSOJjIzk1ltvpaLC8k8oVKDoYIn6EqytBIFuds2+Pm1oIHHphRZPnqYoF6vzSTPu6enJjz/+yKFDh1i6dCl33XVXs9cYMGAA33//PZdeemmD40eOHGHFihXExcXxyy+/8NBDD2E0GttMM56SksKGDRsIDg5u8b6eeOIJli9f3uT4/PnzmTt3LidPnsTNzY0lS5aY/bsylwoUHSxRX0wvDwe0Vs3/qqdGB2BtJfgmNqXZ1xVFaZ/zSTM+ZMiQuhxKUVFRlJWVUV5e3uQa/fr1o0+fPk2Or169mhkzZmBra0toaCgRERHs2rWrzTTjc+fO5Y033mg1U8OECRNwcnJqcExKyebNm5k2bRoAM2fOZNWqVeb8mtrFrBQeQohJwHuAFbBYSvlao9dtgWXAMCAHuFVKmVzz2tPALMAIPCylXN9anUKIz4DLgIKa6v8ipdx/7rfYtRKzS5oMZNfn5mDDNQP8+G5PKk9M7IODrcqqolwg1j0FGYcsW6fvQLim9d3dzifNuKenZ12Z7777jiFDhtSlD7/33nt54IEHiIlpeSFzWloao0aNanDtxvtONE4zvmbNGgICAhg8eHCDcrGxsXz44YcsXry4xevl5OTg6uqKVqtt8XqW0Oa7khDCClgEXAWkAruFEGuklPX7crOAPCllhBBiBvA6cKsQoj8wA4gC/IGNQojeNee0VucTUsqVFri/LlVlrOZUTglX9fdptdzMS0JYcyCd7/emctfokM5pnKJcoM4nzXituLg45s+fz//+97+6Y629YZtbb+M04waDgZdffrnBdWrFxMS0ec3OSj9uzsfXEUC8lDKxphErgKlA/UAxFfi/mu9XAguFqbVTgRVSynIgSQgRX1MfZtTZ46XklVJplE0W2zU2NNiVgQEuLP3jFHeO6qUSBSoXhjY++XeU80kzXlv+xhtvZNmyZYSHh1vs2s2lGU9ISCApKamuN5GamsrQoUPZtWsXvr6+bV7P09OT/Px8qqqq0Gq1HZZ+3JwxigCg/gP01JpjzZaRUlZhemzk0cq5bdX5shDioBDi3ZrHWk0IIe4TQsQKIWL1+u65ujlRXzs1tumMp/qEEPzlkhDis4rZHp/TGU1TlAvW+aQZz8/P59prr+XVV19lzJgx7b72lClTWLFiBeXl5SQlJXHy5ElGjBjRYprxgQMHkpWVVZfaPDAwkL1795oVJMD03jF+/HhWrjQ9gFm6dClTp05td7vbYk6gaO7jbeP+Tktl2nsc4GmgLzAccAfmN1MWKeXHUsoYKWWMl5dXc0W6XGLN1NjWxihqXTfYDw8HGz7bkdzBrVKUC1v9NOP9+vVj+vTpdWnG16xZA8CsWbPIyckhIiKCd955p24K7cKFC4mPj+fFF18kOjqa6OhosrKyANMYRe1U2B9++IHAwED++OMPrr32WiZOnAiYBsCnT59O//79mTRpEosWLcLKyort27ezfPlyNm/eXFfv2rVrW72P2NhY7r333rqfx40bxy233MKmTZsIDAysm3r7+uuv88477xAREUFOTg6zZs2y7C8UM9KMCyFGA/8npZxY8/PTAFLKV+uVWV9T5g8hhBbIALyAp+qXrS1Xc1qrddYcvxyYJ6VsOHG4ke6aZvzp7w+yPi6Tvc9eZVb5t9YfZ9Gv8fw2bzzBHvYd3DpFsTyVZrz76ug047uBSCFEqBDCBtPg9JpGZdYAM2u+nwZslqYItAaYIYSwFUKEApHArtbqFEL41fwpgBuAw2a0sVtK0DdNBtiaO0YFoxGCZX8kd1ibFEVR2qvNQFEz5jAHWA8cBb6RUsYJIV4QQtQ++FsCeNQMVj/G2Z5EHPANpkHqX4DZUkpjS3XW1PWFEOIQcAjwBF6yzK12vkR9SbOpO1ri52LHtQP9+GrXafINKv+Toijdg1mT9qWUa4G1jY49V+/7MuCWFs59GXjZnDprjl9hTpu6u8KySrKLy9scyG7sofHhrDmQztIdp3jkysgOap2iKIr51MrsDnJ2ILt9gaKvrzNX9vPhvzuSKCmv6oimKYqitIsKFB3k7NRY8x891Zo9Ppx8QyVf7jxt6WYpiqK0mwoUHSRBX4xWIwh2b//spSHBboyJ8ODjbYmUVRo7oHWKoijmU4GigyTqSwh2t8e6hWSAbZl9eQT6onKVglxRzkFHpxn/v//7PwICApqsicjJyWH8+PE4OjoyZ86cuvIGg4Frr72Wvn37EhUVxVNPPdVsvcnJydjZ2dXV+8ADD5znb8IyVAa6DtLeGU+NjQ73YEiwK+9viWfasEB01lYWbJ2iXLhq04xv2LCBwMBAhg8fzpQpU+jfv39dmfppxlesWMH8+fP5+uuv69KM+/v7c/jwYSZOnNhikr25c+cyb968Bsd0Oh0vvvgihw8f5vDhhjP7582bx/jx46moqGDChAmsW7eOa665pkm94eHh7N/fvfKgqh5FBzBWS5JySto946k+IQRPXN2H9IIyPv/zlAVbpygXts5IM94SBwcHxo4di06na3Dc3t6e8ePHA2BjY8PQoUNJTe05TwtUj6IDpOeXUlFV3a7Fds25JMKTcZGeLNoSz/ThQTjrrC3UQkXpeK/vep1juccsWmdf977MH9FsVp86nZVmfOHChSxbtoyYmBjefvtt3NzczLqH/Px8fvzxRx555BHAlGY8NjaWF154ATDtWDdkyBCcnZ156aWXGDdunFn1diTVo+gACWYmAzTHkxP7kmeoZPHWxPOuS1EuBpZMM/7RRx/VHVu8eHFdkHjwwQdJSEhg//79+Pn58fjjj5vVtqqqKm677TYefvhhwsLCAFMiwdog4efnx+nTp9m3bx/vvPMOt99+O4WFXb/7pepRdID2JANsy8BAF64d5Mfi35O4a3QIXk7NJtNVlG6nrU/+HaUz0oz7+JzdY+Zvf/tbk32sW3LfffcRGRnJo48+2uzrtra2dT2YYcOGER4ezokTJ1rdLKkzqB5FB0jMLsbFzhp3BxuL1Dfv6j6UV1Xzn80nLVKfolzIOiPN+JkzZ+q+/+GHHxgwYECb7frnP/9JQUEBCxYsaLGMXq/HaDRNiU9MTOTkyZN1PY8uJaXs8V/Dhg2T3cltH/8hb1j0u0XrfOaHgzLs6Z/l8YxCi9arKJZ05MiRrm6ClFLKn3/+WUZGRsqwsDD50ksvSSmlfPbZZ+Xq1aullFKWlpbKadOmyfDwcDl8+HCZkJAgpZTyxRdflPb29nLw4MF1X5mZmVJKKWfNmiV3794tpZTyzjvvlAMGDJADBw6U119/vUxPT6+7dq9evaSbm5t0cHCQAQEBMi4uTqakpEhA9u3bt67eTz75REop5erVq+Wzzz4rpZRy5cqVsn///nLQoEFyyJAhcs2aNRb7nTT3dwPESjPeY9tMM94TdLc04yNf2cjYCC/enj647cJmyi2pYPxbvxLl78wX945Uu+Ap3ZJKM959dXSacaUdisuryCwsP681FM1xd7Dh8at7syMhh3WHMyxat6IoSmtUoLCwJAsOZDd2+4hg+vo68fLPRymtUKk9FEXpHCpQWFhituWmxjamtdLwrylRpOWX8sGv8RavX1EUpTkqUFhYgr4EjYBeHbSV6cgwD6YM9ufD3xKJzyrukGsoiqLUpwKFhSXqiwl0s8dW23G5mZ69rj92NlY89d1Bqqt7/mQERVG6NxUoLCxRX9Ih4xP1eTnZ8ux1/Yk9lcfnO1UeKEVROpYKFBZUXS1Jyj6/ZIDmunloAOMiPXl93THS8ks7/HqK0pP01DTjrV1/z549DBw4kIiICB5++OFm05B0GHMWW3T3r+6y4C4tzyB7zf9Jfv5ncqdc73ROiez37Dp595Kdsrq6ulOuqSit6Q4L7qqqqmRYWJhMSEiQ5eXlctCgQTIuLq5BmUWLFsn7779fSinlV199JadPny6llHLv3r0yLS1NSinloUOHpL+/f7PXeP755+Wbb77Z5HhxcbHctm2b/OCDD+Ts2bPrjpeUlMjNmzdLKaUsLy+XY8eOlWvXrm1yfmvXHz58uNyxY4esrq6WkyZNavb81pzPgjvVo7CgumSAnh3fowAIcrdn/qS+/HZCzxdq21RFAXp2mvGWrn/mzBkKCwsZPXo0QgjuvvtuVq1aZf4v5TyppIAWZMlkgOa6a1QvNh7N5KWfjzAqzIMI784JUorSloxXXqH8qGXTjNv264vvP/7Rapmenma8ueunpaURGBjY4J5a2lCpI6gehQUl6otxtNV2aoZXjUbw9i2DsbO24tGv91FRVd1p11aU7kj24DTjLV3fnHvqSKpHYUGJ2abtTzs7D5O3s47Xbh7E/cv38O7GE8yf1LdTr68ozWnrk39H6clpxlu6fmBgYINHVc3dU0dSPQoLStSXnPeududqYpQvM4YH8eFvCfx2Qt8lbVCU7qAnpxlv6fp+fn44OTnx559/IqVk2bJlTJ06tc1rWow5I97d/as7zHoylFfJXvN/ku9tPNGlbbj6nd9k9L/Wy9Q8Q5e1Q7l4dYdZT1L23DTjrV1/9+7dMioqSoaFhcnZs2e3e6ajSjPeDdKMH0kvZPK/t7Hw9iFcN6jzuoSNJeqLmbJwOxHejnxz/2hstKrTqHQelWa8++rwNONCiElCiONCiHghRJOVIkIIWyHE1zWv7xRChNR77ema48eFEBPbUed/hBA9JplRbTLA8E5YbNeaMC9H3pg2iP0p+byy9miXtkVRlAtDm4FCCGEFLAKuAfoDtwkh+jcqNgvIk1JGAO8Cr9ec2x+YAUQBk4D3hRBWbdUphIgBXM/z3jpVor4EISC0i8Yo6ps80I9ZY0P5bEcy38amtH2CoihKK8zpUYwA4qWUiVLKCmAF0HgUZSqwtOb7lcAEYZr6MxVYIaUsl1ImAfE19bVYZ00QeRN48vxurXMl6Ivxd7FDZ91xyQDb46lr+jImwoNnfjhMbHJuVzdHUZQezJxAEQDU/1iaWnOs2TJSyiqgAPBo5dzW6pwDrJFSnqEVQoj7hBCxQohYvb7rZ/kk6kssvqvd+bC20rDo9qH4u+q4f/keUvMMXd0kRVF6KHMCRXOLAhqPgLdUpl3HhRD+wC3Af9pqlJTyYylljJQyxsvLq63iFpFdms1Xx75qsvhFSkmivrjLxycac7W3YfHM4VQYq7l3aSzF5VVd3SRFUXogcwJFKhBU7+dAIL2lMkIILeAC5LZybkvHhwARQLwQIhmwF0J0m63cfk78mVd2vsLR3IaDxFlF5ZRUGLtVj6JWhLcj798xlJNZxTz4+R61cltRlHYzJ1DsBiKFEKFCCBtMg9NrGpVZA8ys+X4asLlmju4aYEbNrKhQIBLY1VKdUsqfpZS+UsoQKWUIYKgZIO8WMkoyAIjNaDgVt7OTAbbXuEgvXrtpINtOZvPkygNqsyPlgtdT04y3dD50bZrxNgNFzZjDHGA9cBT4RkoZJ4R4QQhRu9xxCeBR8+n/MeCpmnPjgG+AI8AvwGwppbGlOi17a5aXacgEIDazYaCoTQbYHXsUtW6JCeKJiX1YtT+d136xbKI2RelOjEYjs2fPZt26dRw5coSvvvqKI0eONCizZMkS3NzciI+PZ+7cucyfPx8AT09PfvzxRw4dOsTSpUu56667WrzO3Llz2b9/P/v372fy5MkA6HQ6XnzxRd56660m5efNm8exY8fYt28f27dvZ926dU3KtHb+gw8+yMcff8zJkyc5efIkv/zyS7t+L+fDrHUUUsq1UsreUspwKeXLNceek1Kuqfm+TEp5i5QyQko5QkqZWO/cl2vO6yOlXNdanc1ct1t9RK8NFHsy91Atzz7CSdSXYGdtha+zrqVTu4WHLg/n7tG9+HhrIu//2m2e6CmKRfXkNOMtna/SjPcgWYYs7LR2FFYUcjLvJH3c+wCmxXZhXg5oNJ2bDLC9hBA8f30U+YZK3vjlODqtFX8dG9rVzVIuUNu+OUF2imXXzHoGOTJueu9Wy1woacYbt1elGe8BjNVG9AY9E4InALA7Y3fdawn64k7Z/tQSrDSCt6cPZmKUDy/8dIQv1YZHygWmuWf3PS3N+LncU0dSPQoz5ZTlYJRGBnsNZl/WPmIzY7mz/52UVRpJzSvlpiGBbVfSTVhbafjPbUO5f3ksz6w6hFYjmD48qO0TFaUd2vrk31F6eprxlu5JpRnvAbIMWQD42PsQ4xNTN05xKseAlN17ILs5NloNH9w5jHGRXjz53UE+//NUVzdJUSyiJ6cZb4lKM95D0oxvTN4oB3w2QMZlx8lVJ1fJAZ8NkMdzj8u1B9Nlr/k/yUOp+R3eho5QWlElZ322S/aa/5NcvC2xq5uj9HAqzfj5pRlv6XwpVZrx89YZaca/OPoFr+16jS3Tt1BuLGfSd5N4esTT5GWM4M31x4n710QcbHvmk7yKqmoeWbGPdYczePyq3sy5IqLTd+lTLgwqzXj31eFpxhXToyetRou7zp0AxwD8HPyIzYwlQV+Mr7OuxwYJMD2G+s9tQ7hpSABvbzjB/62JU4vyFEWp03Pf3TpZpiETH3sfNMIUW4f7Dmdb6jZcc27qceMTzdFaaXjrlsF4ONrwybYksksqeGf6YGy13SMbrqIoXUf1KMyUWZKJt7133c8xPjHkleeRVHDygggUABqN4Jlr+/P0NX35+eAZ7l6yi3xDRVc3S+lhLoTH2Rea8/07UYHCTFmGLHzsz06JGxc4DoGg3OZgt8sae77uvyycBbdGs+90Pje9v4Pk7JKubpLSQ+h0OnJyclSw6EaklOTk5DRZ7d0e6tGTGaSUZBoyGR80vu6Yp50n4c5RHC+L6zGL7drjhiEBBLjZcd+yWG54fzsf3TmMkWEeXd0spZurne/fHfaIUc7S6XQNVna3lwoUZigoL6DcWN7g0RNAL90o4nWLsbPLBzpnT4zONDzEnVWzx3DPZ7u5Y/FOnr++P3eO6qVmRCktsra2JjRUpYW50KhHT2aoTQbo4+DT4Lhd5WAA4gq2d3qbOksvDwdWzR7DZb29eHZ1HE99d4jyKmNXN0tRlE6kAoUZ6gKFfcNAkZnrgHVVIJtPb+qKZnUaZ501n9wdw8NXRPB1bArTP/yDlFy1taqiXCxUoDBD/fQd9SXqi/G3Gc5+/X70hgv7maxGI3js6j58dNcwErNLuO4/v7PpaGZXN0tRlE6gAoUZMg2ZCASe9mdTEFdUVZOSV8pgt0sB2JKypaua16kmRvny09/HEuhmx6ylsbyy9qjaXlVRLnAqUJghsyQTTztPrDXWdcdO55ZgrJYM9etDiHMIG09t7MIWdq5eHg589+Al3DkqmI+3JnLzBztI1Ft23wFFUboPFSjMkGXIajLjKaFm+9NwbyeuCL6C3Rm7KSgv6IrmdQmdtRUv3TCQj+4aRkqegWv//Tsrdp1W8+cV5QKkAoUZatN31Fd/n+yre11NlaxiffL6rmhel5oY5csvj1zKkGBXnvr+ELOWxpJVWNbVzVIUxYJUoDBDZklmk6mxifpivJxscdZZ09+jP33c+rDyxMouamHX8nXR8fmskTx/fX+2x2dz1btbWb0/TfUuFOUCoQJFGwyVBooqi5p59FRMmKcpx5MQgpt738zR3KPE5cR1RTO7nEYjuGdMKGsfGUeYlwOPrNjPvUtjOVNQ2tVNUxTlPKlA0YaW1lAkZpc0SN1xbdi16Kx0fHfiu05tX3cT7uXIygcu4Z/X9mN7QjZXvbOV5X+eUmnLFaUHU4GiDbWBwtfBt+5YbkkF+YZKwutljXW2cebqkKtZm7QWQ+XFvRjNSiO4d1wY/3v0MgYHufDsqsPc9MEODqddPIP9inIhUYGiDc0ttqudCto4vfi03tMoqSzhl+RfOq+B3Viwhz2fzxrJu7cOJjXPwJSFv/OvH+MoLKvs6qYpitIOKlC0IbPE1KPwsj+b9K9uxpNnw6yx0V7RhLuEX/SPn+oTQnDjkEA2PXY5t48M5rMdyVzx1q98sztFPY5SlB5CBYo2ZBoycbF1wU5rV3csIbsYaytBoJtdg7K1g9oHsw9yPPd4Zze1W3Oxt+alGwby45yxhHg48OR3B7nh/e3sSsrt6qYpitIGFSjakGnIbDLjKVFfQi8PB7RWTX99U8KnYKe1479x/+2sJvYoAwJc+PaB0Sy4NZqswnKmf/QHDyzfozZHUpRuzKxAIYSYJIQ4LoSIF0I81czrtkKIr2te3ymECKn32tM1x48LISa2VacQYokQ4oAQ4qAQYqUQokt3BcosaW6xXXGDgez6XGxdmN57OuuS1pFSlNIZTexxhBDcMCSALfMu5/GrerP1pJ6r3v2N51YfRl9U3tXNUxSlkTYDhRDCClgEXAP0B24TQvRvVGwWkCeljADeBV6vObc/MAOIAiYB7wshrNqoc66UcrCUchBwGphznvd4Xhqvyq4yVnM619DqrnYzo2ZiJaz49PCnndHEHsvOxoq/T4jk13mXc0tMEF/sPM1lb27hnf8dp6BUDXgrSndhTo9iBBAvpUyUUlYAK4CpjcpMBZbWfL8SmCBM26BNBVZIKcullElAfE19LdYppSwEqDnfDuiyEc9KYyW5ZbkNAkVKXimVRlm32K45XvZe3BhxI6vjV9cNhiu59vXqAAAgAElEQVQt83bW8cqNA9n42GWM7+vNvzfHM+71zSzcfJLi8qqubp6iXPTMCRQBQP1nKKk1x5otI6WsAgoAj1bObbVOIcR/gQygL/Cf5holhLhPCBErhIjtqP15s0prpsbWS9+RkFU7Nbb1J2L3DLiHalnNsiPLOqRtF6JQTwcW3T6Un/4+lhGh7rz1vxNc+sYWFm2Jp0hNqVWULmNOoGhug+TGn/JbKtPe46ZvpLwH8AeOArc21ygp5cdSyhgpZYyXV8fsV13bG2iwhiLbFChaGqOoFegUyOTQyXx74lvyyvI6pH0XqgEBLiyeOZxVs8cwKNCFN9cfZ+zrW1iw8QQFBhUwFKWzmRMoUoGgej8HAuktlRFCaAEXILeVc9usU0ppBL4GbjajjR2idrFd/VlPifoS3B1scLW3afP8WQNnUVZVxpJDSzqsjRey6CBXPrtnBGvmjGF4iDsLNp7kktc28crao2SqDLWK0mnMCRS7gUghRKgQwgbT4PSaRmXWADNrvp8GbJam1KFrgBk1s6JCgUhgV0t1CpMIqBujuB44dn63eO7q8jw51F+VXdLq+ER94a7h3BBxA18c+4KUQjUD6lwNCnRl8cwY1j0yjiv7+7B4WyLjXt/CkysPcCKzqKubpygXvDYDRc2YwxxgPaZHQd9IKeOEEC8IIabUFFsCeAgh4oHHgKdqzo0DvgGOAL8As6WUxpbqxPRIaqkQ4hBwCPADXrDY3bZTRkkGdlo7nKyd6o4lZhc3Sd3Rmr8P+TvWGmve2fNORzTxotLPz5n3Zgzh13njuXV4EGsOpHP1u1uZ+ekutp3Uq7TmitJBtOYUklKuBdY2OvZcve/LgFtaOPdl4GUz66wGxpjTps6QZcjCx94HU+cGCkoryS6uaHMguz4vey9mDZjFwv0Lic2IJcY3pqOae9EI9rDnxRsG8NhVvfn8z1Ms/eMUdy3ZRaS3I38ZE8KNQwKwtzHrn7aiKGZQK7Nb0XgNRV0yQDMfPdW6O+pufOx9eDP2TapltUXbeDFzc7Dh7xMi2f7UeN6+ZTC21hqe+eEwI1/ZxAs/HlH7eCuKhahA0YpMQ2aT8Qloe2psY3ZaOx6JfpSIrVew4uefLdpGBWy1Vtw8LJAf54xl5QOjubyPN8v+SOaKt3/jzsU7WXvoDJVGFaAV5Vyp/nkLjNVGsg3ZDWc8ZRej1Qh6edi3u75BlaM4XXiQjF9KOT0snWA/f0s2V8GUGiQmxJ2YEHeyruvHil0prNh1moe+2Iunoy23xAQyPSaI0Hb2CBXlYqd6FC3ILculSlY1ePSUkFVCsLs91s0kA2zLqYO5aLQCq2otX36yhepq9Qm3I3k76Xh4QiTb5l/Bp3+JITrIhY+3JjL+rV+Z/tEffLcnFUOFWvWtKOZQgaIFzW5Y1M4ZT7WklCQd1NMrygP7UcU4pfvx3boNFmur0jIrjeCKvj4snjmcHU9dwZOT+pBVWMbj3x4g5qWNzPv2AH8k5Ki9MRSlFerRUwsyDBkAeDuYHj0ZqyXJOQYu7+Pd2mnNyk4tpji3nOHXhnLVyP68fWQlZb84kD4yC3/P9tennBsfZx0PXR7Bg5eFszs5j+/2pPLzoTOs3JOKv4uOqUMCuGlIAJE+Tm1XpigXEdWjaEHj9B1peaVUVFW3e8YTQPLBbBAQMtATa62WSX8ZhHWVLcsWbVCPoLqAEIIRoe68Pm0Qu5+5kvdmRNPH14mPtyZy1btbmbRgK+//Gk9K7sW997mi1FKBogVZhiy0Gi3uOnfAtKsdtH/GE0DSgWx8Q12wdzal/Yju2x+HcSU4nfFjyZc/WK7RSrvZ2VgxNTqA/94zgj+fnsC/pkRhb2PFG78cZ9wbW7hh0XYWb0skPb+0q5uqKF1GBYoWZBoy8bbzRiNMv6KzU2Pb16MozitDf7qI0MGeDY7fM2MqxYFnKNvuzLa9sZZptHJevJxsmXlJCN8/NIZtT47nyUl9qDRW89LPR7nktc3csGg7H29NUD0N5aKjAkULmq6hKMZZp8XDoe1kgPUlH8wGIGRQw0Ch0WiYNecaSnVF/Lk0hazcnPNvtGIxQe72PHR5BD8/PI4t8y7niYl9qKqu5pW1xxj3xhYmv7eN9zae5FhGoUodolzwVKBoQW36jlqJ+hLCvR3r0nmYK+lgNi5edrj5Nl174enqzqX3hGFb4cCnC9ZTXqm2Ae2OQj0dmD0+gp/+Po6tT4znH5P7Ym9jxYJNJ5i0YBuXvrmFf/0Yx474bLWwT7kgqUDRDCklmSWZTRbbhXm2b3yioqyK1ON5hAz2bDHAjBoUjcfVFThl+bLwPyvV4HY3F+xhz32XhrPywUvY+Y8JvHLjQCK9nfhi52luX7yToS9uYM6Xe/lhXyq5JRVd3VxFsQg1PbYZhRWFlBnL6noURWWVZBaWt3t8IuVILtVVktBGj50au+PG6/h3xgp0B0yD23+7s8u24FDawdtJx+0jg7l9ZDCGiiq2nshmy7EsNh3L4qeDZxACBge6Mr6PN5f38WJggAsaTft6pIrSHahA0YzG+1AkZZsGstva1a6xpIPZ2Dpo8Qt3abPs7Ptu4a3XvsLhd19WeW/ihqsntLPVSleyt9EyaYAvkwb4Ul0tOZRWwK/H9Ww5nsWCTSd4d+MJ3OytGRfpxaW9vRgX6YmPs66rm60oZlGBohmN11CcSzLAamM1yYeyCRngicaMlB9WVlbMnnsjC1/4kdM/uLPe/ncmjh17Dq1XuppGIxgc5MrgIFceuTKSnOJyfo/P5rfjerae1LPmgGkzx94+joyN8GJspAcjQj1wtFX/HZXuSf3LbEZdj6IuUBSjEbQrGWBGYgHlJVVNZju1xtHegb8+eRWfvrqJo19WY2uzi8tHjGhf45Vux8PRlqnRAUyNDqC6WnI0o5DfT2az7WQ2n+88xafbk9BqBNFBrlwS7sGocA+GBruhs7bq6qYrCqACRbOyDFkIBJ72pjf5hOwSAt3ssdWa/x836UA2Gq0gOMq9Xdf2dvfgzifG8cXr29m3tBobm31cEj2kXXUo3ZdGI4jydyHK34X7LwunrNLInlN5/B6fzY6EHBZuieffm+Ox0WoYGuzKqDAPRoV5EB3kqgKH0mVUoGhGpiETDzsPrDXWQM0+2e0Yn5BSknQgm8A+btjo2v8rDvT245bHRrLyrd3s/OQMFTMrVc/iAqWztmJMhCdjIkwfSgrLKtmVmMufiTn8mZTDe5tOsmDjSWysNEQHuTI81I3hIe4M6+WGk866i1uvXCxUoGhGZsnZne2qqyVJ2cWMDvMw+/y8DAMF+lKirww65zaEBQRx82OS797dxYHPcjGUbmXyZZeec31Kz+Css+bK/j5c2d/076/AUMnu5Fx2JeeyMzGHD39LZNGWBDTCtId4TC83hoW4MzzEDT8Xuy5uvXKhUoGiGZmGTIKcTG/yZwrLKKusJtzb/B5FS6ux2ys8KJg7nrJh+Vu/Eb9Cw3el/+PmSVefV51Kz+Ji3zBwlJRXse90PruSc4lNzuXbPaks/eMUAP4uOob0cmNYsBtDgl2J8nfBRquWSinnTwWKZmQaMonxiQHq75Nt/oynpAPZeAU74eh2/tMfA7x9ufcfV7H4jfWcWeXFR1kr+dudN6HRqDeAi5GDrZaxkZ6MjTR9CKkyVnP0TBGxp3LZezqfvafy+PngGQBstBoG+DsTHeRGdLArQ4JcCXSza3d2AUVRgaIRQ6WBooqiujUUCVmmQGHuGgpDYQUZSQWMuC7UYm3ydHVnzj+n8P6CH3DY4c87mV8x5+Fp6GxtLXYNpWfSWmkYGOjCwEAX7hljOpZRUMa+03nsS8ln3+k8vtxlmlkF4O5gw6BAFwYHujI4yIWBAa54Oal/R0rrVKBopPHOdonZJTjaas3+z3TqcDZI094TluRo78Dj82/jgyUrsdvrx4IXvuPOR8YT6O1n0esoPZ+vi45rBvpxzUDTv41KYzXHM4rYn5LPwdR8DqQUsPXESWo39fN30TEgwIWBAS4MCDT96emogodylgoUjTRdQ2Ga8WRudz3pQDaObrZ4BrV/34q2WFlZMee+W1nx4zqq1nmw4qVdDL/Ln8uGD7f4tZQLh7WVhgEBLgwIcAF6Aaaxjrj0Qg6m5nMwtYDDaQX870hm3Tm+zjoGBDgT5e9Cf39novydCXBVj60uVipQNNI4fUeivpgRoeathaiqMJJyJJd+l/h16H+oGddfQ2zYYTYvPs7BT/M5efx7/nr7DWrcQjGbg62WEaHuDf5tF5ZVEpdWSFy6KXAcSitg87Gsup6Hi501/fyc6O/nQj8/J/r5ORPp49iu9UVKz6QCRSO1j5687b0xVFSRXlBmduqO1GN5VFVWEzrYqyObCEBM1ABCnvdnyX/WofndjzdOfsUdD11BkI96FKWcG2edNaPDPRgdfnYquKGiimMZRcSlF3L0TCFH0gv5ctcpyipNWY61GkGYlwN9fZ3p6+dEX18n+vg64++iU72PC4gKFI1klGTgbOOMndaOuPQCwPxd7ZIOZmOts8K/t2tHNrGOp6s7TzxzG8u+/ZGq3zz55qVYIqY6MPXKKzrl+sqFz95Gy9BgN4YGu9UdM1ZLknNKOHrGFDyOnSliz6m8uhxWAE46LX18nIj0caKPjyO9a773dLRRAaQHMitQCCEmAe8BVsBiKeVrjV63BZYBw4Ac4FYpZXLNa08DswAj8LCUcn1rdQohvgBigEpgF3C/lLLy/G7TfPV3tqtLBmjG1FhZLUk+mE2vKA+sOnHuukaj4S+3TuXgkGOsXXyA1JUOvBH7OXfdOxE/z47v2SgXHyuNINzLkXAvR64b5F93vKC0kuMZRRzPLOJERhHHM4pYe+gMX+06+9/Xzd6aSB8nevs4EuntRIS3I5Hejng52aoA0o21GSiEEFbAIuAqIBXYLYRYI6U8Uq/YLCBPShkhhJgBvA7cKoToD8wAogB/YKMQonfNOS3V+QVwZ02ZL4F7gQ/O8z7NlmXIqtuwKFFfghCmHc7aknmqEENhxXkvsjtXg3r3pfeLoSxZthq7vT58+a8/CZ5kw83XXKXGLpRO4WJn3WTcQ0qJvqjcFDwyizmZWcSJzCJW70+nqKyqrpyTTkuEtyMRXo6E1/szyM0OrRnZl5WOZU6PYgQQL6VMBBBCrACmAvUDxVTg/2q+XwksFKaPB1OBFVLKciBJCBFfUx8t1SmlXFtbqRBiFxB4jvd2TjJLMunn3g8w7Wrn72KHnU3bg3XJB7IRGkGvAean+rA0na0ts/82nT1H49jw2SGyfnTgjZ1fcd3dwxkQ0bvtChTFwoQQeDvr8HbWMS7ybA9XSklWUTknM4uJzyoiXl9MfFYxv57Q8+2e1Lpy1laCXh4OhHk6EOblWPOnA6GeDrg7qMdYncWcQBEApNT7ORUY2VIZKWWVEKIA8Kg5/mejcwNqvm+1TiGENXAX8EhzjRJC3AfcBxAcHGzGbbSt0lhJbllu3dTYBH1xu8Yn/CNd0Dm0kqgtaRt49weHjg0mw/pFMfCl3ny+8icqfndj89vJ7PLcxI1/vwk3b5+2K1CUDiaEwMdZh4+zrm6Vea0CQyXx+mIS9cUkZpeQkGX6c8vxLCqNsq6cs05LqJcjoR72hHiagkeIhwMhng642KmEiZZkTqBoLmRLM8u0dLy5vmTjOt8HtkoptzXXKCnlx8DHADExMY3PPSf6Uj0Sibe9tykDrL6EmJi2p8YW6EvJTS9h7C2RLRfKOARLrwOvfnDPWrBvX/rx9rKxtuavt91IyuVpbHnmG4r0g/nm6T/wDE7h+qfuw0at6la6KRd7a4b1cmNYL7cGx6uM1aTmlZKUU0KivoSk7GKSsw3sTs5j9YF0ZL13AXcHG3p52NPL3Z5eHg6m72v+9FA9kXYzJ1CkAvXToAYC6S2USRVCaAEXILeNc1usUwjxPOAF3G9G+yym/hqKzMJySiqMZvUoziYBbKWnsPND0OogNxG+uAXuXg22ll+U15jN0s8ZvuMz8m66g6Pp/mSciWLZ/SsJGFHB1fffjZWVmgOv9AxaKw0hnqYew/g+DV8rqzRyOtdAUnYJydklJOcYOJVTwq6k3CZBxMHGimAPB3q52xPkbkewuz1B7vYEu9sT4Gan1oU0w5xAsRuIFEKEAmmYBqdvb1RmDTAT+AOYBmyWUkohxBrgSyHEO5gGsyMxzWQSLdUphLgXmAhMkFJWn+f9tUv9VdntSQaYdFCPu78DLl4t7IBXkg0Hv4Uhd0D4BPjmLvj6Drj9G9B23Cf73C+/JPezz3C78076/fMZRhqNrPvPp5w54EziQT8+m7WMXpfZMP7uGSpgKD2aztqK3j5O9PZxavJaWaWR1LxSTueWcCrHwKkcAym5BuL1xWw5nkV51dm3GSHAx0lHsLs9ge52BLnZE+hmR5C76U9fZ91FObjeZqCoGXOYA6zHNJX1UyllnBDiBSBWSrkGWAIsrxmszsX0xk9NuW8wDXxXAbOllEaA5uqsueSHwCngj5ru4fdSyhcsdsetqN0r29vemz+P5wNtr6EoK6kk/WQBQ65uZZxkz3/BWA4j7gfvvjBlIax+CL6+E6YvA2vL7yNQ9OuvZL70Mo7jx+Pz9FOAKQXIdY/+jTJDMT+/9Rk5yb4c3+nOqd8+I3iMDVf89XYVMJQLjs7ayjSjyrvph77qaom+uLwueKTkGTidayA1t5Q/EnL4oTCtQW/ESiPwc9ER6GZHgKupBxLoakeAmx0Brnb4ueouyB6JWesoamYirW107Ll635cBt7Rw7svAy+bUWXO8yxYBZhoysdPa4WzjTKI+FTtrK3ydW08VfjouB1ktCR3cwrRYYyXsXgJh401BAkw9C2M5/PQYfD4NbvsKdM4Wu4/SuDjSHnscXd++BLz9FqLRm7/O3pGbn5tDUUE+695aSn5aACf2uHN6++f4RFdw9YN3qzEM5aKg0ZwdVG8uVU95lZEz+WWk5pWSkmcgLa+U1DwDqXml7EjIJqOwrEEgEQK8HG3xdzUFDn9XHf6udvi5nA0kPXGMRK3MrifLkIWPvQ9CCBL1JYR6OqDRtP4XmnQwGztnG3x6tfBGf2Q1FJ2B699reDzmr2DjBD/cD8umwB3fWWQ2VOWZM6Q+8CBWri4EfvgBGvsWHocBTi6uTH/xEUqLiln77mfkJnlz6lgQSx9YhWtQJlf+fQZuXt7n3SZF6alstVZ14yLNqaiqJqOgjNR8A+n5ZaTllZKeX0pafilHzhSy8Whmg0dbYNonxM9Fh5+LDn8XU/DwdbHD30WHr4sOPxc73Oytu1UwUYGinsySzLOL7bKLiQ5ya7W8saqa04dziBjmjWgpoOz8ENzDIOKqpq8NusU0oP3NTFhyFdy2ArzOfb2DsbiYlPsfoLq0lF5LvsDa27w3eTsnUw+joryc9e8vI+uANVlZA/j66V046E4w7M5L6D9q1Dm3S1EuVDZaDcEe9gR7NP+BTEpJbkkF6fllpBeYgsiZgjLS80vJKChjZ1IuGYVlGKtlk3r9XHT4OpuCh29Nr8fX5eyf3k62WHfSeIkKFPXU7mxXO/h105DW1/qln8inosxISEtJANP2QOpumPQatLQ6us81MHMNrLgDFl8Jt3wKEVe2u+2yspK0Rx6lPCGBoI8/Qte7/QHHxtaW6+f+DaPRyPZvfiBh4xkKKwey5b/F/PnRB/gN13HFPTOwtVN7MyuKOYQQeDja4uFoy8BAl2bLGKsl2cXldUEko6CMjMIyzhSUkVlQxr7T+WQUlFFhrG5UN3g42LLivpFEeDcdxLckFShqVMtq9AY9Pg4+nMoxIGXbA9lJB7PRWmsI6ttCz+PPD02Pl6LvaP3iwaPgvi3w1W2mqbNXvQCj55j+JZhBSknGCy9Qsn07fi+9iOOYMWad1xIrKysuvW0al94GiQcPsOPTTZRUhpJ40IWU2WtxcDnFsBlj6Duy8bpLRVHay6reOMmQFspIKckzVNYEkVIyC8vJKCgjs7AMD4eOH09UgaJGblkuVbIKb3tvEvS125+2PDVWSknSQT1B/d3RNpfioygD4n6A4bPMG6h2DYa/rodVD8D//gnJv8PU980at8j5ZDH5367E4777cJ02re1rtUPYoMGELRhMmaGYjZ+sIGu/kXzDIDb9t4QdH3yCW+8KLpt1E+4qvbmidBghBO4ONrg72NDf33ITX8ylAkWN2qmxPvY+HIk3BYrWkgHmpBVTnFvO8Gtb2Bs79lOoroIR95nfCFtHmL4cdn1sChYfjoWbP4GQsS2eUrhuHfp33sF58mS8Hm0224lF6Owdue6RewE4uXcPu7/4jZLyANJPe/H1Pw9gJ1fiM9SRy+66GXunzv+HrChKx1GBokb9Vdk/6UvwddbhYNvyryfpQDaIFvbGrio3BYrIq8EjvH0NEQJG3g9BI2HlPfDZdaafJzwHNg0Dl2HvXtLnP4Xd0KH4vfoKopOyxEYOHUbk0GEYjUZ2fLeaxM2nKKsMJ/GgI6fmbkWnScA/xo1xt92EnVPHrz5XFKVjqUBRo/6q7ITs422PTxzIxjfUGXtnm6YvHv4eSvQw6oFzb5B/NNy/DTa9YJo5deIXmPIfCL0UgIpTp0h9aDZaP18CFy1E0wXrHqysrBg3/SbGTYfSomK2frGSM3sKKKuO4OReOxJ3/YpOJOI1yIFLZlyvptoqSg+lAkWNzJJMtEKLm60bifpipkb7t1i2OK8M/ekiRt/YTG9BStj5AXj2MS2yOx+2jjD5Deg/BVbPgaXXw8DpVI2YR8p9psdMwR99hNat9Wm8ncHOyZGJD/wFgKKCfLZ/+QMZewspk2Ekxzlw+h970RkTcQ6pZuhNlxE6YGDXNlhRFLOpQFEjy5CFl70XuSVVFJVVtZrjKflQDkDzmxSd/hPOHIBr3zZ71lKbQsbCgzvg93ep3rqA1Pd+pTLXluBPF2MTEmKZa1iQk4srkx68B4AyQzHbv/2RtJ0ZlBkDyUj3YO1CPbZly9C5ZhM8uhcjp0xWU24VpRtTgaJGpiGzQTLA8GbywtRKOqDHxcsON99mFtns/BB0LjD4Nss20MYeefnTnPkhhdKs3/Afrcd+x99A9xwMuLnldRpdTGfvyISZt8FMMBqN7N+4iRMbD2Moc6GgdBCHtmg4smETttWncAoy0n/icPpfMrqrm60oSj0qUNTINGTS170vidm1+2S3sGS/rIrU43kMvDyw6RL7glQ4+iOMfqjJwLMl6P/9bwo3/IbX3Lm4XBkBG56D7++FHf+G8f+A3pMs14vpAFZWVgybeDXDJl4NQObpZHZ9t4HcYyWUVweSmelO5rJStn/yLdbWabiE2TB48hjCBg3u4pYrysVNBQpqtmU0ZHFZ4GUkZhVjq9UQ4Nr8o5CUI7lUV0lCm3vstHsxIGH43yzexvzvviPnw49wvWUaHvf9zRQQQi+HQ9/Cr6/AVzPALxoufwoiJ3bbHkZ9PsEhXD/X9LsyGo0c2bGDYxv2UZxqRVl1OCXJDqS/n4NN+dfYWJ/BOdSGflcMIzImRmW5VZROpAIFUFhRSGlVKd723vzWRjLApIPZ2Dpo8QtvtBy/wgB7PoM+k8Gtl0XbV7JjB2ee/z8cLrkE3+eeO9uT0Whg8K0w4CY4+DVsfdMUMLz6wSV/h4G3gLaZWVndkJWVFQPHjWPguHEAVJSXs3/DJhK3H6c005bS6nCKTzmQ/t8Stn60GmuRhoNvNYHDQxl69RXo7NU0XEXpKCpQ0HANRaK+uMWVj9XGapIPZRMywBNN42Rch76F0jwY9aBF21Z24gSpDz+CbWgoAe8tQFg3sxewlTUMuRMG3WpaDb79PdN+F5tfhJhZMOwv4NhCPqpuysbWlhHXTWbEdZMBqKqsZN/GzST/cZzidEFltT/6HDf0v8CBn3/HpjING8cC3CKc6HtZDKGDBqleh6JYiAoUmGY8AXjYepOSl8F1g5qfGpuRWEB5SVXT2U5SmgaxfQZCr/PLs1RfZVYWKQ88gLDTEfTRh1g5tZH4y8oaBk039SQSNsGOhbDlJdj6BkTdBDH3mBbydeNxjJZora0Zfs1Ehl8zse5Y/L59HNmyi/yEEior3SiqiKLwmDWnjuWhrfwRG5mOjWsZnr3d6Xf5SIL79uvCO1CUnksFCs6m76iscMRYLVtcbJd0IBuNlSC4f6MNTpK2QtYR0851FnoTrjYYSH3wIYx5+fRavhxr/5bXdTQhhCkDbcSVoD8Buz+B/V/BwRWm9R1D7zb1PnpYL6OxiCFDiBhyNo2aoaiQfRt+JW1fEoYMqKryJr/Ei/z9GuL3n8G64ghasrB1KcU1zIWIUYOIGDpU9TwUpQ0qUGDqUQgEeYWmAeywZpIBSilJOpBNYB83bOwa/dp2fgT2HqZP8hYgjUbSHp9H2dGjBC5ciN2AqHOvzKs3TH4TJjxveiy1dyn87xnTjKnIq2DwDNNsqQ7YjrWz2Ts5M+amKXDT2WPZ6Wkc2vw7mXFnKNNrqKryJN8QSX6chuS4IrZ8tA7rqgys7Ytw8LXGd0Aw/ceNUqvIFaUeFSgwjVF42HlwOqcCaD69eH6mgQJ9KdFXBjV8ITcJjq+FcY+Ddevbpprdntdep3jLFnz++U+crjjP1d21bB1h6F2mr6yjcGAFHPzGlBrExhH6XmtajxE2vscMgJvD0z+A8Xfe2uBYnj6LuN+2k3E4hZIMI1VVrpRU9qUo3ZaMdNi//iA2FbloRTbWTmU4BtgRMDCMvpeMxMnFtYvuRFG6jgoUQIYhA297bxL1xXg52eKsazpgnHQgG2hmNfauT0BjZUonbgG5y5aTt3w57jPvxv3ONvaxOFfe/eCqf5kSDSZthbjv4cga08wpW2dTD6PfdRA+wRRgLjBuXt6MnXYj1MvIXlVZyYk9e0jaFUf+qQIq8q0xVntQWupFQaIVaYmwa1UsNhU5aEUOWggzeUMAABrWSURBVIcyHHxt8e7jT+SIIfgEh3TZ/ShKR1OBAtOjp0DHQBJTSlpcaJd0IBuvYCcc3er1GsqLYd9y6D8VnNsxhtCCok2byHz1VRyvnID3k0+ed31t0lhB+HjT1+S3IfFXOLoajq2FQ9+Ala0pCWGfSabg4dL6jn89mdbamv6jRjXZ8rW0qJijf/xJ6sEEitJKqKiywWh0o7Q8nMIUa86kwIGNiWgrD6A1ZqO1KcLGTeIc6Ix/3xAiRgxTvRClx1OBAtNg9jDvYWzVF3PNgKYb8BgKK8hIKmi698SBr6C8EEae/5TY0kOHSZv3BLoBAwh4801EZw+wam2g99Wmr+uq4PQOOL7O9PXz46Yvr34QMcH0FTz6ghjXaIudkyNDr76SoVc33J62vLSU+L17OX3gBAWn8inPE1QZnSgzhlBc4ExuASTHwY6VsVhX5qOVuVjZGrB1kTgFOOPdO5CIodG4eft00Z0pivku+kBRWlVKYUUhTtYe5BsqCW9mfOLU4WyQEDq43mOn6mrTILb/UAiMOa82VKalkfLgg2jd3Ah6fxGark6QZ6U19SRCL4WJr0D2CTi5AeI3mDZV+mOhqbfRazSEXmYq5xdtOu8iYWtnR9SYMUQ1s+1sVspp4vfsJ+tEGiUZpVQWajFWOVFWGUZxgSM5BZB8BHatikNb+QdaYx4abRHWDlXYedrgGuyBX59QwgYNUAsJlW7h4vmf3YLaNRRUmVZaNzeQnXQgG0c32/9v78yj5Crue//59b5Nz75pZiSNFoSFwAbE4i3mODbGeQRIXmIUJ3nES0CY1cEhxie2McHh4ZMYAjYyi8E4MfA4mBOI43iJzQvExsJi8SCBZCTNaPalZ+npfa38UXeY1miW1oxGI3XX55w6t27dutVVqlF/u7bfj7rWgv+0B34Oo2/BHz64pC2xuUiEnu3bUakUbd95BEf9CbZlVQTqN+nwnmv1dNuhX+ppqoPPwc++ovO5KrTv7zXv0WHVmeA4/j4yTgQa2lbT0LZ61mfDPd10vvY6w2/1ER2Ikp60kcv7yOaaiCerCffZGeyDvS/GeU79CmdmAns+jN0Zw+HL4q1xEmypon5DG+2nb6GydhZTMgbDMabshWLqDEUyoQ+zzTQvnk3n6HlzjHe8u/lwI4A7d0CgETZftujPVpkMfTfcQKqzi9UPPYh7w4ZFl3XccAemp6gAoiPQ9YIOh345LRx2N7ScBW3n6kN+LVuhwkyzzCciqUSCrt276X1jP+GeURKjaTJRO7m8n3SumViyivCAncEB+O2uPL94ogNHJoo9F8Zui2B3p3BWKLx1XqpaamlYv5q1p202rmkNS8YIhWW+YzzixWlP0Vp9+LRP795xsuk8awunnUJvwf7/hAu+sOitpEopBm69ldgvX6T5jjvwz1hEPWkI1GtbU1uswwuxEHS/qP1y9OyEF+/TJkUAqlZrwWg5S0/ZNb+zJHdVLRa318umc85h0znnzPo8lUhw6I09DOw7yFh3iEQoSSYq5PIecvlKUplKcpM+mIS+g7DnhTiwC0cmgj03ic0Wxe5K4fDl8VS7qGgMUr26kZaN62lc224OHhrmxAiFJRRDYx7W1DpwzLDh1NkRwumx03JKgRe5nfeD3aVNYiyS0fsfIPz9p6n7zNVU/cHiRyUnHP46eMfv6wCQScLAa9C7C3p/rcOep63MAnWnaMFY9S5oOl0H78p77DsRcXu9nHL2Vk45e+41sbGhAbr37GWks4/IYJjEWIpszEZeucnlA2Qyq8jGKyAOg33AKwDdSP4AjmwEez6CzRbH5krj9CvcVS4C9RVUtdTR0L6aVes3GCdTZUhRQiEiFwH/BNiBh5RS/3fGczfwXeBsYBS4XCnVZT27BfgUkAOuV0r9eL4yReRa4EZgPVCvlAotsY3zMhQbIugKcqg/w/oZJ7JVXtHVEWLNabXYHZaAJCbgtcdgyx9BYHGnd8M/+HdG7r6b4CW/T9111y21CSc2To9eu1hdMGKKDkP/q1Z4Dbr+W2/HnaJyNTSeBk1b9LXhNKhZV1aL5YulprGZmsYjd+4VEo9M0rN3H8MHe5joHyUeipGO5MklHOSVh1y+ilQmSDTmhxjQB7wGMAJqCEc2hj0XxSYxbPYkNk8Wp1/wVLrw11VQ2VxL3eoWVq3bgLfCjBhLgQX/54mIHfgm8GGgF/i1iDyrlHqjINungHGl1AYR2QbcCVwuIpuBbcBpwCrgP0XkFOuducr8BfAD4P8fiwYuxHB8mAZfA7vH4lx4WtNhz4YOTRKfTB9+yO6170EmBuddtajPi+/axcAtt+DbupXm228/0vlRORBogFM+osMU0WEY7IDB13UY2gNv/QRUTj+3u7SdqoZToX4qbILqdiMgR4mvIjjvFNcUkfAEA/v3M9LVy0T/KInRGKnJLNm4jbxykc97yeSqyKaC5LMuCAPdU2+PAS9hz8a1qKgYNlsSmyuD3ZPH5bfjrnLjqwlQ2VRHXVszze3rjbCcoBTzP+xcYL9S6iCAiDwBXAoUCsWlwK1W/CngG6K/AS8FnlBKpYBOEdlvlcdcZSqlXrXSltKuohmKDxF01pHJqSMO23X9JoTYhDVbanVCPqennVa/W0+VHCWpzk56r7kWZ2srrd+4F5urdExlLJlAw7QhwykySRjZq8PQHm16pPtX2qT7FDYn1K6H2g16GqtuI9Ru1Gm+miM/x1A0FZVVVCww1TXF+PAQA/sPMtozwOTwOImxOKnJDLmEkEs5UHk9Ukln/GRVAJI2PfdwYKqEUWAUezaBPRfDpuKIJLE50thcWRxecPkduKu8+GsrqGiopnZVE/Vr1pgDjceBYoSiBegpuO8Fzpsrj1IqKyJhoNZK/9WMd1us+EJlzouIXAlcCbB69ey7SIphKD7EhoB+f6YxwM6OEKs2VuLxWyY9fvsjmDgEH77tqD8nOzZGz1XbwWbTJsOrzB/3gjg9WpBninIqoq3ihvbByD59zmNkn+6ffHY6n7dGC0bNOh2q2614uzbiWI6juWWiuqGx6MOD2UyG4e5DDB/qYWJghGhokuREgnQkQzYB+bSDfNaFUl4y2VqyBMjn3BAFhgpL0uJiy6Ww5+LY8nFskkRsKWyOLHZ3HrtXcAWceCq9+KoDBOqrqG5soLa1hWBNrVnAL5JihGK2/02qyDxzpc/mp3NmmfOilHoAeABg69atR/XuFJl8htHEKOvcevtg4WG78EiCsf4Y7/vjjdMv7PwWBFvh1IuP6nPyqRS911xLdnCQ1Y9+B9cShM0AuCug9WwdCsllYPwQjO63wlswdlBv2+14ksP+xFwVUL1WeyOsXqt3ZFWt0feVbWY31jLicDpZtX4Dq9YXvx08Hplk6NAhQr39RIbHiY1GSE0kSMey5JKQS9lROMnn3eTztaSzXnLKj0rb9ZRYX2Fp48A4ks9aApPAppKIpBB7xhIZhcMjOP1O3EE3nio/gZogwbpaqpsaqWluLqtF/WKEohcoNJnaCvTPkadXRBxAJXqScr53Fypz2QnFQygUiUQFNX4XVb7pqaCujikjgNa009AebUDvQ7ce1Zy4yufp//znSbz6Ki1334WvwH+C4Rhjd0LdBh1mkknq0eBYJ4x3wniXDqMHYP/PIJs4PL+3BqratGhUtul4sEXbu6psBX/DSeGXvFTwVQRp33I67VtOL/qdXC7H5GiIkZ4eJgZDREcniI9FSE2mSMcyZBOKfNpGPmNH5fUIJperJqd85JQXUjYtMod9MyWALqALWy6JPZfAlk8ipCyhyWJzZLG5FA43OLwOnAEnngoP3ko/vupKgnU1VNbXUd3YdNKITTHfeL8GNopIO1qXtwEfn5HnWeAK4EW0Tc6fK6WUiDwLPCYiX0cvZm8EXkKPNBYqc9mZ2ho7Mek7Yn2is2OEmlV+Kut9OmHn/eDwwllXHNVnjNx1N5H/+BENf/05ghdddEzqbVgETs/0CfOZKAWxET0amTgE4R6Y6IaJHj0yOfCc3sBQiM0BFaugskUbhKxo1kISbNbpwWYINJWUyfaTDbvdflRTYoXkcjnCoWFG+/qZGA4RGw0TH4+QiqTIRDNkEjlyKchnbKisE6WcKOUjl/NYO8e8qLQDIsDwzNIjVui0ps2SiEpiUylE0ohktOA489icCrtHcHrsOPxO3AEPngof3ko/gZoqgrU1NK1bj8u9vFYQFhQKa83hWuDH6K2sDyul9ojIbcAupdSzwLeBf7YWq8fQX/xY+Z5EL3xngWuU0ttYZivTSr8euBloAjpE5IdKqU8f01ZbTAnF4LiLD62fFopkLEP/W2HOvNCaIoqPaRPcZ1x+VAuk408+yeiDD1J1+eXUfPKTx7TuhmOIiF5MDzRA2yw7gZTS/tAn+yDcq8NkH0z2Q7gP+l6ByABkk0e+66vTIlLRqIWjwgqBBuu+UY9OXL7lb6ehaOx2e1Fbjecil8sRmwwzMTjI+LAezSTCEZLhOOloikw8SzaZJ5cGlbGRz9pReSdKuckrP3ncWnBynumRzRGkgUHe+7FB3vXBDy6luQtS1ByKUuqHwA9npH2pIJ4EZnXvppT6KvDVYsq00u8B7immXktlynzHWNh32EJ2955RVF5NGwF85VH9JXDe9qLLjr7w3wx+5Tb8738/TV/82/LcBlsqiOgfCL4afSBwNt4Wk34tGpP9EBnU8cggRAf19GV0eHrLbyGuimmxCjRo8fDX65Pv/sJQp32GmL+nExq73U6wuoZgdQ2rl+CqPZfLMTk2Snh4mMjYONGxCeITUVKROKloimw8S8umP1y4oCVS1hvQh+PDuGweyHsPm3rq7AjhDbpoXBOEXBZeekhbSW3cXFS5yX376LvxRtwbN9Jy112Io6z/mcuDw8Rky9z58jmIj1riMaSFIzo0HY+N6G3Asee18MyG3aVHKv5aLR6+Oi0gvhq9m8tXGK/VJ93tRzrjMpz42O12qusbVtw1b1l/gw3Fhwg4ahlF3h5R5LJ5unePsuHsBsQmsOffYLJX+50ugszQMD1Xbcfm99P2rR3YA7M7QjKUKTb79KhhIbJpiIe0gMRD2o7W2/FRLSrxkN7ZFRuFdGTustyV4KvWi/S+moKrleattp5bwVMFnkpdX0PZU9ZC0eRvosaeoc8mrKnVc8T9v50gncyx9p2Wue+d9+vtk4WniOcgH4vRc/V28pOTrPnev+BsalrwHYNhThwuvVBerPfEbEqPVmIhSIzpeHzMCqM6JMb1NfSWNkeTmnXy20LAE9Si4a2a/eqptOKV4Km2rkF9LVMz86VIWQvFTVtvYv/elwnXRHBaxgA7O0I4nDZaT63Wdoi6X4SP3LHgLyuVzdL3VzeR2ruPth334XnHEiYmDYbF4HAfnbCAPnuSmNACkhjXApOYgORUmnVNTuh4ZGD6eS49f9l2tyUclni4g4fH3cGCeIUVr9CjH3eFDk6vWY85AShroQA4MBJ921mRUorO34zQtrkGp8uuD9i5AnDmn85bhlKKob+/g+h//RdNt36ZwAc+cDyqbjAsHbtTL5gHFuEwK5OYFo3kJCTDVjxcEJ/U7oKnrpN90/FMfOHPELslGkF9CHJKQFwz44HptML7mXFz9mVRlLVQ5PKKrtE4F2zS88Wh3ijR8RTnXNyu54J3fx/O/gv9K2gexh59lPHHHqPmk5+ketu241Bzg+EEwOnVIbi4LaTkMtocS2pSX5PWNRXRU2KpaMHzqHWd1FNpE93WsyikoxRt2MHpA5ffCoE5rjOC06+3LxfGp8pxWvESF6CyFoq+8QTpbP7tHU9dHSEQWHt6Hey6Sw+tz71y3jImf/pThu/8GhUXXkjD5246HtU2GEoDu3N6p9hSyOf1gcgp0UhF9DU9lRbR8XTMembFp/Ikw3qkk45Pp+VSR1cHh9cSkAIhcfoOjzu9M+5902L7drwwreDe4VnRKbiyFooDoSgwbQyw8zchmtqD+HzArm/Dhg9ra6RzkOjooP+vb8Zzxums+tqdSIn/qjAYTkhstulpqGNFLqvFJx2bFpBMfEY8NuMa19fCeHRYT9EVps80F1MsjkLx8Ezf/+8H9YabZaSsheLgiDbLsK7eT3Q8yUh3hPMvWwdv/Kve137+3Afs0r199Fz9GRx1dbTddx82j+d4VdtgMCw3dgfYKxecdl4U+bw+wFsoIJl4wf3Us5iOZxPTApMpCNmkzm9b/jMyZS4UUYIeB7V+F3ue1+Yl28+ogx/s0D4N1s1+LD4XDtNz1VWoTIa27z6Ko7b2eFbbYDCczNhs1pqHD+2N4cSnrOdKDo7EWFcfQETo7AgRrPdSnX0D+l/RHuxmmUpS6TS9199Aurub1nvvxb1+/QrU3GAwGI4fZT2ieOQT5zAeT5NOZundN87pF7QiL92h93G/80+OyK+UYuCLXyK+cyervnYn/vPOnaVUg8FgKC3KekThcdpprvTSvWeMfFbRvl7gjWfgrD+f1XFN6L77CD/zDHXXX0flJZesQI0NBoPh+FPWQjFFV0cIt99Bc+hxbbTt3L88Ik/4mWcI3fsNKi+7jLqrr16BWhoMBsPKUPZCkc/l6dodYu3mamyvPgKbfu+IrWaxnS/R/7dfxHfuuTTf9hVjMtxgMJQVZS8UgwfDpGJZ1lbu1cbSZmyJTR04QO911+Fqa6P13nsQl/FYZjAYyouyF4qDvwlhcwirB3dAw2ZY+/63n2VHR+m5ajvidNL2wP3YK5dhT7XBYDCc4JS1UGgjgCFa28AVell7sLOmlfLJJD2f+QzZUIi2Hffham1d4doaDAbDylDWQjE+GGdyJEG78xfaWcsZHwNA5fP03/w3JDteZ9XX7sR7xhkrXFODwWBYOcpaKLo6QgCsnXhEW4l1egEY/od/JPKTn9Bw880EL7xwBWtoMBgMK095C8XrIeqrIgTs43DOpwEYf/xxxh5+mOqPf5yav7hihWtoMBgMK09ZC8XFf7mBD/nugM2XQGUr0eefZ/DvbifwgQ/Q+IVbzDZYg8FgoMyFwvXbp6jJvwnnbSf55pv03fhZ3KduouXr/4g4ytq6icFgMLxNeX8b7n4amt9FxrmGnqu2YQsGadvxLWx+/0rXzGAwGE4Yylso/uz75Ab307P9avKxGGseewxnY8NK18pgMBhOKMpaKJQ46LvtHlL799N2//14Np2y0lUyGAyGE46yXaNQSjH4d7cTe+EFmr78JQLve+9KV8lgMBhOSIoSChG5SET2ich+Efn8LM/dIvL/rOc7RWRtwbNbrPR9IvKRhcoUkXarjLesMpfNuJJ7XTu1V15J9cc+tlwfYTAYDCc9CwqFiNiBbwIfBTYDfyIim2dk+xQwrpTaANwF3Gm9uxnYBpwGXATcJyL2Bcq8E7hLKbURGLfKPuaICDVXXEHDX312OYo3GAyGkqGYEcW5wH6l1EGlVBp4Arh0Rp5LgUet+FPA74o+hHAp8IRSKqWU6gT2W+XNWqb1zgetMrDKvGzxzTMYDAbDUilGKFqAnoL7Xitt1jxKqSwQRnsNn+vdudJrgQmrjLk+CwARuVJEdonIrpGRkSKaYTAYDIbFUIxQzHY8WRWZ51ilH5mo1ANKqa1Kqa319fWzZTEYDAbDMaAYoegF2gruW4H+ufKIiAOoBMbmeXeu9BBQZZUx12cZDAaD4ThSjFD8Gtho7UZyoRenn52R51lgyoLeHwE/V0opK32btSuqHdgIvDRXmdY7z1llYJX5zOKbZzAYDIalsuCBO6VUVkSuBX4M2IGHlVJ7ROQ2YJdS6lng28A/i8h+9Ehim/XuHhF5EngDyALXKKVyALOVaX3k3wBPiMjtwKtW2QaDwWBYIUT/iD+52bp1q9q1a9dKV8NgMBhOKkTkZaXU1oXyle3JbIPBYDAUR0mMKERkBDh0FK/UoRfOy4lybDOUZ7vLsc1Qnu1eapvXKKUW3DZaEkJxtIjIrmKGW6VEObYZyrPd5dhmKM92H682m6kng8FgMMyLEQqDwWAwzEu5CsUDK12BFaAc2wzl2e5ybDOUZ7uPS5vLco3CYDAYDMVTriMKg8FgMBRJWQnFQg6YSgURaROR50TkTRHZIyI3WOk1IvJTyynUT0WkeqXreqyx/J28KiI/sO6PmyOslUJEqkTkKRHZa/X5u0u9r0Xks9bf9m4ReVxEPKXY1yLysIgMi8jugrRZ+1Y091jfbx0ictaxqkfZCEWRDphKhSxwk1LqHcD5wDVWWz8P/MxyCvUz677UuAF4s+D+uDjCWmH+CfiRUupU4J3o9pdsX4tIC3A9sFUptQVtBmgbpdnX30E7fStkrr79KNqe3kbgSmDHsapE2QgFxTlgKgmUUgNKqVeseAT9xdHC4Q6mSs4plIi0Av8LeMi6L3lHWCISBH4HyyaaUiqtlJqgxPsabafOa1ma9gEDlGBfK6WeR9vPK2Suvr0U+K7S/Aptibv5WNSjnISiGAdMJYflv/xMYCfQqJQaAC0mQMPK1WxZuBu4Gchb90U7wjqJWQeMAI9YU24PiYifEu5rpVQf8A9AN1ogwsDLlH5fTzFX3y7bd1w5CUXRTpFKBREJAN8HblRKTa50fZYTEbkYGFZKvVyYPEvWUutzB3AWsEMpdSYQo4SmmWbDmpO/FGgHVgF+9LTLTEqtrxdi2f7ey0koinHAVDKIiBMtEt9TSj1tJQ9NDUWt6/BK1W8ZeC9wiYh0oacVP4geYZS6I6xeoFcptdO6fwotHKXc1x8COpVSI0qpDPA08B5Kv6+nmKtvl+07rpyEohgHTCWBNTf/beBNpdTXCx4VOpgqKadQSqlblFKtSqm16L79uVLqTylxR1hKqUGgR0Q2WUm/i/b/UrJ9jZ5yOl9EfNbf+lSbS7qvC5irb58F/o+1++l8IDw1RbVUyurAnYj8HvpX5pSzpK+ucJWWBRF5H/AC8DrT8/VfQK9TPAmsRv9n+2Ol1MyFspMeEbkA+JxS6mIRWYceYdSgHWH9mVIqtZL1O9aIyLvQC/gu4CDwCfSPwJLtaxH5CnA5eoffq8Cn0fPxJdXXIvI4cAHaSuwQ8GXgX5mlby3R/AZ6l1Qc+IRS6pg46ikroTAYDAbD0VNOU08Gg8FgWARGKAwGg8EwL0YoDAaDwTAvRigMBoPBMC9GKAwGg8EwL0YoDAaDwTAvRigMBoPBMC9GKAwGg8EwL/8DMKGc9ykV69cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a23f369160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps*2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.4g:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps*2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 9464337\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    src_vocab = ntokens, tgt_vocab = ntokens, embed_size = embed_size,\n",
    "    h_size = h_size, encode_size = encode_size, decode_size = decode_size,\n",
    "    n_enc_layers = n_enc_layers, attn_rnn_layers = attn_rnn_layers,\n",
    "    n_dec_layers = n_dec_layers, align_location = align_location,\n",
    "    skip_connections = skip_connections, smooth_align = smooth_align,\n",
    "    dropout = dropout\n",
    ")\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "eval_criterion = LabelSmoothing(ntokens, smoothing = 0)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, decay_factor, optimizer)\n",
    "# Reference\n",
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12562, 74]), torch.Size([7376, 10]), torch.Size([8243, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100) lr = 2.795e-05 (warmup)\n",
      " b 150/713 >> 1952.3 ms/b | lr:  2.8e-05 | grad norm: 2.70 | max abs grad:   0.114 | loss: 7.08 | perp.: 1184.49\n",
      " b 300/713 >> 1954.2 ms/b | lr:  2.8e-05 | grad norm: 2.34 | max abs grad:   0.099 | loss: 6.13 | perp.: 461.23\n",
      " b 450/713 >> 1909.2 ms/b | lr:  2.8e-05 | grad norm: 2.26 | max abs grad:   0.098 | loss: 6.05 | perp.: 422.25\n",
      " b 600/713 >> 1947.8 ms/b | lr:  2.9e-05 | grad norm: 1.92 | max abs grad:   0.070 | loss: 5.99 | perp.: 400.42\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1381.48 sec | train_loss:  6.25 | train_perp: 517.11 | valid_loss:  5.87 | valid_perp.: 354.24\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   2/100) lr = 5.59e-05 (warmup)\n",
      " b 150/717 >> 1941.8 ms/b | lr:  4.9e-05 | grad norm: 2.46 | max abs grad:   0.096 | loss: 5.94 | perp.: 378.96\n",
      " b 300/717 >> 1933.9 ms/b | lr:  5.4e-05 | grad norm: 1.89 | max abs grad:   0.068 | loss: 5.86 | perp.: 349.04\n",
      " b 450/717 >> 1967.2 ms/b | lr:  6.3e-05 | grad norm: 2.64 | max abs grad:   0.052 | loss: 5.80 | perp.: 329.89\n",
      " b 600/717 >> 1900.0 ms/b | lr:  5.4e-05 | grad norm: 1.79 | max abs grad:   0.052 | loss: 5.75 | perp.: 314.07\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1385.30 sec | train_loss:  5.81 | train_perp: 334.69 | valid_loss:  5.64 | valid_perp.: 280.62\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   3/100) lr = 8.385e-05 (warmup)\n",
      " b 150/722 >> 1928.5 ms/b | lr:  7.9e-05 | grad norm: 2.42 | max abs grad:   0.054 | loss: 5.70 | perp.: 297.88\n",
      " b 300/722 >> 1931.9 ms/b | lr:  8.8e-05 | grad norm: 2.59 | max abs grad:   0.040 | loss: 5.59 | perp.: 268.77\n",
      " b 450/722 >> 1882.8 ms/b | lr:  8.1e-05 | grad norm: 2.34 | max abs grad:   0.037 | loss: 5.53 | perp.: 252.92\n",
      " b 600/722 >> 1912.3 ms/b | lr:  8.6e-05 | grad norm: 2.47 | max abs grad:   0.035 | loss: 5.50 | perp.: 244.91\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1378.65 sec | train_loss:  5.56 | train_perp: 260.82 | valid_loss:  5.39 | valid_perp.: 218.69\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   4/100) lr = 0.0001118 (warmup)\n",
      " b 150/718 >> 1933.3 ms/b | lr:  0.00011 | grad norm: 2.27 | max abs grad:   0.059 | loss: 5.43 | perp.: 229.11\n",
      " b 300/718 >> 1907.0 ms/b | lr:  0.00012 | grad norm: 2.92 | max abs grad:   0.072 | loss: 5.34 | perp.: 208.62\n",
      " b 450/718 >> 1900.0 ms/b | lr:  0.00011 | grad norm: 2.38 | max abs grad:   0.063 | loss: 5.31 | perp.: 202.97\n",
      " b 600/718 >> 1950.4 ms/b | lr:  0.00011 | grad norm: 2.39 | max abs grad:   0.046 | loss: 5.27 | perp.: 194.28\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1376.69 sec | train_loss:  5.32 | train_perp: 205.17 | valid_loss:  5.17 | valid_perp.: 176.58\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   5/100) lr = 0.0001398 (warmup)\n",
      " b 150/720 >> 1968.7 ms/b | lr:  0.00015 | grad norm: 2.32 | max abs grad:   0.055 | loss: 5.20 | perp.: 180.78\n",
      " b 300/720 >> 1872.6 ms/b | lr:  0.00013 | grad norm: 3.23 | max abs grad:   0.098 | loss: 5.11 | perp.: 165.09\n",
      " b 450/720 >> 1954.9 ms/b | lr:  0.00013 | grad norm: 3.25 | max abs grad:   0.078 | loss: 5.09 | perp.: 161.98\n",
      " b 600/720 >> 1893.3 ms/b | lr:  0.00014 | grad norm: 2.86 | max abs grad:   0.074 | loss: 5.01 | perp.: 149.60\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1383.91 sec | train_loss:  5.08 | train_perp: 161.49 | valid_loss:  4.94 | valid_perp.: 139.22\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   6/100) lr = 0.0001677 (warmup)\n",
      " b 150/724 >> 1921.5 ms/b | lr:  0.00016 | grad norm: 3.62 | max abs grad:   0.175 | loss: 4.93 | perp.: 138.24\n",
      " b 300/724 >> 1936.2 ms/b | lr:  0.00019 | grad norm: 3.29 | max abs grad:   0.237 | loss: 4.84 | perp.: 126.66\n",
      " b 450/724 >> 1880.2 ms/b | lr:  0.00018 | grad norm: 4.24 | max abs grad:   0.304 | loss: 4.79 | perp.: 120.20\n",
      " b 600/724 >> 1959.6 ms/b | lr:  0.00016 | grad norm: 4.42 | max abs grad:   0.397 | loss: 4.76 | perp.: 117.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1384.61 sec | train_loss:  4.82 | train_perp: 123.48 | valid_loss:  4.70 | valid_perp.: 110.20\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   7/100) lr = 0.0001957 (warmup)\n",
      " b 150/713 >> 1984.4 ms/b | lr:   0.0002 | grad norm: 3.81 | max abs grad:   0.233 | loss: 4.66 | perp.: 105.63\n",
      " b 300/713 >> 1910.5 ms/b | lr:  0.00018 | grad norm: 4.00 | max abs grad:   0.278 | loss: 4.58 | perp.:  97.86\n",
      " b 450/713 >> 1933.4 ms/b | lr:  0.00018 | grad norm: 5.01 | max abs grad:   0.399 | loss: 4.55 | perp.:  94.61\n",
      " b 600/713 >> 1976.9 ms/b | lr:  0.00021 | grad norm: 4.65 | max abs grad:   0.367 | loss: 4.53 | perp.:  92.59\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1386.90 sec | train_loss:  4.58 | train_perp:  97.60 | valid_loss:  4.51 | valid_perp.:  91.25\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   8/100) lr = 0.0002236 (warmup)\n",
      " b 150/718 >> 1902.9 ms/b | lr:  0.00024 | grad norm: 4.36 | max abs grad:   0.357 | loss: 4.42 | perp.:  82.72\n",
      " b 300/718 >> 1934.7 ms/b | lr:  0.00024 | grad norm: 5.27 | max abs grad:   0.432 | loss: 4.37 | perp.:  79.22\n",
      " b 450/718 >> 1958.3 ms/b | lr:  0.00021 | grad norm: 4.16 | max abs grad:   0.299 | loss: 4.36 | perp.:  78.10\n",
      " b 600/718 >> 1919.7 ms/b | lr:  0.00022 | grad norm: 5.78 | max abs grad:   0.409 | loss: 4.32 | perp.:  75.35\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1384.10 sec | train_loss:  4.37 | train_perp:  79.05 | valid_loss:  4.36 | valid_perp.:  78.01\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   9/100) lr = 0.0002516 (warmup)\n",
      " b 150/722 >> 1954.1 ms/b | lr:  0.00024 | grad norm: 5.12 | max abs grad:   0.276 | loss: 4.25 | perp.:  70.10\n",
      " b 300/722 >> 1963.0 ms/b | lr:  0.00024 | grad norm: 5.34 | max abs grad:   0.386 | loss: 4.20 | perp.:  66.91\n",
      " b 450/722 >> 1874.4 ms/b | lr:  0.00026 | grad norm: 5.78 | max abs grad:   0.486 | loss: 4.16 | perp.:  64.22\n",
      " b 600/722 >> 1921.8 ms/b | lr:  0.00027 | grad norm: 9.58 | max abs grad:   1.053 | loss: 4.15 | perp.:  63.27\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1391.33 sec | train_loss:  4.20 | train_perp:  66.95 | valid_loss:  4.19 | valid_perp.:  66.31\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  10/100) lr = 0.0002795 (warmup)\n",
      " b 150/716 >> 2007.4 ms/b | lr:   0.0002 | grad norm: 6.71 | max abs grad:   0.279 | loss: 4.11 | perp.:  60.88\n",
      " b 300/716 >> 1944.4 ms/b | lr:  0.00027 | grad norm: 8.15 | max abs grad:   0.964 | loss: 4.05 | perp.:  57.54\n",
      " b 450/716 >> 1958.3 ms/b | lr:  0.00028 | grad norm: 9.86 | max abs grad:   1.424 | loss: 4.04 | perp.:  56.67\n",
      " b 600/716 >> 2036.6 ms/b | lr:  0.00026 | grad norm: 6.79 | max abs grad:   0.933 | loss: 4.07 | perp.:  58.76\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1424.52 sec | train_loss:  4.09 | train_perp:  59.65 | valid_loss:  4.08 | valid_perp.:  58.86\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  11/100) lr = 0.0002665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/713 >> 2057.6 ms/b | lr:  0.00025 | grad norm: 5.80 | max abs grad:   0.572 | loss: 3.99 | perp.:  54.31\n",
      " b 300/713 >> 2058.9 ms/b | lr:  0.00027 | grad norm: 11.21 | max abs grad:   1.808 | loss: 3.94 | perp.:  51.57\n",
      " b 450/713 >> 2057.5 ms/b | lr:  0.00027 | grad norm: 11.77 | max abs grad:   2.113 | loss: 3.97 | perp.:  53.01\n",
      " b 600/713 >> 2117.0 ms/b | lr:  0.00025 | grad norm: 5.87 | max abs grad:   0.735 | loss: 3.94 | perp.:  51.36\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1487.12 sec | train_loss:  3.99 | train_perp:  54.05 | valid_loss:  4.01 | valid_perp.:  54.97\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  12/100) lr = 0.0002552\n",
      " b 150/720 >> 2137.5 ms/b | lr:  0.00026 | grad norm: 13.24 | max abs grad:   3.143 | loss: 3.88 | perp.:  48.63\n",
      " b 300/720 >> 2135.3 ms/b | lr:  0.00028 | grad norm: 41.12 | max abs grad:   8.318 | loss: 3.88 | perp.:  48.21\n",
      " b 450/720 >> 2156.7 ms/b | lr:  0.00022 | grad norm: 7.37 | max abs grad:   1.085 | loss: 3.87 | perp.:  47.76\n",
      " b 600/720 >> 2116.0 ms/b | lr:  0.00023 | grad norm: 6.87 | max abs grad:   0.878 | loss: 3.84 | perp.:  46.61\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1545.72 sec | train_loss:  3.90 | train_perp:  49.31 | valid_loss:  3.97 | valid_perp.:  52.96\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  13/100) lr = 0.0002451\n",
      " b 150/721 >> 2149.3 ms/b | lr:  0.00024 | grad norm: 22.41 | max abs grad:   3.871 | loss: 3.85 | perp.:  47.13\n",
      " b 300/721 >> 2217.9 ms/b | lr:  0.00025 | grad norm: 75.47 | max abs grad:  16.858 | loss: 3.80 | perp.:  44.78\n",
      " b 450/721 >> 2218.7 ms/b | lr:  0.00026 | grad norm: 40.59 | max abs grad:   7.019 | loss: 3.81 | perp.:  45.02\n",
      " b 600/721 >> 2272.5 ms/b | lr:  0.00023 | grad norm: 17.88 | max abs grad:   2.761 | loss: 3.80 | perp.:  44.80\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1597.00 sec | train_loss:  3.85 | train_perp:  47.03 | valid_loss:  3.94 | valid_perp.:  51.48\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  14/100) lr = 0.0002362\n",
      " b 150/712 >> 2236.7 ms/b | lr:  0.00024 | grad norm: 34.57 | max abs grad:   6.410 | loss: 3.81 | perp.:  45.25\n",
      " b 300/712 >> 2300.6 ms/b | lr:  0.00022 | grad norm: 12.50 | max abs grad:   1.738 | loss: 3.81 | perp.:  45.36\n",
      " b 450/712 >> 2246.6 ms/b | lr:  0.00024 | grad norm: 57.82 | max abs grad:   8.965 | loss: 3.80 | perp.:  44.76\n",
      " b 600/712 >> 2290.3 ms/b | lr:  0.00023 | grad norm: 45.87 | max abs grad:   7.609 | loss: 3.78 | perp.:  43.92\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1609.43 sec | train_loss:  3.84 | train_perp:  46.34 | valid_loss:  3.85 | valid_perp.:  46.94\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  15/100) lr = 0.0002282\n",
      " b 150/716 >> 2242.9 ms/b | lr:  0.00021 | grad norm: 24.02 | max abs grad:   3.743 | loss: 3.76 | perp.:  43.12\n",
      " b 300/716 >> 2239.8 ms/b | lr:  0.00023 | grad norm: 37.22 | max abs grad:   7.259 | loss: 3.75 | perp.:  42.73\n",
      " b 450/716 >> 2128.0 ms/b | lr:  0.00024 | grad norm: 56.94 | max abs grad:   9.207 | loss: 3.73 | perp.:  41.60\n",
      " b 600/716 >> 2211.1 ms/b | lr:  0.00023 | grad norm: 77.44 | max abs grad:  12.855 | loss: 3.74 | perp.:  42.21\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1574.51 sec | train_loss:  3.79 | train_perp:  44.11 | valid_loss:  3.76 | valid_perp.:  42.85\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  16/100) lr = 0.000221\n",
      " b 150/718 >> 2200.6 ms/b | lr:  0.00021 | grad norm: 19.90 | max abs grad:   3.111 | loss: 3.70 | perp.:  40.47\n",
      " b 300/718 >> 2143.5 ms/b | lr:  0.00021 | grad norm: 26.63 | max abs grad:   4.065 | loss: 3.64 | perp.:  38.10\n",
      " b 450/718 >> 2232.3 ms/b | lr:   0.0002 | grad norm: 16.68 | max abs grad:   2.501 | loss: 3.75 | perp.:  42.36\n",
      " b 600/718 >> 2193.8 ms/b | lr:  0.00021 | grad norm: 12.35 | max abs grad:   1.328 | loss: 3.71 | perp.:  40.89\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1578.67 sec | train_loss:  3.74 | train_perp:  42.07 | valid_loss:  3.76 | valid_perp.:  42.80\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  17/100) lr = 0.0002144\n",
      " b 150/717 >> 2203.8 ms/b | lr:   0.0002 | grad norm: 15.01 | max abs grad:   2.636 | loss: 3.67 | perp.:  39.10\n",
      " b 300/717 >> 2276.5 ms/b | lr:  0.00023 | grad norm: 51.22 | max abs grad:   9.252 | loss: 3.69 | perp.:  40.09\n",
      " b 450/717 >> 2238.1 ms/b | lr:  0.00022 | grad norm: 66.53 | max abs grad:  10.086 | loss: 3.65 | perp.:  38.55\n",
      " b 600/717 >> 2269.3 ms/b | lr:  0.00022 | grad norm: 41.68 | max abs grad:   9.401 | loss: 3.65 | perp.:  38.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1606.24 sec | train_loss:  3.69 | train_perp:  40.18 | valid_loss:  3.76 | valid_perp.:  42.84\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  18/100) lr = 0.0002083\n",
      " b 150/715 >> 2297.7 ms/b | lr:  0.00019 | grad norm: 18.28 | max abs grad:   2.663 | loss: 3.69 | perp.:  40.00\n",
      " b 300/715 >> 2272.6 ms/b | lr:  0.00023 | grad norm: 195.28 | max abs grad:  34.479 | loss: 3.63 | perp.:  37.56\n",
      " b 450/715 >> 2293.0 ms/b | lr:  0.00023 | grad norm: 103.55 | max abs grad:  16.420 | loss: 3.66 | perp.:  38.79\n",
      " b 600/715 >> 2266.9 ms/b | lr:  0.00023 | grad norm: 69.39 | max abs grad:  10.890 | loss: 3.66 | perp.:  38.77\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1623.96 sec | train_loss:  3.69 | train_perp:  40.21 | valid_loss:  3.74 | valid_perp.:  41.95\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  19/100) lr = 0.0002028\n",
      " b 150/715 >> 2302.3 ms/b | lr:  0.00016 | grad norm: 12.52 | max abs grad:   1.810 | loss: 3.61 | perp.:  36.79\n",
      " b 300/715 >> 2308.7 ms/b | lr:  0.00019 | grad norm: 19.27 | max abs grad:   2.568 | loss: 3.62 | perp.:  37.33\n",
      " b 450/715 >> 2299.7 ms/b | lr:  0.00019 | grad norm: 27.91 | max abs grad:   5.241 | loss: 3.62 | perp.:  37.31\n",
      " b 600/715 >> 2287.0 ms/b | lr:  0.00019 | grad norm: 33.21 | max abs grad:   6.482 | loss: 3.60 | perp.:  36.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1644.43 sec | train_loss:  3.65 | train_perp:  38.45 | valid_loss:  3.60 | valid_perp.:  36.64\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  20/100) lr = 0.0001976\n",
      " b 150/713 >> 2344.7 ms/b | lr:  0.00019 | grad norm: 115.07 | max abs grad:  22.910 | loss: 3.62 | perp.:  37.21\n",
      " b 300/713 >> 2319.0 ms/b | lr:  0.00021 | grad norm: 98.41 | max abs grad:  17.005 | loss: 3.61 | perp.:  37.04\n",
      " b 450/713 >> 2347.9 ms/b | lr:   0.0002 | grad norm: 73.07 | max abs grad:  11.271 | loss: 3.60 | perp.:  36.73\n",
      " b 600/713 >> 2351.2 ms/b | lr:  0.00021 | grad norm: 70.17 | max abs grad:  13.023 | loss: 3.66 | perp.:  38.96\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1667.86 sec | train_loss:  3.67 | train_perp:  39.19 | valid_loss:  3.68 | valid_perp.:  39.82\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  21/100) lr = 0.0001929\n",
      " b 150/719 >> 2388.1 ms/b | lr:  0.00021 | grad norm: 94.85 | max abs grad:  18.655 | loss: 3.66 | perp.:  38.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/719 >> 2258.7 ms/b | lr:  0.00019 | grad norm: 42.20 | max abs grad:   6.193 | loss: 3.56 | perp.:  35.11\n",
      " b 450/719 >> 2314.9 ms/b | lr:   0.0002 | grad norm: 81.95 | max abs grad:  13.802 | loss: 3.57 | perp.:  35.67\n",
      " b 600/719 >> 2350.2 ms/b | lr:  0.00016 | grad norm: 18.22 | max abs grad:   2.878 | loss: 3.61 | perp.:  36.81\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1674.67 sec | train_loss:  3.63 | train_perp:  37.72 | valid_loss:  3.63 | valid_perp.:  37.88\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  22/100) lr = 0.0001884\n",
      " b 150/716 >> 2367.1 ms/b | lr:  0.00019 | grad norm: 51.37 | max abs grad:   8.043 | loss: 3.62 | perp.:  37.16\n",
      " b 300/716 >> 2324.8 ms/b | lr:  0.00019 | grad norm: 112.91 | max abs grad:  25.686 | loss: 3.56 | perp.:  35.03\n",
      " b 450/716 >> 2319.5 ms/b | lr:   0.0002 | grad norm: 69.61 | max abs grad:  12.612 | loss: 3.58 | perp.:  35.97\n",
      " b 600/716 >> 2386.1 ms/b | lr:  0.00019 | grad norm: 75.68 | max abs grad:  13.192 | loss: 3.57 | perp.:  35.46\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1678.12 sec | train_loss:  3.61 | train_perp:  37.04 | valid_loss:  3.71 | valid_perp.:  40.78\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  23/100) lr = 0.0001843\n",
      " b 150/722 >> 2306.8 ms/b | lr:   0.0002 | grad norm: 99.01 | max abs grad:  16.087 | loss: 3.58 | perp.:  36.01\n",
      " b 300/722 >> 2342.3 ms/b | lr:  0.00019 | grad norm: 61.16 | max abs grad:  10.930 | loss: 3.56 | perp.:  35.08\n",
      " b 450/722 >> 2399.9 ms/b | lr:  0.00017 | grad norm: 22.25 | max abs grad:   2.645 | loss: 3.57 | perp.:  35.49\n",
      " b 600/722 >> 2358.0 ms/b | lr:  0.00018 | grad norm: 42.25 | max abs grad:   8.940 | loss: 3.63 | perp.:  37.82\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1691.56 sec | train_loss:  3.63 | train_perp:  37.75 | valid_loss:  3.59 | valid_perp.:  36.29\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  24/100) lr = 0.0001804\n",
      " b 150/713 >> 2373.3 ms/b | lr:  0.00017 | grad norm: 140.65 | max abs grad:  23.466 | loss: 3.60 | perp.:  36.77\n",
      " b 300/713 >> 2430.8 ms/b | lr:  0.00019 | grad norm: 102.19 | max abs grad:  18.513 | loss: 3.54 | perp.:  34.43\n",
      " b 450/713 >> 2322.5 ms/b | lr:  0.00019 | grad norm: 103.55 | max abs grad:  19.655 | loss: 3.54 | perp.:  34.40\n",
      " b 600/713 >> 2432.2 ms/b | lr:  0.00016 | grad norm: 42.86 | max abs grad:   6.434 | loss: 3.58 | perp.:  35.76\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1704.65 sec | train_loss:  3.61 | train_perp:  37.14 | valid_loss:  3.62 | valid_perp.:  37.18\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  25/100) lr = 0.0001768\n",
      " b 150/723 >> 2320.8 ms/b | lr:  0.00018 | grad norm: 55.98 | max abs grad:  11.782 | loss: 3.60 | perp.:  36.51\n",
      " b 300/723 >> 2375.7 ms/b | lr:  0.00014 | grad norm: 23.89 | max abs grad:   3.891 | loss: 3.59 | perp.:  36.11\n",
      " b 450/723 >> 2306.8 ms/b | lr:  0.00015 | grad norm: 26.21 | max abs grad:   4.500 | loss: 3.56 | perp.:  35.18\n",
      " b 600/723 >> 2394.6 ms/b | lr:  0.00018 | grad norm: 147.24 | max abs grad:  33.009 | loss: 3.65 | perp.:  38.44\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1703.45 sec | train_loss:  3.65 | train_perp:  38.52 | valid_loss:  3.56 | valid_perp.:  35.14\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  26/100) lr = 0.0001733\n",
      " b 150/711 >> 2460.3 ms/b | lr:  0.00016 | grad norm: 22.43 | max abs grad:   2.499 | loss: 3.64 | perp.:  38.01\n",
      " b 300/711 >> 2394.9 ms/b | lr:  0.00015 | grad norm: 29.40 | max abs grad:   5.124 | loss: 3.59 | perp.:  36.29\n",
      " b 450/711 >> 2415.8 ms/b | lr:  0.00017 | grad norm: 115.23 | max abs grad:  19.702 | loss: 3.63 | perp.:  37.60\n",
      " b 600/711 >> 2392.1 ms/b | lr:  0.00017 | grad norm: 76.56 | max abs grad:  12.457 | loss: 3.58 | perp.:  35.89\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1719.00 sec | train_loss:  3.65 | train_perp:  38.48 | valid_loss:  3.62 | valid_perp.:  37.21\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  27/100) lr = 0.0001701\n",
      " b 150/716 >> 2380.7 ms/b | lr:  0.00015 | grad norm: 26.08 | max abs grad:   5.451 | loss: 3.59 | perp.:  36.40\n",
      " b 300/716 >> 2452.7 ms/b | lr:  0.00017 | grad norm: 31.26 | max abs grad:   5.711 | loss: 3.60 | perp.:  36.68\n",
      " b 450/716 >> 2394.6 ms/b | lr:  0.00016 | grad norm: 147.53 | max abs grad:  28.154 | loss: 3.58 | perp.:  35.98\n",
      " b 600/716 >> 2452.8 ms/b | lr:  0.00018 | grad norm: 251.70 | max abs grad:  44.835 | loss: 3.59 | perp.:  36.31\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1734.43 sec | train_loss:  3.63 | train_perp:  37.54 | valid_loss:  3.56 | valid_perp.:  35.24\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  28/100) lr = 0.000167\n",
      " b 150/717 >> 2426.6 ms/b | lr:  0.00018 | grad norm: 125.81 | max abs grad:  23.637 | loss: 3.62 | perp.:  37.19\n",
      " b 300/717 >> 2448.1 ms/b | lr:  0.00017 | grad norm: 64.61 | max abs grad:  11.728 | loss: 3.60 | perp.:  36.72\n",
      " b 450/717 >> 2398.1 ms/b | lr:  0.00014 | grad norm: 322.53 | max abs grad:  45.171 | loss: 3.60 | perp.:  36.66\n",
      " b 600/717 >> 2468.7 ms/b | lr:  0.00017 | grad norm: 127.54 | max abs grad:  23.361 | loss: 3.65 | perp.:  38.41\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1746.66 sec | train_loss:  3.66 | train_perp:  38.84 | valid_loss:  3.56 | valid_perp.:  35.13\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  29/100) lr = 0.0001641\n",
      " b 150/714 >> 2434.8 ms/b | lr:  0.00019 | grad norm: 34.86 | max abs grad:   4.422 | loss: 3.60 | perp.:  36.66\n",
      " b 300/714 >> 2429.9 ms/b | lr:  0.00016 | grad norm: 420.37 | max abs grad:  63.395 | loss: 3.63 | perp.:  37.62\n",
      " b 450/714 >> 2374.9 ms/b | lr:  0.00018 | grad norm: 136.44 | max abs grad:  22.400 | loss: 3.58 | perp.:  35.95\n",
      " b 600/714 >> 2489.1 ms/b | lr:  0.00017 | grad norm: 55.08 | max abs grad:   9.287 | loss: 3.64 | perp.:  38.12\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1740.73 sec | train_loss:  3.65 | train_perp:  38.60 | valid_loss:  3.71 | valid_perp.:  40.78\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  30/100) lr = 0.0001614\n",
      " b 150/711 >> 2434.7 ms/b | lr:  0.00015 | grad norm: 33.44 | max abs grad:   4.515 | loss: 3.62 | perp.:  37.20\n",
      " b 300/711 >> 2459.8 ms/b | lr:  0.00014 | grad norm: 128.29 | max abs grad:  22.481 | loss: 3.59 | perp.:  36.36\n",
      " b 450/711 >> 2396.2 ms/b | lr:  0.00017 | grad norm: 109.33 | max abs grad:  21.446 | loss: 3.55 | perp.:  34.81\n",
      " b 600/711 >> 2501.4 ms/b | lr:  0.00015 | grad norm: 79.90 | max abs grad:   9.441 | loss: 3.65 | perp.:  38.64\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1747.75 sec | train_loss:  3.65 | train_perp:  38.31 | valid_loss:  3.65 | valid_perp.:  38.59\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  31/100) lr = 0.0001588\n",
      " b 150/723 >> 2394.4 ms/b | lr:  0.00015 | grad norm: 79.17 | max abs grad:  12.743 | loss: 3.57 | perp.:  35.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/723 >> 2513.1 ms/b | lr:  0.00017 | grad norm: 254.81 | max abs grad:  59.044 | loss: 3.60 | perp.:  36.54\n",
      " b 450/723 >> 2350.8 ms/b | lr:  0.00017 | grad norm: 112.52 | max abs grad:  17.972 | loss: 3.50 | perp.:  33.07\n",
      " b 600/723 >> 2450.1 ms/b | lr:  0.00015 | grad norm: 50.13 | max abs grad:  10.018 | loss: 3.59 | perp.:  36.18\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1754.88 sec | train_loss:  3.61 | train_perp:  36.81 | valid_loss:  3.55 | valid_perp.:  34.69\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  32/100) lr = 0.0001563\n",
      " b 150/728 >> 2406.6 ms/b | lr:  0.00015 | grad norm: 92.34 | max abs grad:  16.867 | loss: 3.52 | perp.:  33.87\n",
      " b 300/728 >> 2445.9 ms/b | lr:  0.00014 | grad norm: 45.72 | max abs grad:   8.069 | loss: 3.52 | perp.:  33.67\n",
      " b 450/728 >> 2398.0 ms/b | lr:  0.00016 | grad norm: 131.21 | max abs grad:  24.883 | loss: 3.65 | perp.:  38.47\n",
      " b 600/728 >> 2490.8 ms/b | lr:  0.00014 | grad norm: 102.17 | max abs grad:  16.569 | loss: 3.58 | perp.:  35.86\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1770.03 sec | train_loss:  3.62 | train_perp:  37.36 | valid_loss:  3.60 | valid_perp.:  36.53\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  33/100) lr = 0.0001539\n",
      " b 150/707 >> 2506.3 ms/b | lr:  0.00017 | grad norm: 327.38 | max abs grad:  55.711 | loss: 3.59 | perp.:  36.36\n",
      " b 300/707 >> 2459.8 ms/b | lr:  0.00015 | grad norm: 58.72 | max abs grad:   9.370 | loss: 3.56 | perp.:  34.99\n",
      " b 450/707 >> 2514.0 ms/b | lr:  0.00017 | grad norm: 70.59 | max abs grad:  12.456 | loss: 3.55 | perp.:  34.78\n",
      " b 600/707 >> 2528.2 ms/b | lr:  0.00017 | grad norm: 296.78 | max abs grad:  53.396 | loss: 3.69 | perp.:  40.20\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1767.54 sec | train_loss:  3.64 | train_perp:  37.98 | valid_loss:  3.57 | valid_perp.:  35.61\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  34/100) lr = 0.0001516\n",
      " b 150/714 >> 2463.7 ms/b | lr:  0.00016 | grad norm: 40.54 | max abs grad:   5.589 | loss: 3.61 | perp.:  36.81\n",
      " b 300/714 >> 2491.4 ms/b | lr:  0.00015 | grad norm: 134.15 | max abs grad:  23.416 | loss: 3.60 | perp.:  36.78\n",
      " b 450/714 >> 2460.1 ms/b | lr:  0.00015 | grad norm: 79.15 | max abs grad:  12.528 | loss: 3.59 | perp.:  36.28\n",
      " b 600/714 >> 2493.0 ms/b | lr:  0.00012 | grad norm: 29.18 | max abs grad:   5.166 | loss: 3.58 | perp.:  35.72\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1764.99 sec | train_loss:  3.64 | train_perp:  38.16 | valid_loss:  3.69 | valid_perp.:  40.17\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  35/100) lr = 0.0001494\n",
      " b 150/720 >> 2427.4 ms/b | lr:  0.00015 | grad norm: 146.71 | max abs grad:  27.284 | loss: 3.60 | perp.:  36.62\n",
      " b 300/720 >> 2469.0 ms/b | lr:  0.00016 | grad norm: 54.58 | max abs grad:   8.201 | loss: 3.69 | perp.:  40.24\n",
      " b 450/720 >> 2401.4 ms/b | lr:  0.00015 | grad norm: 71.01 | max abs grad:  11.993 | loss: 3.54 | perp.:  34.43\n",
      " b 600/720 >> 2455.9 ms/b | lr:  0.00014 | grad norm: 76.26 | max abs grad:  15.520 | loss: 3.55 | perp.:  34.73\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1763.26 sec | train_loss:  3.64 | train_perp:  38.08 | valid_loss:  3.61 | valid_perp.:  37.07\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  36/100) lr = 0.0001473\n",
      " b 150/715 >> 2462.8 ms/b | lr:  0.00015 | grad norm: 71.14 | max abs grad:  10.282 | loss: 3.61 | perp.:  36.86\n",
      " b 300/715 >> 2465.9 ms/b | lr:  0.00015 | grad norm: 66.24 | max abs grad:  12.234 | loss: 3.57 | perp.:  35.67\n",
      " b 450/715 >> 2465.3 ms/b | lr:  0.00016 | grad norm: 106.62 | max abs grad:  16.848 | loss: 3.57 | perp.:  35.49\n",
      " b 600/715 >> 2516.2 ms/b | lr:  0.00014 | grad norm: 64.61 | max abs grad:  11.707 | loss: 3.61 | perp.:  37.06\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1768.73 sec | train_loss:  3.63 | train_perp:  37.70 | valid_loss:  3.62 | valid_perp.:  37.26\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  37/100) lr = 0.0001453\n",
      " b 150/719 >> 2452.5 ms/b | lr:  0.00014 | grad norm: 510.70 | max abs grad: 103.234 | loss: 3.58 | perp.:  35.84\n",
      " b 300/719 >> 2497.7 ms/b | lr:  0.00016 | grad norm: 162.33 | max abs grad:  26.293 | loss: 3.59 | perp.:  36.24\n",
      " b 450/719 >> 2402.6 ms/b | lr:  0.00015 | grad norm: 135.03 | max abs grad:  19.172 | loss: 3.55 | perp.:  34.68\n",
      " b 600/719 >> 2465.9 ms/b | lr:  0.00014 | grad norm: 42.97 | max abs grad:   6.921 | loss: 3.51 | perp.:  33.57\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1764.19 sec | train_loss:  3.60 | train_perp:  36.61 | valid_loss:  3.57 | valid_perp.:  35.56\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  38/100) lr = 0.0001434\n",
      " b 150/720 >> 2477.4 ms/b | lr:  0.00014 | grad norm: 124.12 | max abs grad:  23.930 | loss: 3.58 | perp.:  35.94\n",
      " b 300/720 >> 2473.2 ms/b | lr:  0.00014 | grad norm: 140.27 | max abs grad:  25.108 | loss: 3.52 | perp.:  33.62\n",
      " b 450/720 >> 2409.2 ms/b | lr:  0.00014 | grad norm: 74.03 | max abs grad:  13.829 | loss: 3.59 | perp.:  36.24\n",
      " b 600/720 >> 2471.1 ms/b | lr:  0.00015 | grad norm: 64.90 | max abs grad:  10.553 | loss: 3.62 | perp.:  37.38\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1771.19 sec | train_loss:  3.62 | train_perp:  37.18 | valid_loss:  3.59 | valid_perp.:  36.23\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  39/100) lr = 0.0001415\n",
      " b 150/715 >> 2434.2 ms/b | lr:  0.00015 | grad norm: 65.12 | max abs grad:  11.343 | loss: 3.60 | perp.:  36.49\n",
      " b 300/715 >> 2492.0 ms/b | lr:  0.00013 | grad norm: 78.66 | max abs grad:   9.415 | loss: 3.59 | perp.:  36.22\n",
      " b 450/715 >> 2504.4 ms/b | lr:  0.00014 | grad norm: 94.34 | max abs grad:  15.105 | loss: 3.55 | perp.:  34.91\n",
      " b 600/715 >> 2521.7 ms/b | lr:  0.00015 | grad norm: 360.30 | max abs grad:  58.093 | loss: 3.56 | perp.:  35.05\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1771.33 sec | train_loss:  3.61 | train_perp:  36.89 | valid_loss:  3.74 | valid_perp.:  42.19\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  40/100) lr = 0.0001398\n",
      " b 150/721 >> 2475.1 ms/b | lr:  0.00013 | grad norm: 31.83 | max abs grad:   4.794 | loss: 3.54 | perp.:  34.46\n",
      " b 300/721 >> 2439.6 ms/b | lr:  0.00013 | grad norm: 44.53 | max abs grad:   5.677 | loss: 3.54 | perp.:  34.50\n",
      " b 450/721 >> 2439.8 ms/b | lr:  0.00012 | grad norm: 61.19 | max abs grad:   9.363 | loss: 3.57 | perp.:  35.39\n",
      " b 600/721 >> 2465.2 ms/b | lr:  0.00015 | grad norm: 123.39 | max abs grad:  23.457 | loss: 3.51 | perp.:  33.61\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1770.73 sec | train_loss:  3.58 | train_perp:  36.00 | valid_loss:  3.54 | valid_perp.:  34.64\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  41/100) lr = 0.000138\n",
      " b 150/709 >> 2589.1 ms/b | lr:  0.00015 | grad norm: 170.27 | max abs grad:  29.397 | loss: 3.62 | perp.:  37.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/709 >> 2498.0 ms/b | lr:  0.00014 | grad norm: 104.88 | max abs grad:  18.017 | loss: 3.54 | perp.:  34.43\n",
      " b 450/709 >> 2591.5 ms/b | lr:  0.00014 | grad norm: 102.92 | max abs grad:  15.555 | loss: 3.62 | perp.:  37.32\n",
      " b 600/709 >> 3050.6 ms/b | lr:  0.00013 | grad norm: 201.03 | max abs grad:  40.357 | loss: 3.54 | perp.:  34.52\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1963.54 sec | train_loss:  3.61 | train_perp:  36.96 | valid_loss:  3.51 | valid_perp.:  33.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  42/100) lr = 0.0001364\n",
      " b 150/718 >> 3138.9 ms/b | lr:  0.00012 | grad norm: 75.61 | max abs grad:  14.083 | loss: 3.56 | perp.:  35.28\n",
      " b 300/718 >> 3075.2 ms/b | lr:  0.00011 | grad norm: 166.74 | max abs grad:  35.100 | loss: 3.54 | perp.:  34.48\n",
      " b 450/718 >> 3180.6 ms/b | lr:  0.00014 | grad norm: 72.68 | max abs grad:  13.805 | loss: 3.55 | perp.:  34.97\n",
      " b 600/718 >> 3069.5 ms/b | lr:  0.00015 | grad norm: 83.54 | max abs grad:  12.728 | loss: 3.50 | perp.:  32.96\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2229.97 sec | train_loss:  3.58 | train_perp:  36.01 | valid_loss:  3.52 | valid_perp.:  33.63\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  43/100) lr = 0.0001348\n",
      " b 150/714 >> 3049.6 ms/b | lr:  0.00013 | grad norm: 92.80 | max abs grad:  15.609 | loss: 3.55 | perp.:  34.70\n",
      " b 300/714 >> 3086.4 ms/b | lr:  0.00013 | grad norm: 144.45 | max abs grad:  20.078 | loss: 3.52 | perp.:  33.65\n",
      " b 450/714 >> 3113.0 ms/b | lr:  0.00015 | grad norm: 88.17 | max abs grad:  15.276 | loss: 3.53 | perp.:  34.01\n",
      " b 600/714 >> 3267.7 ms/b | lr:  0.00012 | grad norm: 43.39 | max abs grad:   8.267 | loss: 3.54 | perp.:  34.38\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2243.80 sec | train_loss:  3.57 | train_perp:  35.65 | valid_loss:  3.58 | valid_perp.:  35.76\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  44/100) lr = 0.0001333\n",
      " b 150/726 >> 3110.7 ms/b | lr:  0.00014 | grad norm: 327.42 | max abs grad:  58.124 | loss: 3.57 | perp.:  35.67\n",
      " b 300/726 >> 3056.3 ms/b | lr:  0.00013 | grad norm: 177.03 | max abs grad:  23.212 | loss: 3.55 | perp.:  34.97\n",
      " b 450/726 >> 3056.5 ms/b | lr:  0.00012 | grad norm: 110.18 | max abs grad:  17.836 | loss: 3.52 | perp.:  33.78\n",
      " b 600/726 >> 3026.1 ms/b | lr:  0.00015 | grad norm: 201.76 | max abs grad:  37.397 | loss: 3.50 | perp.:  33.00\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2225.13 sec | train_loss:  3.57 | train_perp:  35.47 | valid_loss:  3.54 | valid_perp.:  34.54\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  45/100) lr = 0.0001318\n",
      " b 150/724 >> 3055.1 ms/b | lr:  0.00012 | grad norm: 157.22 | max abs grad:  22.152 | loss: 3.50 | perp.:  33.03\n",
      " b 300/724 >> 3073.3 ms/b | lr:  0.00013 | grad norm: 132.95 | max abs grad:  19.686 | loss: 3.58 | perp.:  35.74\n",
      " b 450/724 >> 2982.2 ms/b | lr:  0.00014 | grad norm: 139.09 | max abs grad:  21.163 | loss: 3.44 | perp.:  31.30\n",
      " b 600/724 >> 3105.7 ms/b | lr:  0.00013 | grad norm: 198.25 | max abs grad:  31.344 | loss: 3.59 | perp.:  36.09\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2229.39 sec | train_loss:  3.57 | train_perp:  35.67 | valid_loss:  3.57 | valid_perp.:  35.43\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  46/100) lr = 0.0001303\n",
      " b 150/723 >> 3178.4 ms/b | lr:  9.7e-05 | grad norm: 33.64 | max abs grad:   9.609 | loss: 3.55 | perp.:  34.73\n",
      " b 300/723 >> 3304.8 ms/b | lr:  0.00013 | grad norm: 153.59 | max abs grad:  28.361 | loss: 3.58 | perp.:  35.91\n",
      " b 450/723 >> 3124.9 ms/b | lr:  0.00012 | grad norm: 43.99 | max abs grad:   7.530 | loss: 3.52 | perp.:  33.76\n",
      " b 600/723 >> 3052.2 ms/b | lr:  0.00012 | grad norm: 41.38 | max abs grad:   6.922 | loss: 3.52 | perp.:  33.65\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2276.56 sec | train_loss:  3.57 | train_perp:  35.61 | valid_loss:  3.53 | valid_perp.:  34.01\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  47/100) lr = 0.0001289\n",
      " b 150/726 >> 3086.9 ms/b | lr:  0.00012 | grad norm: 61.67 | max abs grad:   8.510 | loss: 3.52 | perp.:  33.76\n",
      " b 300/726 >> 3271.4 ms/b | lr:  0.00011 | grad norm: 163.28 | max abs grad:  25.158 | loss: 3.50 | perp.:  33.02\n",
      " b 450/726 >> 3351.2 ms/b | lr:  0.00011 | grad norm: 34.47 | max abs grad:   6.731 | loss: 3.54 | perp.:  34.47\n",
      " b 600/726 >> 3276.7 ms/b | lr:  0.00013 | grad norm: 149.34 | max abs grad:  22.632 | loss: 3.49 | perp.:  32.76\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2347.48 sec | train_loss:  3.55 | train_perp:  34.91 | valid_loss:  3.46 | valid_perp.:  31.69\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  48/100) lr = 0.0001276\n",
      " b 150/726 >> 3092.2 ms/b | lr:  0.00013 | grad norm: 154.74 | max abs grad:  28.234 | loss: 3.53 | perp.:  34.08\n",
      " b 300/726 >> 3037.1 ms/b | lr:  0.00013 | grad norm: 152.54 | max abs grad:  30.026 | loss: 3.46 | perp.:  31.66\n",
      " b 450/726 >> 3079.0 ms/b | lr:  0.00013 | grad norm: 545.49 | max abs grad:  92.320 | loss: 3.52 | perp.:  33.65\n",
      " b 600/726 >> 3181.9 ms/b | lr:  0.00012 | grad norm: 37.61 | max abs grad:   5.214 | loss: 3.53 | perp.:  34.14\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2250.11 sec | train_loss:  3.55 | train_perp:  34.74 | valid_loss:  3.55 | valid_perp.:  34.75\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  49/100) lr = 0.0001263\n",
      " b 150/714 >> 3118.8 ms/b | lr:  0.00012 | grad norm: 56.14 | max abs grad:   9.720 | loss: 3.47 | perp.:  32.29\n",
      " b 300/714 >> 3097.2 ms/b | lr:  0.00012 | grad norm: 253.55 | max abs grad:  40.900 | loss: 3.56 | perp.:  35.23\n",
      " b 450/714 >> 3133.7 ms/b | lr:  0.00012 | grad norm: 755.88 | max abs grad: 130.316 | loss: 3.49 | perp.:  32.62\n",
      " b 600/714 >> 3349.9 ms/b | lr:  0.00013 | grad norm: 178.60 | max abs grad:  30.472 | loss: 3.53 | perp.:  34.06\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2297.20 sec | train_loss:  3.57 | train_perp:  35.43 | valid_loss:  3.67 | valid_perp.:  39.10\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  50/100) lr = 0.000125\n",
      " b 150/717 >> 3538.2 ms/b | lr:  0.00014 | grad norm: 122.93 | max abs grad:  23.838 | loss: 3.59 | perp.:  36.22\n",
      " b 300/717 >> 3331.0 ms/b | lr:  0.00013 | grad norm: 119.29 | max abs grad:  24.110 | loss: 3.55 | perp.:  34.85\n",
      " b 450/717 >> 3322.8 ms/b | lr:  0.00012 | grad norm: 59.74 | max abs grad:   6.123 | loss: 3.43 | perp.:  30.83\n",
      " b 600/717 >> 3260.6 ms/b | lr:  0.00011 | grad norm: 43.56 | max abs grad:   6.664 | loss: 3.51 | perp.:  33.45\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2393.21 sec | train_loss:  3.55 | train_perp:  34.79 | valid_loss:  3.50 | valid_perp.:  33.01\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  51/100) lr = 0.0001238\n",
      " b 150/717 >> 3232.2 ms/b | lr:  0.00012 | grad norm: 55.50 | max abs grad:   7.357 | loss: 3.51 | perp.:  33.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/717 >> 3149.9 ms/b | lr:  0.00011 | grad norm: 48.63 | max abs grad:   8.894 | loss: 3.49 | perp.:  32.88\n",
      " b 450/717 >> 3183.6 ms/b | lr:  0.00013 | grad norm: 534.05 | max abs grad: 115.757 | loss: 3.47 | perp.:  32.17\n",
      " b 600/717 >> 3209.8 ms/b | lr:  0.00012 | grad norm: 2248.28 | max abs grad: 473.084 | loss: 3.52 | perp.:  33.68\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2294.54 sec | train_loss:  3.55 | train_perp:  34.66 | valid_loss:  3.55 | valid_perp.:  34.85\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  52/100) lr = 0.0001226\n",
      " b 150/727 >> 3177.4 ms/b | lr:  0.00011 | grad norm: 62.15 | max abs grad:  10.124 | loss: 3.52 | perp.:  33.71\n",
      " b 300/727 >> 3211.7 ms/b | lr:  0.00013 | grad norm: 112.51 | max abs grad:  20.057 | loss: 3.51 | perp.:  33.54\n",
      " b 450/727 >> 3196.4 ms/b | lr:  0.00013 | grad norm: 117.34 | max abs grad:  16.610 | loss: 3.51 | perp.:  33.46\n",
      " b 600/727 >> 3030.3 ms/b | lr:  0.00014 | grad norm: 104.40 | max abs grad:  16.001 | loss: 3.45 | perp.:  31.58\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2286.87 sec | train_loss:  3.55 | train_perp:  34.96 | valid_loss:  3.53 | valid_perp.:  33.99\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  53/100) lr = 0.0001214\n",
      " b 150/717 >> 3200.2 ms/b | lr:  0.00011 | grad norm: 121.46 | max abs grad:  18.610 | loss: 3.54 | perp.:  34.38\n",
      " b 300/717 >> 3179.3 ms/b | lr:  0.00012 | grad norm: 59.51 | max abs grad:   8.143 | loss: 3.49 | perp.:  32.83\n",
      " b 450/717 >> 3185.6 ms/b | lr:  0.00013 | grad norm: 133.65 | max abs grad:  17.111 | loss: 3.53 | perp.:  34.04\n",
      " b 600/717 >> 3221.6 ms/b | lr:  0.00011 | grad norm: 285.90 | max abs grad:  41.588 | loss: 3.52 | perp.:  33.72\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2287.92 sec | train_loss:  3.56 | train_perp:  35.08 | valid_loss:  3.54 | valid_perp.:  34.55\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  54/100) lr = 0.0001203\n",
      " b 150/724 >> 3264.6 ms/b | lr:  0.00014 | grad norm: 48.90 | max abs grad:   7.348 | loss: 3.63 | perp.:  37.86\n",
      " b 300/724 >> 3096.8 ms/b | lr:  0.00013 | grad norm: 375.47 | max abs grad:  70.269 | loss: 3.51 | perp.:  33.44\n",
      " b 450/724 >> 3089.8 ms/b | lr:  0.00011 | grad norm: 1160.23 | max abs grad: 124.361 | loss: 3.43 | perp.:  30.74\n",
      " b 600/724 >> 3174.8 ms/b | lr:  0.00013 | grad norm: 120.87 | max abs grad:  21.381 | loss: 3.49 | perp.:  32.95\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2276.55 sec | train_loss:  3.55 | train_perp:  34.87 | valid_loss:  3.64 | valid_perp.:  38.24\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  55/100) lr = 0.0001192\n",
      " b 150/719 >> 3251.6 ms/b | lr:  0.00012 | grad norm: 246.47 | max abs grad:  39.043 | loss: 3.57 | perp.:  35.65\n",
      " b 300/719 >> 3164.3 ms/b | lr:  0.00011 | grad norm: 56.99 | max abs grad:   6.300 | loss: 3.49 | perp.:  32.93\n",
      " b 450/719 >> 3221.0 ms/b | lr:  0.00012 | grad norm: 129.03 | max abs grad:  21.834 | loss: 3.51 | perp.:  33.31\n",
      " b 600/719 >> 3102.6 ms/b | lr:  0.00013 | grad norm: 191.56 | max abs grad:  36.343 | loss: 3.45 | perp.:  31.64\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2284.78 sec | train_loss:  3.56 | train_perp:  35.05 | valid_loss:  3.51 | valid_perp.:  33.58\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  56/100) lr = 0.0001181\n",
      " b 150/721 >> 3166.1 ms/b | lr:  0.00012 | grad norm: 319.07 | max abs grad:  40.552 | loss: 3.52 | perp.:  33.95\n",
      " b 300/721 >> 3164.3 ms/b | lr:  0.00012 | grad norm: 76.00 | max abs grad:  10.163 | loss: 3.46 | perp.:  31.89\n",
      " b 450/721 >> 3384.1 ms/b | lr:  0.00011 | grad norm: 879.29 | max abs grad: 165.660 | loss: 3.57 | perp.:  35.51\n",
      " b 600/721 >> 3249.8 ms/b | lr:   0.0001 | grad norm: 44.50 | max abs grad:   5.294 | loss: 3.49 | perp.:  32.94\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2322.97 sec | train_loss:  3.55 | train_perp:  34.65 | valid_loss:  3.44 | valid_perp.:  31.17\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  57/100) lr = 0.0001171\n",
      " b 150/716 >> 3131.1 ms/b | lr:  0.00011 | grad norm: 112.53 | max abs grad:  15.580 | loss: 3.53 | perp.:  34.08\n",
      " b 300/716 >> 3167.8 ms/b | lr:  0.00011 | grad norm: 98.07 | max abs grad:  13.753 | loss: 3.56 | perp.:  35.10\n",
      " b 450/716 >> 3065.6 ms/b | lr:   0.0001 | grad norm: 41.30 | max abs grad:   7.268 | loss: 3.46 | perp.:  31.85\n",
      " b 600/716 >> 3116.1 ms/b | lr:  0.00012 | grad norm: 132.48 | max abs grad:  25.639 | loss: 3.50 | perp.:  33.01\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train(\n",
    "        model, train_data, batch_size, seq_len, ntokens,\n",
    "        criterion, optimizer, lr_scheduler, clip, log_interval\n",
    "    )\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(\n",
    "        model, val_data, eval_batch_size, \n",
    "        seq_len, ntokens, eval_criterion,\n",
    "        save_wts = False\n",
    "    )\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    enc_states, attn_states, dec_states = states\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoder(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoder(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, save_wts = True)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 4\n",
    "model.eval(save_wts = True)\n",
    "# Get some data from a random point in the test_data set\n",
    "states = model.init_states(nb)\n",
    "data, targets = get_batch(test_data, 120, seq_len, evaluate = True)\n",
    "data = data[:,:nb].contiguous()\n",
    "targets = targets.view(seq_len, -1)[:,:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output, states = model(data, states)\n",
    "# Convert the output log probabilities to normal probabilities\n",
    "output = output.exp()\n",
    "# Get the argmax of each step in the output\n",
    "output_p, output_idx = output.max(dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted output word indices to the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.t()\n",
    "output_idx = output_idx.t()\n",
    "for i in range(nb):\n",
    "    # Print the output with the targets\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Some basic weight heat maps to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_wts = np.array(model.embedding.weight.data)\n",
    "embed_norm = (embed_wts - embed_wts.mean()) / (embed_wts.max() - embed_wts.min())\n",
    "plt.imshow(embed_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('dim'); plt.ylabel('word index');\n",
    "plt.title('Embedding layer')\n",
    "plt.colorbar()\n",
    "embed_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_wts = np.array(attn.attention.weight.data)\n",
    "attn_norm = (attn_wts - attn_wts.mean()) / (attn_wts.max() - attn_wts.min())\n",
    "plt.imshow(attn_wts, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_input+d_state'); plt.ylabel('d_output')\n",
    "plt.title('Output attention sublayer (in attention mechanism)')\n",
    "plt.colorbar()\n",
    "attn_wts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence attention visualization by mapping the alignment weights (in the attention mechanism) at each step of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = nb//cols\n",
    "fig, axs = plt.subplots(rows, cols, figsize = (30, 20))\n",
    "for b in range(nb):\n",
    "    wts = attn.attn_wts[:,b,:]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    r = b // cols\n",
    "    c = b % cols\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(wts, aspect = 'auto', cmap = 'jet')\n",
    "    # Fix labels\n",
    "    xlabels = list(targets[b].data)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_xlabel('Targets')\n",
    "    ylabels = list(data[:,b].data)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_ylabel('Inputs')\n",
    "    ax.set_title('Example %d' % b)\n",
    "    fig.colorbar(im, ax = ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
