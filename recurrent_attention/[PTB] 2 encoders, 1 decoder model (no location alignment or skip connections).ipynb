{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from model import *\n",
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_name = '2_encoder_1_decoder'\n",
    "ckpt_loc = os.path.join('checkpoints', model_test_name+'.pt')\n",
    "\n",
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 4\n",
    "lr = 0.005\n",
    "warmup_steps = 3\n",
    "decay_factor = 0.5\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 150\n",
    "early_stopping = 10\n",
    "log_interval = 150  # Print log every `log_interval` batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 512\n",
    "encode_size = 256\n",
    "h_size = 256\n",
    "align_size = 256\n",
    "decode_size = 256\n",
    "decode_out_size = 512\n",
    "n_enc_layers = 2\n",
    "attn_rnn_layers = 1\n",
    "n_dec_layers = 1\n",
    "smooth_align = True\n",
    "align_location = False\n",
    "skip_connections = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler\n",
    "\n",
    "The learning rate scheduler sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x133c429c080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133c42983c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps*2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.4g:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps*2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 9128705\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    src_vocab = ntokens, tgt_vocab = ntokens, embed_size = embed_size,\n",
    "    encode_size = encode_size, h_size = h_size, align_size = align_size,\n",
    "    decode_size = decode_size, decode_out_size = decode_out_size,\n",
    "    n_enc_layers = n_enc_layers, attn_rnn_layers = attn_rnn_layers,\n",
    "    n_dec_layers = n_dec_layers, align_location = align_location,\n",
    "    smooth_align = smooth_align, skip_connections = skip_connections,\n",
    "    dropout = dropout\n",
    ")\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "eval_criterion = LabelSmoothing(ntokens, smoothing = 0)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, decay_factor, optimizer)\n",
    "# Reference\n",
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12562, 74]), torch.Size([7376, 10]), torch.Size([8243, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/150) lr = 4.253e-05 (warmup)\n",
      " b 150/726 >> 3927.4 ms/b | lr: 4.369e-05 | grad norm:  0.96 | inf norm:  0.028 | loss: 7.34 | perp: 1540.22\n",
      " b 300/726 >> 3825.4 ms/b | lr: 4.369e-05 | grad norm:  0.59 | inf norm:  0.014 | loss: 6.20 | perp: 492.02\n",
      " b 450/726 >> 3794.2 ms/b | lr: 4.483e-05 | grad norm:  0.63 | inf norm:  0.013 | loss: 6.17 | perp: 478.16\n",
      " b 600/726 >> 3702.8 ms/b | lr: 4.253e-05 | grad norm:  0.60 | inf norm:  0.013 | loss: 6.18 | perp: 480.65\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2721.58 sec | train loss, perp: 6.421, 614.77    | valid loss, perp: 6.529, 684.86   \n",
      "Grad norm:  0.499 | Grad inf. norm:  0.0105 | Max abs param:  0.1574\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   2/150) lr = 8.505e-05 (warmup)\n",
      " b 150/722 >> 3130.8 ms/b | lr: 8.738e-05 | grad norm:  0.62 | inf norm:  0.017 | loss: 6.21 | perp: 497.66\n",
      " b 300/722 >> 2911.8 ms/b | lr: 9.187e-05 | grad norm:  0.58 | inf norm:  0.015 | loss: 6.13 | perp: 459.04\n",
      " b 450/722 >> 2847.8 ms/b | lr: 8.266e-05 | grad norm:  0.50 | inf norm:  0.015 | loss: 6.10 | perp: 445.70\n",
      " b 600/722 >> 2834.2 ms/b | lr: 8.505e-05 | grad norm:  0.58 | inf norm:  0.017 | loss: 6.08 | perp: 435.60\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2070.38 sec | train loss, perp: 6.107, 449.08    | valid loss, perp: 6.381, 590.23   \n",
      "Grad norm:  0.628 | Grad inf. norm:  0.0133 | Max abs param:  0.1753\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   3/150) lr = 0.0001276 (warmup)\n",
      " b 150/720 >> 2733.3 ms/b | lr:  0.000141 | grad norm:  0.55 | inf norm:  0.010 | loss: 6.07 | perp: 432.39\n",
      " b 300/720 >> 2672.8 ms/b | lr:  0.000124 | grad norm:  0.60 | inf norm:  0.016 | loss: 6.00 | perp: 401.98\n",
      " b 450/720 >> 2609.8 ms/b | lr: 0.0001203 | grad norm:  0.51 | inf norm:  0.014 | loss: 5.97 | perp: 392.03\n",
      " b 600/720 >> 2707.6 ms/b | lr: 0.0001125 | grad norm:  0.70 | inf norm:  0.014 | loss: 5.95 | perp: 382.06\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1918.18 sec | train loss, perp: 5.977, 394.29    | valid loss, perp: 6.280, 533.72   \n",
      "Grad norm:  0.518 | Grad inf. norm:  0.0165 | Max abs param:  0.2176\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   4/150) lr = 0.0001105\n",
      " b 150/715 >> 2687.5 ms/b | lr: 0.0001276 | grad norm:  0.47 | inf norm:  0.013 | loss: 5.95 | perp: 381.87\n",
      " b 300/715 >> 2642.7 ms/b | lr: 0.0001042 | grad norm:  0.68 | inf norm:  0.020 | loss: 5.88 | perp: 359.59\n",
      " b 450/715 >> 2691.4 ms/b | lr: 0.0001074 | grad norm:  0.57 | inf norm:  0.014 | loss: 5.88 | perp: 356.44\n",
      " b 600/715 >> 2798.7 ms/b | lr: 0.0001009 | grad norm:  0.55 | inf norm:  0.011 | loss: 5.87 | perp: 354.00\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1933.84 sec | train loss, perp: 5.880, 357.82    | valid loss, perp: 6.184, 485.16   \n",
      "Grad norm:  0.853 | Grad inf. norm:  0.0198 | Max abs param:  0.2327\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   5/150) lr = 9.882e-05\n",
      " b 150/721 >> 2761.0 ms/b | lr: 0.0001067 | grad norm:  0.64 | inf norm:  0.010 | loss: 5.86 | perp: 350.44\n",
      " b 300/721 >> 2649.7 ms/b | lr: 9.021e-05 | grad norm:  0.68 | inf norm:  0.012 | loss: 5.80 | perp: 331.16\n",
      " b 450/721 >> 2635.8 ms/b | lr: 9.021e-05 | grad norm:  0.63 | inf norm:  0.012 | loss: 5.79 | perp: 326.84\n",
      " b 600/721 >> 2688.7 ms/b | lr: 8.715e-05 | grad norm:  0.72 | inf norm:  0.013 | loss: 5.78 | perp: 323.21\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1938.72 sec | train loss, perp: 5.796, 329.14    | valid loss, perp: 6.097, 444.67   \n",
      "Grad norm:  0.662 | Grad inf. norm:  0.0146 | Max abs param:  0.2467\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   6/150) lr = 9.021e-05\n",
      " b 150/712 >> 2783.6 ms/b | lr: 9.509e-05 | grad norm:  0.77 | inf norm:  0.015 | loss: 5.78 | perp: 324.14\n",
      " b 300/712 >> 2828.8 ms/b | lr:  0.000102 | grad norm:  0.63 | inf norm:  0.011 | loss: 5.72 | perp: 306.34\n",
      " b 450/712 >> 2692.2 ms/b | lr: 9.021e-05 | grad norm:  0.67 | inf norm:  0.013 | loss: 5.72 | perp: 304.57\n",
      " b 600/712 >> 2742.7 ms/b | lr: 8.505e-05 | grad norm:  0.77 | inf norm:  0.017 | loss: 5.69 | perp: 296.29\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1962.60 sec | train loss, perp: 5.719, 304.45    | valid loss, perp: 5.984, 397.18   \n",
      "Grad norm:  0.990 | Grad inf. norm:  0.0220 | Max abs param:  0.2695\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   7/150) lr = 8.352e-05\n",
      " b 150/713 >> 4049.1 ms/b | lr: 8.352e-05 | grad norm:  0.75 | inf norm:  0.013 | loss: 5.68 | perp: 291.63\n",
      " b 300/713 >> 3831.0 ms/b | lr: 7.874e-05 | grad norm:  1.09 | inf norm:  0.027 | loss: 5.63 | perp: 277.40\n",
      " b 450/713 >> 2943.5 ms/b | lr: 7.366e-05 | grad norm:  0.90 | inf norm:  0.019 | loss: 5.62 | perp: 276.27\n",
      " b 600/713 >> 2919.2 ms/b | lr: 8.117e-05 | grad norm:  0.83 | inf norm:  0.014 | loss: 5.60 | perp: 269.65\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2394.50 sec | train loss, perp: 5.617, 275.13    | valid loss, perp: 5.834, 341.60   \n",
      "Grad norm:  1.506 | Grad inf. norm:  0.0247 | Max abs param:  0.2876\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   8/150) lr = 7.813e-05\n",
      " b 150/713 >> 2914.5 ms/b | lr: 7.132e-05 | grad norm:  1.26 | inf norm:  0.029 | loss: 5.58 | perp: 265.35\n",
      " b 300/713 >> 2829.8 ms/b | lr: 7.132e-05 | grad norm:  0.87 | inf norm:  0.017 | loss: 5.52 | perp: 249.51\n",
      " b 450/713 >> 2796.8 ms/b | lr: 7.366e-05 | grad norm:  1.32 | inf norm:  0.023 | loss: 5.49 | perp: 241.75\n",
      " b 600/713 >> 3458.0 ms/b | lr: 7.813e-05 | grad norm:  1.83 | inf norm:  0.038 | loss: 5.45 | perp: 233.15\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2170.60 sec | train loss, perp: 5.500, 244.59    | valid loss, perp: 5.710, 301.76   \n",
      "Grad norm:  1.192 | Grad inf. norm:  0.0171 | Max abs param:  0.3205\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch   9/150) lr = 7.366e-05\n",
      " b 150/722 >> 3131.6 ms/b | lr: 7.568e-05 | grad norm:  1.68 | inf norm:  0.032 | loss: 5.46 | perp: 235.47\n",
      " b 300/722 >> 3129.4 ms/b | lr: 6.014e-05 | grad norm:  1.46 | inf norm:  0.031 | loss: 5.38 | perp: 216.67\n",
      " b 450/722 >> 3064.4 ms/b | lr: 6.724e-05 | grad norm:  1.03 | inf norm:  0.017 | loss: 5.39 | perp: 218.81\n",
      " b 600/722 >> 3403.3 ms/b | lr:  6.26e-05 | grad norm:  2.04 | inf norm:  0.046 | loss: 5.35 | perp: 211.25\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2347.49 sec | train loss, perp: 5.388, 218.71    | valid loss, perp: 5.620, 275.81   \n",
      "Grad norm:  1.742 | Grad inf. norm:  0.0448 | Max abs param:  0.3522\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  10/150) lr = 6.988e-05\n",
      " b 150/716 >> 3370.7 ms/b | lr: 6.163e-05 | grad norm:  1.21 | inf norm:  0.027 | loss: 5.38 | perp: 216.05\n",
      " b 300/716 >> 3137.5 ms/b | lr: 6.988e-05 | grad norm:  3.05 | inf norm:  0.086 | loss: 5.29 | perp: 199.23\n",
      " b 450/716 >> 2967.2 ms/b | lr: 8.069e-05 | grad norm:  6.35 | inf norm:  0.136 | loss: 5.29 | perp: 197.66\n",
      " b 600/716 >> 2893.5 ms/b | lr: 6.791e-05 | grad norm:  1.73 | inf norm:  0.033 | loss: 5.27 | perp: 193.53\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2192.36 sec | train loss, perp: 5.291, 198.48    | valid loss, perp: 5.426, 227.35   \n",
      "Grad norm:  1.664 | Grad inf. norm:  0.0305 | Max abs param:  0.3760\n",
      "============================================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/150) lr = 6.663e-05\n",
      " b 150/715 >> 2932.6 ms/b | lr: 7.196e-05 | grad norm:  4.06 | inf norm:  0.100 | loss: 5.29 | perp: 198.60\n",
      " b 300/715 >> 2803.9 ms/b | lr: 6.845e-05 | grad norm:  2.41 | inf norm:  0.049 | loss: 5.24 | perp: 189.42\n",
      " b 450/715 >> 2710.4 ms/b | lr: 6.845e-05 | grad norm:  1.88 | inf norm:  0.039 | loss: 5.18 | perp: 177.26\n",
      " b 600/715 >> 2693.5 ms/b | lr: 6.845e-05 | grad norm:  4.11 | inf norm:  0.076 | loss: 5.18 | perp: 177.44\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1969.30 sec | train loss, perp: 5.214, 183.86    | valid loss, perp: 5.531, 252.39 :(\n",
      "Grad norm:  8.162 | Grad inf. norm:  0.1780 | Max abs param:  0.3959\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  12/150) lr = 6.379e-05\n",
      " b 150/717 >> 2645.0 ms/b | lr: 6.554e-05 | grad norm:  2.91 | inf norm:  0.074 | loss: 5.18 | perp: 177.07\n",
      " b 300/717 >> 2632.1 ms/b | lr: 6.379e-05 | grad norm:  2.86 | inf norm:  0.066 | loss: 5.13 | perp: 169.31\n",
      " b 450/717 >> 2577.2 ms/b | lr: 6.724e-05 | grad norm:  5.30 | inf norm:  0.137 | loss: 5.08 | perp: 160.79\n",
      " b 600/717 >> 2602.8 ms/b | lr: 6.199e-05 | grad norm:  4.98 | inf norm:  0.118 | loss: 5.10 | perp: 164.18\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1869.34 sec | train loss, perp: 5.126, 168.27    | valid loss, perp: 5.450, 232.83 :(\n",
      "Grad norm:  7.581 | Grad inf. norm:  0.1473 | Max abs param:  0.4122\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  13/150) lr = 6.129e-05\n",
      " b 150/713 >> 2625.7 ms/b | lr: 5.778e-05 | grad norm:  2.15 | inf norm:  0.038 | loss: 5.14 | perp: 170.27\n",
      " b 300/713 >> 2619.8 ms/b | lr: 6.129e-05 | grad norm:  1.38 | inf norm:  0.027 | loss: 5.07 | perp: 158.76\n",
      " b 450/713 >> 2638.2 ms/b | lr: 5.405e-05 | grad norm:  3.17 | inf norm:  0.075 | loss: 5.04 | perp: 154.26\n",
      " b 600/713 >> 2612.6 ms/b | lr: 5.778e-05 | grad norm:  2.69 | inf norm:  0.071 | loss: 5.00 | perp: 148.09\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1871.47 sec | train loss, perp: 5.074, 159.78    | valid loss, perp: 5.321, 204.68   \n",
      "Grad norm:  1.412 | Grad inf. norm:  0.0296 | Max abs param:  0.4283\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  14/150) lr = 5.906e-05\n",
      " b 150/716 >> 2630.3 ms/b | lr: 5.208e-05 | grad norm:  5.47 | inf norm:  0.129 | loss: 5.04 | perp: 154.20\n",
      " b 300/716 >> 2627.6 ms/b | lr: 5.391e-05 | grad norm:  2.05 | inf norm:  0.045 | loss: 5.03 | perp: 152.71\n",
      " b 450/716 >> 2603.6 ms/b | lr: 5.906e-05 | grad norm:  1.52 | inf norm:  0.028 | loss: 4.97 | perp: 143.41\n",
      " b 600/716 >> 2644.8 ms/b | lr: 6.379e-05 | grad norm: 10.07 | inf norm:  0.214 | loss: 4.99 | perp: 146.68\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1873.06 sec | train loss, perp: 4.999, 148.22    | valid loss, perp: 5.080, 160.79   \n",
      "Grad norm:  1.861 | Grad inf. norm:  0.0345 | Max abs param:  0.4466\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  15/150) lr = 5.705e-05\n",
      " b 150/720 >> 2593.6 ms/b | lr: 5.032e-05 | grad norm:  3.81 | inf norm:  0.099 | loss: 5.01 | perp: 150.37\n",
      " b 300/720 >> 2843.0 ms/b | lr: 5.545e-05 | grad norm:  1.62 | inf norm:  0.032 | loss: 4.97 | perp: 144.21\n",
      " b 450/720 >> 3754.5 ms/b | lr: 6.449e-05 | grad norm:  7.09 | inf norm:  0.219 | loss: 4.97 | perp: 143.53\n",
      " b 600/720 >> 3776.2 ms/b | lr: 5.705e-05 | grad norm: 11.33 | inf norm:  0.293 | loss: 4.94 | perp: 140.44\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2381.07 sec | train loss, perp: 4.968, 143.76    | valid loss, perp: 5.376, 216.26 :(\n",
      "Grad norm:  3.197 | Grad inf. norm:  0.0871 | Max abs param:  0.4625\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  16/150) lr = 5.524e-05\n",
      " b 150/718 >> 3028.2 ms/b | lr: 4.872e-05 | grad norm:  2.29 | inf norm:  0.066 | loss: 4.98 | perp: 145.23\n",
      " b 300/718 >> 2927.7 ms/b | lr: 5.208e-05 | grad norm:  3.03 | inf norm:  0.072 | loss: 4.87 | perp: 130.94\n",
      " b 450/718 >> 2896.8 ms/b | lr: 5.524e-05 | grad norm:  2.12 | inf norm:  0.042 | loss: 4.89 | perp: 133.04\n",
      " b 600/718 >> 3066.3 ms/b | lr: 5.208e-05 | grad norm:  4.02 | inf norm:  0.086 | loss: 4.88 | perp: 131.40\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2138.73 sec | train loss, perp: 4.900, 134.31    | valid loss, perp: 4.889, 132.82   \n",
      "Grad norm:  6.529 | Grad inf. norm:  0.1709 | Max abs param:  0.4774\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  17/150) lr = 5.359e-05\n",
      " b 150/719 >> 2877.4 ms/b | lr: 5.789e-05 | grad norm: 15.77 | inf norm:  0.359 | loss: 4.95 | perp: 141.05\n",
      " b 300/719 >> 2670.8 ms/b | lr: 5.506e-05 | grad norm:  5.08 | inf norm:  0.105 | loss: 4.90 | perp: 134.45\n",
      " b 450/719 >> 2766.2 ms/b | lr:  4.19e-05 | grad norm:  6.87 | inf norm:  0.171 | loss: 4.83 | perp: 124.81\n",
      " b 600/719 >> 2764.9 ms/b | lr: 5.359e-05 | grad norm:  3.17 | inf norm:  0.085 | loss: 4.82 | perp: 124.34\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1984.83 sec | train loss, perp: 4.871, 130.51    | valid loss, perp: 4.904, 134.80 :(\n",
      "Grad norm:  2.926 | Grad inf. norm:  0.0678 | Max abs param:  0.4891\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  18/150) lr = 5.208e-05\n",
      " b 150/713 >> 2776.6 ms/b | lr:  4.91e-05 | grad norm: 13.19 | inf norm:  0.363 | loss: 4.80 | perp: 122.11\n",
      " b 300/713 >> 2706.3 ms/b | lr:  4.91e-05 | grad norm:  6.46 | inf norm:  0.158 | loss: 4.82 | perp: 124.32\n",
      " b 450/713 >> 2779.9 ms/b | lr:  5.49e-05 | grad norm:  2.01 | inf norm:  0.046 | loss: 4.82 | perp: 123.44\n",
      " b 600/713 >> 2833.0 ms/b | lr: 5.351e-05 | grad norm:  8.22 | inf norm:  0.208 | loss: 4.81 | perp: 122.72\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1971.36 sec | train loss, perp: 4.810, 122.70    | valid loss, perp: 4.732, 113.48   \n",
      "Grad norm: 12.798 | Grad inf. norm:  0.2875 | Max abs param:  0.4981\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  19/150) lr = 5.069e-05\n",
      " b 150/719 >> 2694.0 ms/b | lr: 4.927e-05 | grad norm:  6.17 | inf norm:  0.145 | loss: 4.75 | perp: 115.43\n",
      " b 300/719 >> 2811.6 ms/b | lr: 5.069e-05 | grad norm:  9.66 | inf norm:  0.259 | loss: 4.81 | perp: 122.86\n",
      " b 450/719 >> 2727.6 ms/b | lr: 5.069e-05 | grad norm:  1.53 | inf norm:  0.041 | loss: 4.72 | perp: 112.16\n",
      " b 600/719 >> 2717.6 ms/b | lr: 5.344e-05 | grad norm:  3.81 | inf norm:  0.084 | loss: 4.72 | perp: 111.82\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1969.17 sec | train loss, perp: 4.754, 116.00    | valid loss, perp: 5.036, 153.90 :(\n",
      "Grad norm: 10.630 | Grad inf. norm:  0.1973 | Max abs param:  0.5065\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  20/150) lr = 4.941e-05\n",
      " b 150/717 >> 2782.7 ms/b | lr: 4.941e-05 | grad norm: 11.62 | inf norm:  0.278 | loss: 4.80 | perp: 121.84\n",
      " b 300/717 >> 2772.7 ms/b | lr: 5.076e-05 | grad norm:  7.90 | inf norm:  0.218 | loss: 4.69 | perp: 109.30\n",
      " b 450/717 >> 2759.0 ms/b | lr: 4.034e-05 | grad norm:  4.00 | inf norm:  0.108 | loss: 4.69 | perp: 109.00\n",
      " b 600/717 >> 2741.9 ms/b | lr: 5.208e-05 | grad norm: 11.79 | inf norm:  0.263 | loss: 4.63 | perp: 102.32\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1975.56 sec | train loss, perp: 4.724, 112.66    | valid loss, perp: 4.761, 116.87 :(\n",
      "Grad norm:  5.465 | Grad inf. norm:  0.1041 | Max abs param:  0.5173\n",
      "============================================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21/150) lr = 4.822e-05\n",
      " b 150/716 >> 2859.6 ms/b | lr: 4.253e-05 | grad norm:  2.45 | inf norm:  0.099 | loss: 4.74 | perp: 114.09\n",
      " b 300/716 >> 3658.6 ms/b | lr: 5.083e-05 | grad norm:  7.03 | inf norm:  0.278 | loss: 4.65 | perp: 104.71\n",
      " b 450/716 >> 4164.1 ms/b | lr: 4.546e-05 | grad norm:  2.42 | inf norm:  0.068 | loss: 4.68 | perp: 108.03\n",
      " b 600/716 >> 4058.1 ms/b | lr: 4.253e-05 | grad norm:  7.39 | inf norm:  0.204 | loss: 4.56 | perp:  95.72\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2601.35 sec | train loss, perp: 4.658, 105.39    | valid loss, perp: 4.530,  92.80   \n",
      "Grad norm:  4.190 | Grad inf. norm:  0.1144 | Max abs param:  0.5401\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  22/150) lr = 4.711e-05\n",
      " b 150/719 >> 3010.6 ms/b | lr: 5.089e-05 | grad norm:  3.44 | inf norm:  0.088 | loss: 4.76 | perp: 116.43\n",
      " b 300/719 >> 2764.7 ms/b | lr: 4.442e-05 | grad norm:  8.96 | inf norm:  0.232 | loss: 4.62 | perp: 101.41\n",
      " b 450/719 >> 2682.5 ms/b | lr: 4.301e-05 | grad norm:  3.76 | inf norm:  0.080 | loss: 4.51 | perp:  91.08\n",
      " b 600/719 >> 2660.9 ms/b | lr: 4.711e-05 | grad norm: 16.28 | inf norm:  0.380 | loss: 4.65 | perp: 104.56\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1993.38 sec | train loss, perp: 4.633, 102.80    | valid loss, perp: 4.730, 113.30 :(\n",
      "Grad norm:  3.700 | Grad inf. norm:  0.0792 | Max abs param:  0.5678\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  23/150) lr = 4.608e-05\n",
      " b 150/715 >> 2643.9 ms/b | lr: 4.063e-05 | grad norm:  7.20 | inf norm:  0.205 | loss: 4.59 | perp:  98.90\n",
      " b 300/715 >> 2622.7 ms/b | lr: 5.094e-05 | grad norm:  5.05 | inf norm:  0.138 | loss: 4.57 | perp:  96.30\n",
      " b 450/715 >> 2584.1 ms/b | lr: 3.762e-05 | grad norm:  9.23 | inf norm:  0.278 | loss: 4.54 | perp:  93.70\n",
      " b 600/715 >> 2678.5 ms/b | lr: 4.344e-05 | grad norm:  4.95 | inf norm:  0.134 | loss: 4.63 | perp: 102.29\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1882.27 sec | train loss, perp: 4.600,  99.45    | valid loss, perp: 4.791, 120.39 :(\n",
      "Grad norm:  6.973 | Grad inf. norm:  0.1786 | Max abs param:  0.5954\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  24/150) lr = 4.511e-05\n",
      " b 150/725 >> 2598.3 ms/b | lr: 4.755e-05 | grad norm:  7.01 | inf norm:  0.216 | loss: 4.56 | perp:  95.34\n",
      " b 300/725 >> 2564.4 ms/b | lr: 4.118e-05 | grad norm:  7.19 | inf norm:  0.192 | loss: 4.51 | perp:  91.14\n",
      " b 450/725 >> 2545.7 ms/b | lr: 4.383e-05 | grad norm: 14.81 | inf norm:  0.431 | loss: 4.44 | perp:  84.41\n",
      " b 600/725 >> 2609.0 ms/b | lr: 4.511e-05 | grad norm:  4.28 | inf norm:  0.114 | loss: 4.56 | perp:  95.54\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1864.27 sec | train loss, perp: 4.513,  91.22    | valid loss, perp: 4.537,  93.45 :(\n",
      "Grad norm:  7.664 | Grad inf. norm:  0.1871 | Max abs param:  0.6232\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  25/150) lr = 4.419e-05\n",
      " b 150/721 >> 2604.9 ms/b | lr: 4.774e-05 | grad norm:  4.07 | inf norm:  0.163 | loss: 4.61 | perp: 100.40\n",
      " b 300/721 >> 2551.7 ms/b | lr: 3.756e-05 | grad norm:  9.25 | inf norm:  0.301 | loss: 4.55 | perp:  94.31\n",
      " b 450/721 >> 2582.6 ms/b | lr: 4.167e-05 | grad norm:  2.79 | inf norm:  0.084 | loss: 4.52 | perp:  91.44\n",
      " b 600/721 >> 2620.3 ms/b | lr: 4.419e-05 | grad norm:  2.98 | inf norm:  0.076 | loss: 4.46 | perp:  86.09\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1864.92 sec | train loss, perp: 4.536,  93.33 :( | valid loss, perp: 4.550,  94.59 :(\n",
      "Grad norm: 16.483 | Grad inf. norm:  0.4897 | Max abs param:  0.6488\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  26/150) lr = 4.334e-05\n",
      " b 150/708 >> 2626.2 ms/b | lr: 4.452e-05 | grad norm: 18.01 | inf norm:  0.526 | loss: 4.68 | perp: 108.23\n",
      " b 300/708 >> 2644.1 ms/b | lr: 4.334e-05 | grad norm: 13.25 | inf norm:  0.326 | loss: 4.49 | perp:  89.54\n",
      " b 450/708 >> 2621.5 ms/b | lr: 4.452e-05 | grad norm:  5.95 | inf norm:  0.151 | loss: 4.40 | perp:  81.71\n",
      " b 600/708 >> 2685.6 ms/b | lr: 4.086e-05 | grad norm:  2.38 | inf norm:  0.064 | loss: 4.47 | perp:  87.57\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1870.67 sec | train loss, perp: 4.509,  90.84    | valid loss, perp: 4.278,  72.07   \n",
      "Grad norm:  4.136 | Grad inf. norm:  0.1425 | Max abs param:  0.6744\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  27/150) lr = 4.253e-05\n",
      " b 150/719 >> 2639.7 ms/b | lr: 4.133e-05 | grad norm:  4.58 | inf norm:  0.145 | loss: 4.51 | perp:  90.63\n",
      " b 300/719 >> 2615.7 ms/b | lr: 4.369e-05 | grad norm: 24.41 | inf norm:  0.763 | loss: 4.33 | perp:  76.09\n",
      " b 450/719 >> 2580.9 ms/b | lr: 4.369e-05 | grad norm: 10.02 | inf norm:  0.228 | loss: 4.39 | perp:  80.29\n",
      " b 600/719 >> 2646.4 ms/b | lr: 3.882e-05 | grad norm:  5.25 | inf norm:  0.117 | loss: 4.57 | perp:  96.18\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1873.58 sec | train loss, perp: 4.437,  84.50    | valid loss, perp: 4.245,  69.73   \n",
      "Grad norm:  8.130 | Grad inf. norm:  0.1590 | Max abs param:  0.6986\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  28/150) lr = 4.176e-05\n",
      " b 150/723 >> 2597.7 ms/b | lr: 4.058e-05 | grad norm: 14.76 | inf norm:  0.380 | loss: 4.46 | perp:  86.77\n",
      " b 300/723 >> 2603.9 ms/b | lr: 3.812e-05 | grad norm:  7.74 | inf norm:  0.233 | loss: 4.45 | perp:  85.62\n",
      " b 450/723 >> 2554.9 ms/b | lr: 4.176e-05 | grad norm: 11.53 | inf norm:  0.345 | loss: 4.35 | perp:  77.38\n",
      " b 600/723 >> 2614.9 ms/b | lr: 4.058e-05 | grad norm:  3.99 | inf norm:  0.115 | loss: 4.35 | perp:  77.57\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1867.97 sec | train loss, perp: 4.395,  81.08    | valid loss, perp: 4.251,  70.20 :(\n",
      "Grad norm: 11.264 | Grad inf. norm:  0.2948 | Max abs param:  0.7237\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  29/150) lr = 4.103e-05\n",
      " b 150/717 >> 2638.1 ms/b | lr: 3.869e-05 | grad norm: 18.29 | inf norm:  0.556 | loss: 4.65 | perp: 104.71\n",
      " b 300/717 >> 2650.4 ms/b | lr: 4.216e-05 | grad norm:  4.23 | inf norm:  0.143 | loss: 4.49 | perp:  88.75\n",
      " b 450/717 >> 2611.3 ms/b | lr: 3.869e-05 | grad norm:  7.05 | inf norm:  0.165 | loss: 4.37 | perp:  78.66\n",
      " b 600/717 >> 2515.5 ms/b | lr: 3.619e-05 | grad norm: 11.16 | inf norm:  0.354 | loss: 4.21 | perp:  67.42\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1871.40 sec | train loss, perp: 4.444,  85.08 :( | valid loss, perp: 4.161,  64.14   \n",
      "Grad norm: 10.288 | Grad inf. norm:  0.3147 | Max abs param:  0.7477\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  30/150) lr = 4.034e-05\n",
      " b 150/725 >> 2604.4 ms/b | lr: 3.921e-05 | grad norm:  2.76 | inf norm:  0.117 | loss: 4.46 | perp:  86.13\n",
      " b 300/725 >> 2552.9 ms/b | lr: 4.145e-05 | grad norm:  3.61 | inf norm:  0.185 | loss: 4.23 | perp:  68.79\n",
      " b 450/725 >> 2552.7 ms/b | lr: 3.921e-05 | grad norm: 11.52 | inf norm:  0.295 | loss: 4.37 | perp:  79.11\n",
      " b 600/725 >> 2564.8 ms/b | lr:  4.56e-05 | grad norm: 14.33 | inf norm:  0.294 | loss: 4.27 | perp:  71.34\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1867.59 sec | train loss, perp: 4.334,  76.22    | valid loss, perp: 4.424,  83.43 :(\n",
      "Grad norm: 11.626 | Grad inf. norm:  0.2939 | Max abs param:  0.7717\n",
      "============================================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  31/150) lr = 3.969e-05\n",
      " b 150/720 >> 2601.3 ms/b | lr: 3.969e-05 | grad norm: 12.09 | inf norm:  0.365 | loss: 4.33 | perp:  75.60\n",
      " b 300/720 >> 2542.7 ms/b | lr: 3.742e-05 | grad norm: 17.84 | inf norm:  0.455 | loss: 4.26 | perp:  70.81\n",
      " b 450/720 >> 2593.7 ms/b | lr: 4.183e-05 | grad norm: 18.05 | inf norm:  0.545 | loss: 4.36 | perp:  78.59\n",
      " b 600/720 >> 2615.9 ms/b | lr: 3.857e-05 | grad norm:  4.20 | inf norm:  0.128 | loss: 4.31 | perp:  74.52\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1867.41 sec | train loss, perp: 4.304,  73.98    | valid loss, perp: 4.070,  58.56   \n",
      "Grad norm:  3.581 | Grad inf. norm:  0.1501 | Max abs param:  0.7914\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  32/150) lr = 3.906e-05\n",
      " b 150/713 >> 2594.2 ms/b | lr: 3.906e-05 | grad norm: 13.66 | inf norm:  0.401 | loss: 4.36 | perp:  78.09\n",
      " b 300/713 >> 2683.7 ms/b | lr: 4.013e-05 | grad norm:  9.56 | inf norm:  0.220 | loss: 4.57 | perp:  96.12\n",
      " b 450/713 >> 2614.2 ms/b | lr: 3.796e-05 | grad norm: 12.03 | inf norm:  0.365 | loss: 4.21 | perp:  67.54\n",
      " b 600/713 >> 2647.3 ms/b | lr: 3.796e-05 | grad norm:  6.99 | inf norm:  0.193 | loss: 4.30 | perp:  73.51\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1872.91 sec | train loss, perp: 4.341,  76.77 :( | valid loss, perp: 4.051,  57.48   \n",
      "Grad norm: 10.629 | Grad inf. norm:  0.3004 | Max abs param:  0.8135\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  33/150) lr = 3.847e-05\n",
      " b 150/718 >> 2606.3 ms/b | lr: 3.511e-05 | grad norm:  4.92 | inf norm:  0.141 | loss: 4.28 | perp:  72.52\n",
      " b 300/718 >> 2557.7 ms/b | lr: 3.847e-05 | grad norm: 13.75 | inf norm:  0.450 | loss: 4.15 | perp:  63.15\n",
      " b 450/718 >> 2556.7 ms/b | lr: 3.952e-05 | grad norm:  8.83 | inf norm:  0.275 | loss: 4.19 | perp:  66.23\n",
      " b 600/718 >> 2650.6 ms/b | lr: 3.952e-05 | grad norm: 12.16 | inf norm:  0.284 | loss: 4.27 | perp:  71.48\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1870.22 sec | train loss, perp: 4.257,  70.60    | valid loss, perp: 4.278,  72.13 :(\n",
      "Grad norm: 16.213 | Grad inf. norm:  0.3927 | Max abs param:  0.8358\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  34/150) lr = 3.79e-05\n",
      " b 150/721 >> 2607.3 ms/b | lr: 3.683e-05 | grad norm:  9.82 | inf norm:  0.318 | loss: 4.24 | perp:  69.09\n",
      " b 300/721 >> 2609.4 ms/b | lr: 3.893e-05 | grad norm: 23.29 | inf norm:  0.638 | loss: 4.20 | perp:  66.47\n",
      " b 450/721 >> 2559.2 ms/b | lr: 3.683e-05 | grad norm:  6.01 | inf norm:  0.175 | loss: 4.10 | perp:  60.34\n",
      " b 600/721 >> 2628.2 ms/b | lr: 3.459e-05 | grad norm:  5.98 | inf norm:  0.157 | loss: 4.21 | perp:  67.53\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1870.11 sec | train loss, perp: 4.179,  65.32    | valid loss, perp: 3.947,  51.76   \n",
      "Grad norm:  5.057 | Grad inf. norm:  0.1116 | Max abs param:  0.8586\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  35/150) lr = 3.735e-05\n",
      " b 150/716 >> 2634.8 ms/b | lr: 3.735e-05 | grad norm: 19.16 | inf norm:  0.496 | loss: 4.39 | perp:  80.54\n",
      " b 300/716 >> 2631.1 ms/b | lr: 3.735e-05 | grad norm:  7.65 | inf norm:  0.271 | loss: 4.28 | perp:  72.18\n",
      " b 450/716 >> 2609.4 ms/b | lr: 3.735e-05 | grad norm: 30.12 | inf norm:  0.814 | loss: 4.29 | perp:  72.66\n",
      " b 600/716 >> 2555.6 ms/b | lr: 3.294e-05 | grad norm: 20.71 | inf norm:  0.624 | loss: 4.14 | perp:  62.98\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1870.27 sec | train loss, perp: 4.271,  71.62 :( | valid loss, perp: 3.931,  50.98   \n",
      "Grad norm:  6.887 | Grad inf. norm:  0.2012 | Max abs param:  0.8800\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  36/150) lr = 3.683e-05\n",
      " b 150/714 >> 2651.4 ms/b | lr: 4.072e-05 | grad norm:  3.24 | inf norm:  0.098 | loss: 4.26 | perp:  70.84\n",
      " b 300/714 >> 2591.4 ms/b | lr: 3.362e-05 | grad norm: 16.49 | inf norm:  0.459 | loss: 4.26 | perp:  70.92\n",
      " b 450/714 >> 2593.7 ms/b | lr: 3.683e-05 | grad norm:  5.96 | inf norm:  0.135 | loss: 4.12 | perp:  61.34\n",
      " b 600/714 >> 2954.2 ms/b | lr: 3.784e-05 | grad norm:  4.52 | inf norm:  0.143 | loss: 4.19 | perp:  66.16\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1917.97 sec | train loss, perp: 4.203,  66.87 :( | valid loss, perp: 3.886,  48.74   \n",
      "Grad norm: 11.779 | Grad inf. norm:  0.3334 | Max abs param:  0.9011\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  37/150) lr = 3.633e-05\n",
      " b 150/716 >> 2672.2 ms/b | lr: 2.966e-05 | grad norm: 19.57 | inf norm:  0.504 | loss: 4.19 | perp:  66.17\n",
      " b 300/716 >> 2597.3 ms/b | lr: 3.633e-05 | grad norm:  5.71 | inf norm:  0.219 | loss: 4.15 | perp:  63.19\n",
      " b 450/716 >> 2621.0 ms/b | lr: 3.633e-05 | grad norm:  6.69 | inf norm:  0.146 | loss: 4.10 | perp:  60.58\n",
      " b 600/716 >> 2577.4 ms/b | lr: 4.016e-05 | grad norm: 11.80 | inf norm:  0.248 | loss: 3.99 | perp:  53.91\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1874.03 sec | train loss, perp: 4.133,  62.37    | valid loss, perp: 4.647, 104.28 :(\n",
      "Grad norm: 22.597 | Grad inf. norm:  0.6646 | Max abs param:  0.9221\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  38/150) lr = 3.585e-05\n",
      " b 150/728 >> 2631.5 ms/b | lr: 3.585e-05 | grad norm:  5.18 | inf norm:  0.151 | loss: 4.13 | perp:  62.36\n",
      " b 300/728 >> 2579.4 ms/b | lr: 3.779e-05 | grad norm:  6.75 | inf norm:  0.163 | loss: 4.01 | perp:  55.20\n",
      " b 450/728 >> 2482.7 ms/b | lr: 3.683e-05 | grad norm: 15.18 | inf norm:  0.490 | loss: 4.01 | perp:  55.18\n",
      " b 600/728 >> 2623.3 ms/b | lr: 3.683e-05 | grad norm: 16.56 | inf norm:  0.420 | loss: 4.07 | perp:  58.81\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1867.21 sec | train loss, perp: 4.056,  57.74    | valid loss, perp: 3.871,  47.98   \n",
      "Grad norm:  9.619 | Grad inf. norm:  0.2395 | Max abs param:  0.9439\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  39/150) lr = 3.538e-05\n",
      " b 150/722 >> 2608.7 ms/b | lr: 3.538e-05 | grad norm: 26.53 | inf norm:  1.235 | loss: 4.19 | perp:  65.71\n",
      " b 300/722 >> 2610.5 ms/b | lr: 3.439e-05 | grad norm:  5.62 | inf norm:  0.162 | loss: 4.02 | perp:  55.61\n",
      " b 450/722 >> 2548.7 ms/b | lr:  3.23e-05 | grad norm:  8.27 | inf norm:  0.207 | loss: 4.00 | perp:  54.52\n",
      " b 600/722 >> 2557.3 ms/b | lr: 3.007e-05 | grad norm:  7.24 | inf norm:  0.203 | loss: 3.91 | perp:  50.10\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1869.51 sec | train loss, perp: 4.036,  56.58    | valid loss, perp: 3.807,  45.03   \n",
      "Grad norm:  4.764 | Grad inf. norm:  0.1406 | Max abs param:  0.9647\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  40/150) lr = 3.494e-05\n",
      " b 150/714 >> 2592.9 ms/b | lr: 3.395e-05 | grad norm:  7.16 | inf norm:  0.149 | loss: 4.34 | perp:  76.64\n",
      " b 300/714 >> 2637.4 ms/b | lr: 3.683e-05 | grad norm: 22.00 | inf norm:  0.528 | loss: 4.04 | perp:  57.01\n",
      " b 450/714 >> 2626.6 ms/b | lr: 3.395e-05 | grad norm: 21.38 | inf norm:  0.543 | loss: 4.00 | perp:  54.36\n",
      " b 600/714 >> 2632.3 ms/b | lr: 3.494e-05 | grad norm: 20.63 | inf norm:  0.477 | loss: 4.14 | perp:  62.57\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1872.78 sec | train loss, perp: 4.126,  61.90 :( | valid loss, perp: 3.744,  42.26   \n",
      "Grad norm: 21.015 | Grad inf. norm:  0.5338 | Max abs param:  0.9842\n",
      "============================================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  41/150) lr = 3.451e-05\n",
      " b 150/718 >> 2647.7 ms/b | lr:  3.15e-05 | grad norm:  6.59 | inf norm:  0.262 | loss: 4.04 | perp:  56.57\n",
      " b 300/718 >> 2599.9 ms/b | lr: 3.638e-05 | grad norm: 18.22 | inf norm:  0.533 | loss: 4.04 | perp:  56.95\n",
      " b 450/718 >> 2567.1 ms/b | lr: 3.727e-05 | grad norm: 11.89 | inf norm:  0.338 | loss: 3.91 | perp:  50.04\n",
      " b 600/718 >> 2623.6 ms/b | lr: 3.546e-05 | grad norm:  3.72 | inf norm:  0.090 | loss: 4.05 | perp:  57.22\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1868.62 sec | train loss, perp: 4.018,  55.60    | valid loss, perp: 3.705,  40.66   \n",
      "Grad norm:  4.967 | Grad inf. norm:  0.1705 | Max abs param:  1.0045\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  42/150) lr = 3.41e-05\n",
      " b 150/716 >> 2645.0 ms/b | lr: 3.503e-05 | grad norm: 34.18 | inf norm:  1.297 | loss: 4.28 | perp:  71.96\n",
      " b 300/716 >> 2551.8 ms/b | lr:  3.77e-05 | grad norm: 24.11 | inf norm:  0.852 | loss: 4.00 | perp:  54.37\n",
      " b 450/716 >> 2597.0 ms/b | lr: 3.503e-05 | grad norm: 22.49 | inf norm:  0.766 | loss: 4.08 | perp:  59.17\n",
      " b 600/716 >> 2624.7 ms/b | lr: 3.215e-05 | grad norm: 21.24 | inf norm:  0.612 | loss: 3.97 | perp:  53.00\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1866.96 sec | train loss, perp: 4.150,  63.46 :( | valid loss, perp: 3.723,  41.40 :(\n",
      "Grad norm:  3.606 | Grad inf. norm:  0.1433 | Max abs param:  1.0236\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  43/150) lr = 3.37e-05\n",
      " b 150/720 >> 2602.8 ms/b | lr: 3.809e-05 | grad norm:  4.96 | inf norm:  0.127 | loss: 3.97 | perp:  53.19\n",
      " b 300/720 >> 2623.7 ms/b | lr: 2.972e-05 | grad norm:  5.61 | inf norm:  0.166 | loss: 3.92 | perp:  50.49\n",
      " b 450/720 >> 2632.3 ms/b | lr: 3.725e-05 | grad norm: 23.12 | inf norm:  0.565 | loss: 4.17 | perp:  64.55\n",
      " b 600/720 >> 2563.6 ms/b | lr: 3.177e-05 | grad norm:  5.65 | inf norm:  0.330 | loss: 3.93 | perp:  51.09\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1874.20 sec | train loss, perp: 4.009,  55.07    | valid loss, perp: 3.680,  39.64   \n",
      "Grad norm:  7.105 | Grad inf. norm:  0.2057 | Max abs param:  1.0400\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  44/150) lr = 3.331e-05\n",
      " b 150/722 >> 2551.5 ms/b | lr: 3.237e-05 | grad norm:  4.78 | inf norm:  0.189 | loss: 4.13 | perp:  61.95\n",
      " b 300/722 >> 2578.1 ms/b | lr: 3.141e-05 | grad norm: 26.08 | inf norm:  0.877 | loss: 3.91 | perp:  49.90\n",
      " b 450/722 >> 2610.6 ms/b | lr: 3.683e-05 | grad norm: 21.19 | inf norm:  0.781 | loss: 3.89 | perp:  49.00\n",
      " b 600/722 >> 2530.0 ms/b | lr: 3.237e-05 | grad norm:  4.70 | inf norm:  0.242 | loss: 3.94 | perp:  51.30\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1858.66 sec | train loss, perp: 3.966,  52.76    | valid loss, perp: 3.700,  40.46 :(\n",
      "Grad norm: 17.991 | Grad inf. norm:  0.5678 | Max abs param:  1.0594\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  45/150) lr = 3.294e-05\n",
      " b 150/715 >> 2653.0 ms/b | lr: 3.201e-05 | grad norm:  5.83 | inf norm:  0.183 | loss: 4.13 | perp:  62.48\n",
      " b 300/715 >> 2626.5 ms/b | lr: 3.007e-05 | grad norm:  5.90 | inf norm:  0.197 | loss: 3.90 | perp:  49.27\n",
      " b 450/715 >> 2597.4 ms/b | lr: 3.384e-05 | grad norm: 19.32 | inf norm:  0.501 | loss: 3.89 | perp:  48.82\n",
      " b 600/715 >> 2573.8 ms/b | lr: 3.558e-05 | grad norm: 22.25 | inf norm:  0.561 | loss: 3.78 | perp:  43.92\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1863.11 sec | train loss, perp: 3.920,  50.40    | valid loss, perp: 3.603,  36.71   \n",
      "Grad norm:  6.177 | Grad inf. norm:  0.1886 | Max abs param:  1.0785\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  46/150) lr = 3.258e-05\n",
      " b 150/721 >> 2609.1 ms/b | lr: 2.873e-05 | grad norm:  5.48 | inf norm:  0.161 | loss: 3.97 | perp:  53.09\n",
      " b 300/721 >> 2612.6 ms/b | lr:  2.66e-05 | grad norm: 13.47 | inf norm:  0.373 | loss: 3.84 | perp:  46.38\n",
      " b 450/721 >> 2578.8 ms/b | lr: 3.347e-05 | grad norm: 10.24 | inf norm:  0.271 | loss: 3.88 | perp:  48.61\n",
      " b 600/721 >> 2556.0 ms/b | lr: 3.072e-05 | grad norm:  7.60 | inf norm:  0.244 | loss: 3.69 | perp:  40.02\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1863.83 sec | train loss, perp: 3.857,  47.33    | valid loss, perp: 3.576,  35.71   \n",
      "Grad norm:  4.640 | Grad inf. norm:  0.1505 | Max abs param:  1.0969\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  47/150) lr = 3.223e-05\n",
      " b 150/718 >> 2626.0 ms/b | lr: 3.132e-05 | grad norm: 14.06 | inf norm:  0.401 | loss: 3.89 | perp:  49.00\n",
      " b 300/718 >> 2546.8 ms/b | lr: 3.039e-05 | grad norm:  8.32 | inf norm:  0.215 | loss: 3.83 | perp:  45.94\n",
      " b 450/718 >> 2603.1 ms/b | lr: 3.223e-05 | grad norm:  6.81 | inf norm:  0.309 | loss: 3.83 | perp:  45.94\n",
      " b 600/718 >> 2605.3 ms/b | lr: 2.942e-05 | grad norm: 27.20 | inf norm:  0.793 | loss: 3.84 | perp:  46.75\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1861.24 sec | train loss, perp: 3.872,  48.02 :( | valid loss, perp: 3.934,  51.11 :(\n",
      "Grad norm: 23.462 | Grad inf. norm:  0.6501 | Max abs param:  1.1147\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  48/150) lr = 3.189e-05\n",
      " b 150/722 >> 2536.1 ms/b | lr: 3.277e-05 | grad norm: 23.30 | inf norm:  0.602 | loss: 3.95 | perp:  51.95\n",
      " b 300/722 >> 2557.7 ms/b | lr: 2.493e-05 | grad norm:  8.09 | inf norm:  0.264 | loss: 3.76 | perp:  42.98\n",
      " b 450/722 >> 2612.4 ms/b | lr: 2.912e-05 | grad norm:  6.52 | inf norm:  0.166 | loss: 3.95 | perp:  51.95\n",
      " b 600/722 >> 2587.8 ms/b | lr: 2.711e-05 | grad norm: 16.20 | inf norm:  0.493 | loss: 3.81 | perp:  45.05\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1861.62 sec | train loss, perp: 3.877,  48.29 :( | valid loss, perp: 3.572,  35.59   \n",
      "Grad norm: 12.771 | Grad inf. norm:  0.3042 | Max abs param:  1.1322\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  49/150) lr = 3.157e-05\n",
      " b 150/717 >> 2619.3 ms/b | lr: 2.882e-05 | grad norm: 18.54 | inf norm:  0.535 | loss: 3.85 | perp:  46.78\n",
      " b 300/717 >> 2598.3 ms/b | lr: 3.157e-05 | grad norm: 27.18 | inf norm:  0.930 | loss: 3.91 | perp:  50.07\n",
      " b 450/717 >> 2600.4 ms/b | lr: 2.976e-05 | grad norm:  7.15 | inf norm:  0.220 | loss: 3.78 | perp:  43.60\n",
      " b 600/717 >> 2688.0 ms/b | lr: 3.068e-05 | grad norm:  5.15 | inf norm:  0.210 | loss: 3.69 | perp:  40.06\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1902.95 sec | train loss, perp: 3.820,  45.59    | valid loss, perp: 3.494,  32.93   \n",
      "Grad norm: 15.179 | Grad inf. norm:  0.4546 | Max abs param:  1.1502\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  50/150) lr = 3.125e-05\n",
      " b 150/716 >> 2837.1 ms/b | lr: 3.294e-05 | grad norm: 27.90 | inf norm:  0.914 | loss: 4.00 | perp:  54.85\n",
      " b 300/716 >> 2699.3 ms/b | lr: 3.037e-05 | grad norm:  8.33 | inf norm:  0.248 | loss: 3.81 | perp:  45.18\n",
      " b 450/716 >> 2771.0 ms/b | lr: 3.125e-05 | grad norm: 35.09 | inf norm:  0.951 | loss: 3.83 | perp:  46.15\n",
      " b 600/716 >> 4044.0 ms/b | lr: 2.946e-05 | grad norm:  7.97 | inf norm:  0.181 | loss: 3.71 | perp:  40.84\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2315.60 sec | train loss, perp: 3.852,  47.07 :( | valid loss, perp: 3.478,  32.38   \n",
      "Grad norm:  2.620 | Grad inf. norm:  0.0878 | Max abs param:  1.1677\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  51/150) lr = 3.094e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/715 >> 4124.1 ms/b | lr: 3.007e-05 | grad norm:  5.85 | inf norm:  0.179 | loss: 3.83 | perp:  45.95\n",
      " b 300/715 >> 4068.0 ms/b | lr: 2.825e-05 | grad norm:  6.93 | inf norm:  0.181 | loss: 3.85 | perp:  46.85\n",
      " b 450/715 >> 4088.5 ms/b | lr: 3.262e-05 | grad norm: 17.49 | inf norm:  0.575 | loss: 3.77 | perp:  43.25\n",
      " b 600/715 >> 3867.4 ms/b | lr: 3.421e-05 | grad norm:  5.28 | inf norm:  0.149 | loss: 3.62 | perp:  37.27\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2773.98 sec | train loss, perp: 3.752,  42.60    | valid loss, perp: 3.443,  31.29   \n",
      "Grad norm:  3.606 | Grad inf. norm:  0.1245 | Max abs param:  1.1856\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  52/150) lr = 3.064e-05\n",
      " b 150/716 >> 3006.9 ms/b | lr: 2.702e-05 | grad norm:  5.45 | inf norm:  0.180 | loss: 3.98 | perp:  53.50\n",
      " b 300/716 >> 2781.7 ms/b | lr: 2.604e-05 | grad norm:  5.52 | inf norm:  0.284 | loss: 3.66 | perp:  38.80\n",
      " b 450/716 >> 2789.7 ms/b | lr: 2.978e-05 | grad norm: 27.49 | inf norm:  0.747 | loss: 3.78 | perp:  43.71\n",
      " b 600/716 >> 2788.7 ms/b | lr: 2.978e-05 | grad norm:  5.99 | inf norm:  0.182 | loss: 3.66 | perp:  38.91\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2040.04 sec | train loss, perp: 3.783,  43.94 :( | valid loss, perp: 3.434,  31.01   \n",
      "Grad norm:  9.857 | Grad inf. norm:  0.2651 | Max abs param:  1.2029\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  53/150) lr = 3.035e-05\n",
      " b 150/723 >> 2787.9 ms/b | lr: 3.278e-05 | grad norm:  5.86 | inf norm:  0.175 | loss: 3.81 | perp:  45.26\n",
      " b 300/723 >> 2761.5 ms/b | lr:  2.95e-05 | grad norm: 17.55 | inf norm:  0.583 | loss: 3.64 | perp:  38.07\n",
      " b 450/723 >> 2822.2 ms/b | lr: 2.862e-05 | grad norm: 11.90 | inf norm:  0.368 | loss: 3.70 | perp:  40.53\n",
      " b 600/723 >> 2817.9 ms/b | lr: 3.199e-05 | grad norm:  9.47 | inf norm:  0.531 | loss: 3.63 | perp:  37.65\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2004.66 sec | train loss, perp: 3.721,  41.31    | valid loss, perp: 3.486,  32.66 :(\n",
      "Grad norm:  7.452 | Grad inf. norm:  0.2523 | Max abs param:  1.2201\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  54/150) lr = 3.007e-05\n",
      " b 150/717 >> 2621.9 ms/b | lr: 2.555e-05 | grad norm: 26.62 | inf norm:  0.897 | loss: 3.69 | perp:  40.03\n",
      " b 300/717 >> 2698.2 ms/b | lr: 3.248e-05 | grad norm: 21.33 | inf norm:  0.624 | loss: 3.60 | perp:  36.42\n",
      " b 450/717 >> 2845.4 ms/b | lr: 3.007e-05 | grad norm: 28.11 | inf norm:  0.709 | loss: 3.79 | perp:  44.13\n",
      " b 600/717 >> 2930.3 ms/b | lr: 3.007e-05 | grad norm: 32.59 | inf norm:  0.971 | loss: 3.81 | perp:  45.06\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2005.79 sec | train loss, perp: 3.740,  42.09 :( | valid loss, perp: 3.373,  29.16   \n",
      "Grad norm:  6.146 | Grad inf. norm:  0.1359 | Max abs param:  1.2354\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  55/150) lr = 2.98e-05\n",
      " b 150/724 >> 2822.6 ms/b | lr: 3.218e-05 | grad norm: 21.89 | inf norm:  0.620 | loss: 3.68 | perp:  39.84\n",
      " b 300/724 >> 2827.1 ms/b | lr: 3.368e-05 | grad norm:  5.18 | inf norm:  0.172 | loss: 3.62 | perp:  37.22\n",
      " b 450/724 >> 2829.2 ms/b | lr:  2.98e-05 | grad norm: 15.74 | inf norm:  0.624 | loss: 3.62 | perp:  37.48\n",
      " b 600/724 >> 2814.2 ms/b | lr: 2.896e-05 | grad norm: 15.94 | inf norm:  0.469 | loss: 3.59 | perp:  36.12\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2048.26 sec | train loss, perp: 3.635,  37.90    | valid loss, perp: 3.391,  29.69 :(\n",
      "Grad norm:  3.870 | Grad inf. norm:  0.0988 | Max abs param:  1.2521\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  56/150) lr = 2.953e-05\n",
      " b 150/716 >> 2778.1 ms/b | lr: 2.604e-05 | grad norm: 15.42 | inf norm:  0.391 | loss: 3.69 | perp:  40.07\n",
      " b 300/716 >> 2893.4 ms/b | lr: 2.953e-05 | grad norm:  7.55 | inf norm:  0.307 | loss: 3.83 | perp:  46.17\n",
      " b 450/716 >> 2881.7 ms/b | lr: 2.784e-05 | grad norm:  7.46 | inf norm:  0.247 | loss: 3.63 | perp:  37.78\n",
      " b 600/716 >> 2902.0 ms/b | lr: 3.034e-05 | grad norm: 16.96 | inf norm:  0.781 | loss: 3.68 | perp:  39.77\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2046.15 sec | train loss, perp: 3.716,  41.12 :( | valid loss, perp: 3.423,  30.67 :(\n",
      "Grad norm: 18.195 | Grad inf. norm:  0.5927 | Max abs param:  1.2686\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  57/150) lr = 2.927e-05\n",
      " b 150/714 >> 2852.5 ms/b | lr: 2.844e-05 | grad norm:  4.59 | inf norm:  0.110 | loss: 3.71 | perp:  40.77\n",
      " b 300/714 >> 2879.4 ms/b | lr: 3.007e-05 | grad norm:  4.25 | inf norm:  0.168 | loss: 3.57 | perp:  35.61\n",
      " b 450/714 >> 2857.0 ms/b | lr: 2.844e-05 | grad norm:  5.87 | inf norm:  0.191 | loss: 3.65 | perp:  38.43\n",
      " b 600/714 >> 2892.4 ms/b | lr: 2.759e-05 | grad norm: 15.87 | inf norm:  0.469 | loss: 3.67 | perp:  39.18\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2060.82 sec | train loss, perp: 3.694,  40.21 :( | valid loss, perp: 3.314,  27.51   \n",
      "Grad norm:  7.151 | Grad inf. norm:  0.2302 | Max abs param:  1.2846\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  58/150) lr = 2.901e-05\n",
      " b 150/713 >> 2957.4 ms/b | lr: 2.559e-05 | grad norm: 41.33 | inf norm:  1.065 | loss: 3.67 | perp:  39.32\n",
      " b 300/713 >> 2866.9 ms/b | lr: 2.901e-05 | grad norm: 11.85 | inf norm:  0.326 | loss: 3.63 | perp:  37.68\n",
      " b 450/713 >> 2927.6 ms/b | lr: 2.649e-05 | grad norm: 14.77 | inf norm:  0.462 | loss: 3.55 | perp:  34.66\n",
      " b 600/713 >> 2871.4 ms/b | lr:  2.82e-05 | grad norm: 19.45 | inf norm:  0.565 | loss: 3.50 | perp:  33.01\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2065.30 sec | train loss, perp: 3.604,  36.74    | valid loss, perp: 3.357,  28.70 :(\n",
      "Grad norm: 10.202 | Grad inf. norm:  0.2963 | Max abs param:  1.3013\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  59/150) lr = 2.877e-05\n",
      " b 150/716 >> 2877.7 ms/b | lr: 2.796e-05 | grad norm: 18.57 | inf norm:  0.677 | loss: 3.68 | perp:  39.83\n",
      " b 300/716 >> 2924.4 ms/b | lr: 2.349e-05 | grad norm:  6.82 | inf norm:  0.176 | loss: 3.69 | perp:  40.07\n",
      " b 450/716 >> 2906.4 ms/b | lr: 2.877e-05 | grad norm:  5.14 | inf norm:  0.206 | loss: 3.62 | perp:  37.21\n",
      " b 600/716 >> 2939.0 ms/b | lr: 2.626e-05 | grad norm:  4.72 | inf norm:  0.216 | loss: 3.45 | perp:  31.39\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2257.05 sec | train loss, perp: 3.644,  38.23 :( | valid loss, perp: 3.366,  28.95 :(\n",
      "Grad norm: 27.418 | Grad inf. norm:  0.7553 | Max abs param:  1.3166\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  60/150) lr = 2.853e-05\n",
      " b 150/724 >> 4557.9 ms/b | lr:  2.69e-05 | grad norm:  4.50 | inf norm:  0.194 | loss: 3.71 | perp:  40.71\n",
      " b 300/724 >> 4485.2 ms/b | lr: 2.516e-05 | grad norm:  5.08 | inf norm:  0.129 | loss: 3.53 | perp:  34.09\n",
      " b 450/724 >> 4392.0 ms/b | lr: 3.007e-05 | grad norm: 24.85 | inf norm:  0.736 | loss: 3.61 | perp:  36.93\n",
      " b 600/724 >> 4583.4 ms/b | lr: 2.853e-05 | grad norm: 20.84 | inf norm:  0.709 | loss: 3.57 | perp:  35.64\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 3194.60 sec | train loss, perp: 3.633,  37.81 :( | valid loss, perp: 3.265,  26.19   \n",
      "Grad norm: 13.010 | Grad inf. norm:  0.3290 | Max abs param:  1.3325\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Epoch  61/150) lr = 2.829e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/716 >> 3736.8 ms/b | lr: 2.982e-05 | grad norm: 32.27 | inf norm:  1.183 | loss: 3.73 | perp:  41.65\n",
      " b 300/716 >> 2887.4 ms/b | lr: 3.056e-05 | grad norm: 33.71 | inf norm:  0.963 | loss: 3.83 | perp:  45.91\n",
      " b 450/716 >> 2799.8 ms/b | lr: 2.583e-05 | grad norm:  6.56 | inf norm:  0.187 | loss: 3.71 | perp:  40.69\n",
      " b 600/716 >> 2909.4 ms/b | lr: 2.982e-05 | grad norm: 21.68 | inf norm:  0.554 | loss: 3.49 | perp:  32.69\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at C:\\Anaconda2\\conda-bld\\pytorch_1513133520683\\work\\torch\\lib\\TH\\THGeneral.c:246",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-82b67dc582ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_loc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32mE:\\Coding\\AI\\attention\\recurrent_attention\\trainer.py\u001b[0m in \u001b[0;36mtrain_eval_loop\u001b[1;34m(model, train_data, val_data, batch_size, eval_batch_size, seq_len, ntokens, criterion, eval_criterion, optimizer, lr_scheduler, epochs, warmup_steps, early_stopping, clip, log_interval, ckpt)\u001b[0m\n\u001b[0;32m    189\u001b[0m         stat, train_loss, data, targets, states, nstates, total_norm, inf_norm = train(\n\u001b[0;32m    190\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         )\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCAUSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Coding\\AI\\attention\\recurrent_attention\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data, batch_size, seq_len, ntokens, criterion, optimizer, lr_scheduler, clip, log_interval)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# Run the model forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_epoch_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Coding\\AI\\attention\\recurrent_attention\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, states)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;31m# Projection layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproj_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\conda_jupyter\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at C:\\Anaconda2\\conda-bld\\pytorch_1513133520683\\work\\torch\\lib\\TH\\THGeneral.c:246"
     ]
    }
   ],
   "source": [
    "train_stats, stat, train_loss, data, targets, states, nstates = train_eval_loop(\n",
    "    model, train_data, val_data, batch_size, eval_batch_size,\n",
    "    seq_len, ntokens, criterion, eval_criterion, optimizer,\n",
    "    lr_scheduler, epochs, warmup_steps, early_stopping,\n",
    "    clip, log_interval, ckpt_loc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAUSES = ['output', 'grad']\n",
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    _states = states\n",
    "    if model.n_enc_layers > 0:\n",
    "        enc_states = states[0]\n",
    "        states = states[1:]\n",
    "    if model.n_dec_layers > 0:\n",
    "        dec_states = states[-1]\n",
    "        states = states[:-1]\n",
    "    attn_states = states[0]\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoders(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoders(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])\n",
    "else:\n",
    "    # Save training stats\n",
    "    with open(os.path.join('stats', model_test_name+'.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_stats, f)\n",
    "    with open(os.path.join('models', model_test_name+'.pt'), 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(\n",
    "    model, test_data, eval_batch_size, seq_len,\n",
    "    ntokens, eval_criterion, save_wts = True\n",
    ")\n",
    "print('test_loss: {:5.3f} | test_perplexity: {:7.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 4\n",
    "model.eval(save_wts = True)\n",
    "# Get some data from a random point in the test_data set\n",
    "states = model.init_states(nb)\n",
    "data, targets = get_batch(test_data, 120, seq_len, evaluate = True)\n",
    "data = data[:,:nb].contiguous()\n",
    "targets = targets.view(seq_len, -1)[:,:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output, states = model(data, states)\n",
    "# Convert the output log probabilities to normal probabilities\n",
    "output = output.exp()\n",
    "# Get the argmax of each step in the output\n",
    "output_p, output_idx = output.max(dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted output word indices to the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.t()\n",
    "output_idx = output_idx.t()\n",
    "for i in range(nb):\n",
    "    # Print the output with the targets\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Some basic weight heat maps to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_wts = np.array(model.embedding.weight.data)\n",
    "embed_norm = (embed_wts - embed_wts.mean()) / (embed_wts.max() - embed_wts.min())\n",
    "plt.imshow(embed_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('dim'); plt.ylabel('word index');\n",
    "plt.title('Embedding layer')\n",
    "plt.colorbar()\n",
    "embed_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_wts = np.array(attn.alignment.weight.data)\n",
    "plt.imshow(align_wts, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_align_in'); plt.ylabel('d_align_out')\n",
    "plt.title('Alignment sublayer weights')\n",
    "plt.colorbar()\n",
    "align_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_wts = np.array(attn.attention.weight.data)\n",
    "attn_norm = (attn_wts - attn_wts.mean()) / (attn_wts.max() - attn_wts.min())\n",
    "plt.imshow(attn_wts, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_input+d_state'); plt.ylabel('d_output')\n",
    "plt.title('Attention sublayer weights (in attention mechanism)')\n",
    "plt.colorbar()\n",
    "attn_wts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence attention visualization by mapping the alignment weights (in the attention mechanism) at each step of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = nb//cols\n",
    "fig, axs = plt.subplots(rows, cols, figsize = (30, 20))\n",
    "for b in range(nb):\n",
    "    wts = attn.attn_wts[:,b,:]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    r = b // cols\n",
    "    c = b % cols\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(wts, aspect = 'auto', cmap = 'jet')\n",
    "    # Fix labels\n",
    "    xlabels = list(targets[b].data)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_xlabel('Targets')\n",
    "    ylabels = list(data[:,b].data)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_ylabel('Inputs')\n",
    "    ax.set_title('Example %d' % b)\n",
    "    fig.colorbar(im, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training stat curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1, train_stats['epochs']+1)\n",
    "train_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_range, train_stats['train_loss'])\n",
    "plt.plot(epoch_range, train_stats['val_loss'])\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Loss over training time')\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_range, train_stats['max_param'])\n",
    "plt.xlabel('Epoch'); plt.ylabel('Largest parameter magnitude')\n",
    "plt.title('Largest parameter magnitude over training time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, pnorm_ax = plt.subplots()\n",
    "pnorm_ax.plot(epoch_range, train_stats['grad_norm'], 'b')\n",
    "pnorm_ax.set_xlabel('Epoch');\n",
    "pnorm_ax.set_ylabel('2-norm', color = 'b')\n",
    "\n",
    "infnorm_ax = pnorm_ax.twinx()\n",
    "infnorm_ax.plot(epoch_range, train_stats['grad_inf_norm'], 'r')\n",
    "infnorm_ax.set_ylabel('infinite norm', color = 'r')\n",
    "\n",
    "plt.title('Gradient norms over training time')\n",
    "#fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
