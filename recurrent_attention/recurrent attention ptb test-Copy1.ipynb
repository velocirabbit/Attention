{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from recurrent_attention import RecurrentAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead stuff\n",
    "\n",
    "Helper functions for batching, resetting hidden states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 2\n",
    "lr = 0.02\n",
    "warmup_steps = 10\n",
    "decay_factor = 0.5  # Higher => faster learning rate decay\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 100\n",
    "log_interval = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 512\n",
    "encode_size = 256\n",
    "h_size = 256\n",
    "decode_size = 256\n",
    "decode_out_size = 512\n",
    "n_enc_layers = 1\n",
    "attn_rnn_layers = 1\n",
    "n_dec_layers = 1\n",
    "smooth_align = True\n",
    "\n",
    "bidirectional_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, `batchify` arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into batches\n",
    "    nbatches = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data.narrow(0, 0, nbatches * batch_size)\n",
    "    # Evenly divide the data across the batches\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "# Wraps hidden states into new Variables to detach them from their history\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "# `get_batch` subdivides the source data into chunks of the specified length.\n",
    "# E.g., using the example for the `batchify` function above and a length of 2,\n",
    "# we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the `batchify` function. The chunks are along dimension 0, corresponding\n",
    "# to the `seq_len` dimension in the LSTM.\n",
    "def get_batch(source, i, seq_len, evaluate = False):\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = Variable(source[i : i+seq_len], volatile = evaluate)\n",
    "    target = Variable(source[i+1 : i+1+seq_len].view(-1), volatile = evaluate)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing class for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx = None, smoothing = 0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist.add_(self.smoothing / self.size)\n",
    "        if self.padding_idx is not None:\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad = False)) * ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler that sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(h_size, warmup, optimizer):\n",
    "    lrate = lambda e: h_size**(-0.5) * min((e+1)**(-decay_factor), (e+1) * warmup**(-(decay_factor+1)))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22613881cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226117fed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps*2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.4g:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps*2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus(os.path.join('.', 'data', 'ptb'))\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, embed_size, encode_size, h_size,\n",
    "                 decode_size, decode_out_size, n_enc_layers, attn_rnn_layers,\n",
    "                 n_dec_layers, align_location = False, loc_align_size = 1,\n",
    "                 loc_align_kernel = 1,  smooth_align = False, bidirectional_attn = False,\n",
    "                 tie_wts = True, dropout = 0.1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.embed_size = embed_size\n",
    "        self.encode_size = encode_size\n",
    "        self.h_size = h_size\n",
    "        self.decode_size = decode_size\n",
    "        self.decode_out_size = decode_out_size\n",
    "        self.n_enc_layers = n_enc_layers\n",
    "        self.attn_rnn_layers = attn_rnn_layers\n",
    "        self.n_dec_layers = n_dec_layers\n",
    "        self.align_location = align_location\n",
    "        self.loc_align_size = loc_align_size\n",
    "        self.loc_align_kernel = loc_align_kernel\n",
    "        self.smooth_align = smooth_align\n",
    "        self.bidirectional_attn = bidirectional_attn\n",
    "        self.tie_wts = tie_wts\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab, embed_size)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size = embed_size if i == 0 else encode_size,\n",
    "                hidden_size = encode_size, dropout = dropout\n",
    "            ) for i in range(n_enc_layers)\n",
    "        ])\n",
    "        self.attn = RecurrentAttention(\n",
    "            in_size = encode_size, h_size = h_size, out_size = decode_size,\n",
    "            align_location = align_location, loc_align_size = loc_align_size,\n",
    "            loc_align_kernel = loc_align_kernel, smooth_align = smooth_align,\n",
    "            num_rnn_layers = attn_rnn_layers, attn_act_fn = 'ReLU',\n",
    "            dropout = dropout, bidirectional = bidirectional_attn\n",
    "        )\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size = decode_size, dropout = dropout,\n",
    "                hidden_size = decode_size if i < n_dec_layers-1 else decode_out_size,\n",
    "            ) for i in range(n_dec_layers)\n",
    "        ])\n",
    "        self.projection = nn.Linear(decode_out_size, tgt_vocab)\n",
    "        if tie_wts and src_vocab == tgt_vocab and embed_size == decode_out_size:\n",
    "            self.embedding.weight = self.projection.weight\n",
    "        self.log_softmax = nn.LogSoftmax(dim = -1)\n",
    "            \n",
    "        # For visualizations\n",
    "        self.save_wts = False\n",
    "        self.enc_out = None\n",
    "        self.dec_out = None\n",
    "        \n",
    "    def init(self):\n",
    "        for subnet in [self.encoders, self.decoders]:\n",
    "            for layer in subnet:\n",
    "                for p in layer.parameters():\n",
    "                    if p.dim() > 1:\n",
    "                        nn.init.xavier_normal(p)\n",
    "                    else:\n",
    "                        p.data.fill_(0)\n",
    "        for p in self.projection.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "            else:\n",
    "                p.data.fill_(0)\n",
    "        self.attn.init()\n",
    "        \n",
    "    def init_states(self, batch_size):\n",
    "        encoder_states = [\n",
    "            (\n",
    "                Variable(torch.zeros(1, batch_size, self.encode_size)),\n",
    "                Variable(torch.zeros(1, batch_size, self.encode_size))\n",
    "            ) for _ in range(self.n_enc_layers)\n",
    "        ]\n",
    "        attn_states = self.attn.init_rnn_states(batch_size)\n",
    "        decoder_states = [\n",
    "            (\n",
    "                Variable(torch.zeros(\n",
    "                    1, batch_size, self.decode_size if i < self.n_dec_layers-1 else self.decode_out_size\n",
    "                )),\n",
    "                Variable(torch.zeros(\n",
    "                    1, batch_size, self.decode_size if i < self.n_dec_layers-1 else self.decode_out_size\n",
    "                ))\n",
    "            ) for i in range(self.n_dec_layers)\n",
    "        ]\n",
    "        return encoder_states, attn_states, decoder_states\n",
    "    \n",
    "    def forward(self, inputs, states):\n",
    "        enc_states, attn_states, dec_states = states\n",
    "        if self.save_wts:\n",
    "            self.enc_out = []\n",
    "            self.dec_out = []\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = self.embedding(inputs) * np.sqrt(self.embed_size)\n",
    "        \n",
    "        # Encoder stack\n",
    "        new_enc_states = []\n",
    "        enc_in = self.drop(self.relu(embeddings))\n",
    "        for states, encoder in zip(enc_states, self.encoders):\n",
    "            enc_out, new_enc_state = encoder(enc_in, states)\n",
    "            new_enc_states.append(new_enc_state)\n",
    "            if self.save_wts:\n",
    "                self.enc_out.append(enc_out.data.clone())\n",
    "            enc_in = enc_out\n",
    "                \n",
    "        # Attention mechanism\n",
    "        attn_out, new_attn_states = self.attn(enc_out, attn_states)\n",
    "        \n",
    "        # Decoder stack\n",
    "        new_dec_states = []\n",
    "        dec_in = attn_out\n",
    "        for states, decoder in zip(dec_states, self.decoders):\n",
    "            dec_out, new_dec_state = decoder(dec_in, states)\n",
    "            new_dec_states.append(new_dec_state)\n",
    "            if self.save_wts:\n",
    "                self.dec_out.append(dec_out.data.clone())\n",
    "            dec_in = dec_out\n",
    "        \n",
    "        # Projection layer\n",
    "        logits = self.projection(dec_out)\n",
    "        output = self.log_softmax(logits)\n",
    "        \n",
    "        return output, (new_enc_states, new_attn_states, new_dec_states)\n",
    "    \n",
    "    def train(self, mode = True, save_wts = False):\n",
    "        super(RNNModel, self).train(mode)\n",
    "        self.attn.save_attn_wts = save_wts\n",
    "        self.save_wts = save_wts\n",
    "        \n",
    "    def eval(self, save_wts = True):\n",
    "        super(RNNModel, self).eval()\n",
    "        self.attn.save_attn_wts = save_wts\n",
    "        self.save_wts = save_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 8481297\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    src_vocab = ntokens, tgt_vocab = ntokens, embed_size = embed_size,\n",
    "    encode_size = encode_size, h_size = h_size, decode_size = decode_size,\n",
    "    decode_out_size = decode_out_size, n_enc_layers = n_enc_layers,\n",
    "    attn_rnn_layers = attn_rnn_layers, n_dec_layers = n_dec_layers,\n",
    "    dropout = dropout, smooth_align = smooth_align\n",
    ")\n",
    "model.init()\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, optimizer)\n",
    "# Reference\n",
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12562, 74]), torch.Size([7376, 10]), torch.Size([8243, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Use random length sequences\n",
    "    seq_lens = []\n",
    "    tot_len = 0\n",
    "    jitter = 0.15 * seq_len\n",
    "    num_data = train_data.size(0)\n",
    "    while tot_len < num_data - 2:\n",
    "        if num_data - tot_len - 2 <= seq_len + jitter:\n",
    "            slen = num_data - tot_len - 2\n",
    "        else:\n",
    "            slen = int(np.random.normal(seq_len, jitter))\n",
    "            if slen <= 0:\n",
    "                slen = seq_len    # eh\n",
    "            if tot_len + slen >= num_data - jitter - 2:\n",
    "                slen = num_data - tot_len - 2\n",
    "        seq_lens.append(slen)\n",
    "        tot_len += slen\n",
    "    i_cumseq = [0] + list(np.cumsum(seq_lens)[:-1])\n",
    "    idx = np.arange(len(seq_lens))\n",
    "    np.random.shuffle(idx)\n",
    "    # Turn on training mode\n",
    "    model.train(save_wts = False)\n",
    "    # Initialize RNN states\n",
    "    states = model.init_states(batch_size)\n",
    "    # Prep metainfo\n",
    "    total_loss = 0\n",
    "    total_epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(idx):\n",
    "        # Get training data\n",
    "        data, targets = get_batch(train_data, i_cumseq[i], seq_lens[i])\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states)\n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run the model forward\n",
    "        output, _states = model(data, states)\n",
    "        if np.isnan(output.data).any():\n",
    "            return 0, total_epoch_loss[0], data, targets, states, _states\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        if np.isnan(loss.data[0]):\n",
    "            return 1, total_epoch_loss[0], data, targets, states, _states\n",
    "        states = _states\n",
    "        # Propagate loss gradient backwards\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # Save gradient statistics before they're changed cuz we'll be logging this batch\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            # Calculate the largest (absolute) gradient of all elements in the model parameters\n",
    "            max_grad = max([p.grad.data.abs().max() for p in parameters])\n",
    "        total_norm = nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        # Scale the batch learning rate so that shorter sequences aren't \"stronger\"\n",
    "        scaled_lr = lr_scheduler.get_lr()[0] * np.sqrt(seq_lens[i] / seq_len)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = scaled_lr\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get some metainfo\n",
    "        total_loss += loss.data\n",
    "        total_epoch_loss += loss.data * data.size(0)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            print(' b {:3d}/{:3d} >> {:6.1f} ms/b | lr: {:8.2g} | grad norm: {:4.2f} | max abs grad: {:7.3f} | loss: {:4.2f} | perp.: {:6.2f}'.format(\n",
    "                batch, len(seq_lens), elapsed * 1000/log_interval, scaled_lr, total_norm, max_grad, cur_loss, np.exp(cur_loss)\n",
    "            ))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return -1, total_epoch_loss[0] / num_data, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_src, save_wts = True):\n",
    "    model.eval(save_wts = save_wts)\n",
    "    total_loss = 0\n",
    "    states = model.init_states(eval_batch_size)\n",
    "    for i in range(0, data_src.size(0) - 1, seq_len):\n",
    "        # Get data\n",
    "        data, targets = get_batch(data_src, i, seq_len, evaluate = True)\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states)\n",
    "        # Evaluate\n",
    "        output, states = model(data, states)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        total_loss += loss.data * data.size(0)\n",
    "    return total_loss[0] / data_src.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/ 50) lr = 2.795e-05 (warmup)\n",
      " b 150/711 >> 2320.6 ms/b | lr:  2.8e-05 | grad norm: 2.02 | max abs grad:   0.048 | loss: 7.78 | perp.: 2396.89\n",
      " b 300/711 >> 2383.5 ms/b | lr:  2.7e-05 | grad norm: 0.54 | max abs grad:   0.011 | loss: 6.31 | perp.: 551.07\n",
      " b 450/711 >> 2336.7 ms/b | lr:  2.9e-05 | grad norm: 0.58 | max abs grad:   0.007 | loss: 6.17 | perp.: 478.14\n",
      " b 600/711 >> 2483.9 ms/b | lr:  2.5e-05 | grad norm: 0.67 | max abs grad:   0.010 | loss: 6.18 | perp.: 481.74\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1703.47 sec | train_loss:  6.53 | train_perp: 683.23 | valid_loss:  6.13 | valid_perp.: 459.13\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   2/ 50) lr = 5.59e-05 (warmup)\n",
      " b 150/717 >> 2322.6 ms/b | lr:  5.4e-05 | grad norm: 0.52 | max abs grad:   0.005 | loss: 6.21 | perp.: 495.25\n",
      " b 300/717 >> 2275.9 ms/b | lr:  6.5e-05 | grad norm: 0.52 | max abs grad:   0.008 | loss: 6.17 | perp.: 476.92\n",
      " b 450/717 >> 2357.2 ms/b | lr:  5.7e-05 | grad norm: 0.60 | max abs grad:   0.019 | loss: 6.15 | perp.: 469.70\n",
      " b 600/717 >> 2446.9 ms/b | lr:  5.4e-05 | grad norm: 0.53 | max abs grad:   0.009 | loss: 6.12 | perp.: 454.28\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1685.04 sec | train_loss:  6.14 | train_perp: 463.61 | valid_loss:  6.03 | valid_perp.: 416.90\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   3/ 50) lr = 8.385e-05 (warmup)\n",
      " b 150/720 >> 2364.5 ms/b | lr:  8.4e-05 | grad norm: 0.51 | max abs grad:   0.009 | loss: 6.12 | perp.: 453.07\n",
      " b 300/720 >> 2389.0 ms/b | lr:  8.4e-05 | grad norm: 0.56 | max abs grad:   0.015 | loss: 6.02 | perp.: 412.03\n",
      " b 450/720 >> 2491.6 ms/b | lr:  8.4e-05 | grad norm: 0.63 | max abs grad:   0.014 | loss: 6.01 | perp.: 406.78\n",
      " b 600/720 >> 2590.4 ms/b | lr:  8.6e-05 | grad norm: 0.76 | max abs grad:   0.021 | loss: 6.01 | perp.: 406.45\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1767.30 sec | train_loss:  6.02 | train_perp: 409.86 | valid_loss:  5.94 | valid_perp.: 379.64\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   4/ 50) lr = 0.0001118 (warmup)\n",
      " b 150/721 >> 2397.7 ms/b | lr:  0.00011 | grad norm: 0.69 | max abs grad:   0.018 | loss: 6.00 | perp.: 403.67\n",
      " b 300/721 >> 2484.3 ms/b | lr:  9.9e-05 | grad norm: 0.57 | max abs grad:   0.011 | loss: 5.93 | perp.: 377.72\n",
      " b 450/721 >> 2421.9 ms/b | lr:  0.00011 | grad norm: 0.53 | max abs grad:   0.011 | loss: 5.93 | perp.: 375.22\n",
      " b 600/721 >> 2398.8 ms/b | lr:  0.00011 | grad norm: 0.67 | max abs grad:   0.011 | loss: 5.89 | perp.: 362.61\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1727.16 sec | train_loss:  5.92 | train_perp: 372.49 | valid_loss:  5.84 | valid_perp.: 344.45\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   5/ 50) lr = 0.0001398 (warmup)\n",
      " b 150/716 >> 2256.9 ms/b | lr:  0.00013 | grad norm: 0.54 | max abs grad:   0.011 | loss: 5.89 | perp.: 361.10\n",
      " b 300/716 >> 2313.7 ms/b | lr:  0.00014 | grad norm: 0.50 | max abs grad:   0.014 | loss: 5.83 | perp.: 341.44\n",
      " b 450/716 >> 2234.9 ms/b | lr:  0.00015 | grad norm: 0.64 | max abs grad:   0.018 | loss: 5.83 | perp.: 339.19\n",
      " b 600/716 >> 2454.2 ms/b | lr:  0.00015 | grad norm: 0.91 | max abs grad:   0.017 | loss: 5.80 | perp.: 329.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1703.86 sec | train_loss:  5.82 | train_perp: 337.23 | valid_loss:  5.74 | valid_perp.: 309.52\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   6/ 50) lr = 0.0001677 (warmup)\n",
      " b 150/716 >> 2907.0 ms/b | lr:  0.00014 | grad norm: 0.74 | max abs grad:   0.013 | loss: 5.77 | perp.: 321.33\n",
      " b 300/716 >> 2714.9 ms/b | lr:  0.00015 | grad norm: 0.70 | max abs grad:   0.012 | loss: 5.72 | perp.: 304.96\n",
      " b 450/716 >> 2813.2 ms/b | lr:  0.00017 | grad norm: 0.56 | max abs grad:   0.009 | loss: 5.70 | perp.: 297.86\n",
      " b 600/716 >> 2658.8 ms/b | lr:  0.00013 | grad norm: 0.78 | max abs grad:   0.021 | loss: 5.67 | perp.: 289.57\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1976.11 sec | train_loss:  5.70 | train_perp: 298.03 | valid_loss:  5.61 | valid_perp.: 271.95\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   7/ 50) lr = 0.0001957 (warmup)\n",
      " b 150/719 >> 2796.4 ms/b | lr:  0.00017 | grad norm: 0.71 | max abs grad:   0.013 | loss: 5.65 | perp.: 283.15\n",
      " b 300/719 >> 2745.2 ms/b | lr:   0.0002 | grad norm: 0.85 | max abs grad:   0.017 | loss: 5.57 | perp.: 262.64\n",
      " b 450/719 >> 2731.8 ms/b | lr:   0.0002 | grad norm: 0.64 | max abs grad:   0.012 | loss: 5.53 | perp.: 251.01\n",
      " b 600/719 >> 2667.3 ms/b | lr:  0.00021 | grad norm: 0.69 | max abs grad:   0.012 | loss: 5.50 | perp.: 245.20\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1957.28 sec | train_loss:  5.55 | train_perp: 255.96 | valid_loss:  5.45 | valid_perp.: 232.30\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   8/ 50) lr = 0.0002236 (warmup)\n",
      " b 150/723 >> 2552.9 ms/b | lr:  0.00024 | grad norm: 0.82 | max abs grad:   0.016 | loss: 5.46 | perp.: 235.26\n",
      " b 300/723 >> 2252.5 ms/b | lr:  0.00025 | grad norm: 1.06 | max abs grad:   0.035 | loss: 5.42 | perp.: 224.77\n",
      " b 450/723 >> 2262.3 ms/b | lr:  0.00021 | grad norm: 0.90 | max abs grad:   0.018 | loss: 5.39 | perp.: 218.47\n",
      " b 600/723 >> 2288.1 ms/b | lr:  0.00024 | grad norm: 1.41 | max abs grad:   0.035 | loss: 5.36 | perp.: 212.57\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1673.65 sec | train_loss:  5.39 | train_perp: 219.57 | valid_loss:  5.31 | valid_perp.: 201.96\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   9/ 50) lr = 0.0002516 (warmup)\n",
      " b 150/718 >> 2324.3 ms/b | lr:  0.00025 | grad norm: 1.18 | max abs grad:   0.035 | loss: 5.32 | perp.: 204.76\n",
      " b 300/718 >> 2277.6 ms/b | lr:  0.00028 | grad norm: 0.93 | max abs grad:   0.021 | loss: 5.27 | perp.: 194.90\n",
      " b 450/718 >> 2308.2 ms/b | lr:  0.00025 | grad norm: 0.82 | max abs grad:   0.017 | loss: 5.25 | perp.: 191.27\n",
      " b 600/718 >> 2220.5 ms/b | lr:  0.00027 | grad norm: 1.42 | max abs grad:   0.030 | loss: 5.20 | perp.: 180.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1637.36 sec | train_loss:  5.25 | train_perp: 190.57 | valid_loss:  5.17 | valid_perp.: 176.34\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  10/ 50) lr = 0.0002795 (warmup)\n",
      " b 150/713 >> 2343.7 ms/b | lr:  0.00028 | grad norm: 0.90 | max abs grad:   0.023 | loss: 5.19 | perp.: 179.96\n",
      " b 300/713 >> 2283.0 ms/b | lr:  0.00026 | grad norm: 1.03 | max abs grad:   0.028 | loss: 5.13 | perp.: 168.95\n",
      " b 450/713 >> 2293.4 ms/b | lr:  0.00028 | grad norm: 1.11 | max abs grad:   0.032 | loss: 5.10 | perp.: 164.13\n",
      " b 600/713 >> 2282.3 ms/b | lr:  0.00028 | grad norm: 1.83 | max abs grad:   0.080 | loss: 5.12 | perp.: 168.07\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1639.13 sec | train_loss:  5.13 | train_perp: 168.89 | valid_loss:  5.15 | valid_perp.: 172.52\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  11/ 50) lr = 0.0002665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/722 >> 2292.8 ms/b | lr:  0.00026 | grad norm: 1.07 | max abs grad:   0.023 | loss: 5.04 | perp.: 154.31\n",
      " b 300/722 >> 2221.6 ms/b | lr:  0.00025 | grad norm: 1.21 | max abs grad:   0.033 | loss: 4.98 | perp.: 145.10\n",
      " b 450/722 >> 2253.9 ms/b | lr:  0.00024 | grad norm: 1.26 | max abs grad:   0.029 | loss: 4.97 | perp.: 143.90\n",
      " b 600/722 >> 2278.8 ms/b | lr:  0.00028 | grad norm: 2.03 | max abs grad:   0.050 | loss: 4.94 | perp.: 139.18\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1638.15 sec | train_loss:  4.98 | train_perp: 145.04 | valid_loss:  4.95 | valid_perp.: 140.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  12/ 50) lr = 0.0002552\n",
      " b 150/730 >> 2265.1 ms/b | lr:  0.00024 | grad norm: 1.01 | max abs grad:   0.032 | loss: 4.88 | perp.: 132.15\n",
      " b 300/730 >> 2246.1 ms/b | lr:  0.00026 | grad norm: 1.53 | max abs grad:   0.070 | loss: 4.89 | perp.: 132.41\n",
      " b 450/730 >> 2204.4 ms/b | lr:  0.00023 | grad norm: 1.28 | max abs grad:   0.034 | loss: 4.83 | perp.: 124.87\n",
      " b 600/730 >> 2245.8 ms/b | lr:  0.00027 | grad norm: 1.12 | max abs grad:   0.035 | loss: 4.81 | perp.: 122.12\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1635.57 sec | train_loss:  4.85 | train_perp: 127.79 | valid_loss:  4.79 | valid_perp.: 120.88\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  13/ 50) lr = 0.0002451\n",
      " b 150/719 >> 2290.3 ms/b | lr:  0.00026 | grad norm: 1.29 | max abs grad:   0.028 | loss: 4.81 | perp.: 122.57\n",
      " b 300/719 >> 2220.2 ms/b | lr:  0.00025 | grad norm: 1.35 | max abs grad:   0.055 | loss: 4.73 | perp.: 113.83\n",
      " b 450/719 >> 2309.1 ms/b | lr:   0.0002 | grad norm: 1.41 | max abs grad:   0.031 | loss: 4.73 | perp.: 113.07\n",
      " b 600/719 >> 2284.9 ms/b | lr:  0.00022 | grad norm: 1.39 | max abs grad:   0.035 | loss: 4.71 | perp.: 111.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1637.42 sec | train_loss:  4.74 | train_perp: 114.68 | valid_loss:  4.66 | valid_perp.: 106.01\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  14/ 50) lr = 0.0002362\n",
      " b 150/720 >> 2281.9 ms/b | lr:  0.00021 | grad norm: 1.70 | max abs grad:   0.064 | loss: 4.70 | perp.: 110.29\n",
      " b 300/720 >> 2266.9 ms/b | lr:  0.00026 | grad norm: 4.06 | max abs grad:   0.207 | loss: 4.65 | perp.: 104.78\n",
      " b 450/720 >> 2291.8 ms/b | lr:  0.00024 | grad norm: 1.52 | max abs grad:   0.039 | loss: 4.63 | perp.: 103.00\n",
      " b 600/720 >> 2239.5 ms/b | lr:  0.00024 | grad norm: 1.60 | max abs grad:   0.045 | loss: 4.61 | perp.: 100.89\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1638.64 sec | train_loss:  4.66 | train_perp: 105.51 | valid_loss:  4.57 | valid_perp.:  96.85\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  15/ 50) lr = 0.0002282\n",
      " b 150/719 >> 2284.8 ms/b | lr:   0.0002 | grad norm: 1.20 | max abs grad:   0.028 | loss: 4.55 | perp.:  95.09\n",
      " b 300/719 >> 2278.2 ms/b | lr:   0.0002 | grad norm: 1.58 | max abs grad:   0.064 | loss: 4.52 | perp.:  92.05\n",
      " b 450/719 >> 2309.8 ms/b | lr:  0.00023 | grad norm: 1.22 | max abs grad:   0.038 | loss: 4.55 | perp.:  94.50\n",
      " b 600/719 >> 2237.3 ms/b | lr:  0.00023 | grad norm: 1.89 | max abs grad:   0.059 | loss: 4.50 | perp.:  89.81\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1642.31 sec | train_loss:  4.54 | train_perp:  93.55 | valid_loss:  4.52 | valid_perp.:  91.84\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  16/ 50) lr = 0.000221\n",
      " b 150/722 >> 2289.6 ms/b | lr:  0.00023 | grad norm: 1.79 | max abs grad:   0.065 | loss: 4.50 | perp.:  90.06\n",
      " b 300/722 >> 2228.2 ms/b | lr:  0.00019 | grad norm: 1.39 | max abs grad:   0.036 | loss: 4.39 | perp.:  80.86\n",
      " b 450/722 >> 2274.4 ms/b | lr:  0.00022 | grad norm: 1.91 | max abs grad:   0.080 | loss: 4.45 | perp.:  85.68\n",
      " b 600/722 >> 2281.9 ms/b | lr:  0.00021 | grad norm: 2.19 | max abs grad:   0.109 | loss: 4.42 | perp.:  83.30\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1638.60 sec | train_loss:  4.44 | train_perp:  84.86 | valid_loss:  4.37 | valid_perp.:  79.34\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  17/ 50) lr = 0.0002144\n",
      " b 150/726 >> 2180.2 ms/b | lr:   0.0002 | grad norm: 1.42 | max abs grad:   0.047 | loss: 4.33 | perp.:  76.00\n",
      " b 300/726 >> 2277.9 ms/b | lr:  0.00019 | grad norm: 2.02 | max abs grad:   0.071 | loss: 4.37 | perp.:  79.38\n",
      " b 450/726 >> 2298.1 ms/b | lr:  0.00024 | grad norm: 1.53 | max abs grad:   0.049 | loss: 4.32 | perp.:  74.94\n",
      " b 600/726 >> 2258.4 ms/b | lr:  0.00021 | grad norm: 1.67 | max abs grad:   0.043 | loss: 4.28 | perp.:  72.42\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1638.86 sec | train_loss:  4.34 | train_perp:  76.81 | valid_loss:  4.38 | valid_perp.:  79.79\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  18/ 50) lr = 0.0002083\n",
      " b 150/716 >> 2294.6 ms/b | lr:   0.0002 | grad norm: 1.46 | max abs grad:   0.051 | loss: 4.34 | perp.:  76.81\n",
      " b 300/716 >> 2292.3 ms/b | lr:  0.00021 | grad norm: 3.58 | max abs grad:   0.191 | loss: 4.22 | perp.:  68.01\n",
      " b 450/716 >> 2337.6 ms/b | lr:  0.00022 | grad norm: 2.20 | max abs grad:   0.092 | loss: 4.27 | perp.:  71.42\n",
      " b 600/716 >> 2327.7 ms/b | lr:  0.00018 | grad norm: 1.75 | max abs grad:   0.059 | loss: 4.22 | perp.:  67.87\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1643.26 sec | train_loss:  4.26 | train_perp:  70.61 | valid_loss:  4.96 | valid_perp.: 143.21\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  19/ 50) lr = 0.0002028\n",
      " b 150/719 >> 2295.7 ms/b | lr:   0.0002 | grad norm: 1.84 | max abs grad:   0.051 | loss: 4.23 | perp.:  68.88\n",
      " b 300/719 >> 2258.9 ms/b | lr:   0.0002 | grad norm: 2.70 | max abs grad:   0.114 | loss: 4.14 | perp.:  62.72\n",
      " b 450/719 >> 2338.2 ms/b | lr:  0.00021 | grad norm: 2.15 | max abs grad:   0.052 | loss: 4.14 | perp.:  62.96\n",
      " b 600/719 >> 2282.2 ms/b | lr:   0.0002 | grad norm: 2.53 | max abs grad:   0.125 | loss: 4.17 | perp.:  64.68\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1643.38 sec | train_loss:  4.18 | train_perp:  65.56 | valid_loss:  4.19 | valid_perp.:  65.72\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  20/ 50) lr = 0.0001976\n",
      " b 150/720 >> 2295.5 ms/b | lr:  0.00019 | grad norm: 2.30 | max abs grad:   0.068 | loss: 4.19 | perp.:  66.17\n",
      " b 300/720 >> 2288.8 ms/b | lr:  0.00019 | grad norm: 1.64 | max abs grad:   0.049 | loss: 4.09 | perp.:  59.71\n",
      " b 450/720 >> 2302.2 ms/b | lr:  0.00022 | grad norm: 2.40 | max abs grad:   0.067 | loss: 4.04 | perp.:  56.99\n",
      " b 600/720 >> 2275.7 ms/b | lr:  0.00017 | grad norm: 2.16 | max abs grad:   0.079 | loss: 4.12 | perp.:  61.46\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1646.20 sec | train_loss:  4.11 | train_perp:  61.03 | valid_loss:  4.56 | valid_perp.:  95.86\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  21/ 50) lr = 0.0001929\n",
      " b 150/715 >> 2318.9 ms/b | lr:  0.00019 | grad norm: 1.69 | max abs grad:   0.046 | loss: 4.04 | perp.:  57.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/715 >> 2305.0 ms/b | lr:  0.00017 | grad norm: 2.95 | max abs grad:   0.122 | loss: 4.02 | perp.:  55.57\n",
      " b 450/715 >> 2314.2 ms/b | lr:  0.00017 | grad norm: 2.26 | max abs grad:   0.100 | loss: 3.98 | perp.:  53.63\n",
      " b 600/715 >> 2265.2 ms/b | lr:  0.00019 | grad norm: 1.83 | max abs grad:   0.069 | loss: 3.95 | perp.:  51.96\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1648.64 sec | train_loss:  4.03 | train_perp:  56.14 | valid_loss:  4.30 | valid_perp.:  73.73\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  22/ 50) lr = 0.0001884\n",
      " b 150/716 >> 2315.4 ms/b | lr:  0.00017 | grad norm: 2.33 | max abs grad:   0.216 | loss: 4.00 | perp.:  54.51\n",
      " b 300/716 >> 2286.4 ms/b | lr:  0.00019 | grad norm: 2.29 | max abs grad:   0.109 | loss: 3.97 | perp.:  52.93\n",
      " b 450/716 >> 2276.8 ms/b | lr:   0.0002 | grad norm: 3.67 | max abs grad:   0.100 | loss: 3.95 | perp.:  51.90\n",
      " b 600/716 >> 2352.9 ms/b | lr:  0.00017 | grad norm: 2.25 | max abs grad:   0.105 | loss: 3.95 | perp.:  52.00\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1666.93 sec | train_loss:  3.98 | train_perp:  53.54 | valid_loss:  3.96 | valid_perp.:  52.33\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  23/ 50) lr = 0.0001843\n",
      " b 150/718 >> 2609.7 ms/b | lr:  0.00017 | grad norm: 4.41 | max abs grad:   0.212 | loss: 3.95 | perp.:  51.87\n",
      " b 300/718 >> 2433.4 ms/b | lr:  0.00018 | grad norm: 1.93 | max abs grad:   0.057 | loss: 3.88 | perp.:  48.19\n",
      " b 450/718 >> 2381.7 ms/b | lr:  0.00017 | grad norm: 1.96 | max abs grad:   0.074 | loss: 3.90 | perp.:  49.39\n",
      " b 600/718 >> 2353.2 ms/b | lr:  0.00018 | grad norm: 1.88 | max abs grad:   0.067 | loss: 3.85 | perp.:  47.10\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1734.52 sec | train_loss:  3.90 | train_perp:  49.18 | valid_loss:  3.95 | valid_perp.:  51.69\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  24/ 50) lr = 0.0001804\n",
      " b 150/713 >> 2360.6 ms/b | lr:   0.0002 | grad norm: 6.10 | max abs grad:   0.447 | loss: 4.04 | perp.:  57.11\n",
      " b 300/713 >> 2430.1 ms/b | lr:  0.00018 | grad norm: 2.05 | max abs grad:   0.086 | loss: 3.76 | perp.:  42.94\n",
      " b 450/713 >> 2370.5 ms/b | lr:  0.00018 | grad norm: 2.71 | max abs grad:   0.140 | loss: 3.76 | perp.:  42.95\n",
      " b 600/713 >> 2361.4 ms/b | lr:  0.00016 | grad norm: 2.32 | max abs grad:   0.081 | loss: 3.93 | perp.:  50.94\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1699.17 sec | train_loss:  3.88 | train_perp:  48.61 | valid_loss:  3.75 | valid_perp.:  42.64\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  25/ 50) lr = 0.0001768\n",
      " b 150/722 >> 2316.3 ms/b | lr:  0.00017 | grad norm: 1.71 | max abs grad:   0.042 | loss: 3.83 | perp.:  46.26\n",
      " b 300/722 >> 2221.6 ms/b | lr:  0.00018 | grad norm: 1.87 | max abs grad:   0.046 | loss: 3.76 | perp.:  42.80\n",
      " b 450/722 >> 2281.7 ms/b | lr:   0.0002 | grad norm: 2.09 | max abs grad:   0.053 | loss: 3.77 | perp.:  43.50\n",
      " b 600/722 >> 2239.0 ms/b | lr:  0.00018 | grad norm: 5.00 | max abs grad:   0.172 | loss: 3.72 | perp.:  41.38\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1633.31 sec | train_loss:  3.79 | train_perp:  44.35 | valid_loss:  3.70 | valid_perp.:  40.62\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  26/ 50) lr = 0.0001733\n",
      " b 150/720 >> 2271.4 ms/b | lr:  0.00016 | grad norm: 2.27 | max abs grad:   0.077 | loss: 3.70 | perp.:  40.62\n",
      " b 300/720 >> 2255.7 ms/b | lr:  0.00016 | grad norm: 2.15 | max abs grad:   0.137 | loss: 3.92 | perp.:  50.31\n",
      " b 450/720 >> 2301.1 ms/b | lr:  0.00016 | grad norm: 2.42 | max abs grad:   0.130 | loss: 3.80 | perp.:  44.83\n",
      " b 600/720 >> 2303.4 ms/b | lr:  0.00016 | grad norm: 2.11 | max abs grad:   0.086 | loss: 3.66 | perp.:  38.91\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1641.62 sec | train_loss:  3.79 | train_perp:  44.16 | valid_loss:  3.75 | valid_perp.:  42.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  27/ 50) lr = 0.0001701\n",
      " b 150/720 >> 2299.6 ms/b | lr:  0.00017 | grad norm: 2.44 | max abs grad:   0.093 | loss: 3.83 | perp.:  46.11\n",
      " b 300/720 >> 2243.9 ms/b | lr:  0.00016 | grad norm: 2.47 | max abs grad:   0.124 | loss: 3.68 | perp.:  39.72\n",
      " b 450/720 >> 2315.3 ms/b | lr:  0.00017 | grad norm: 3.58 | max abs grad:   0.139 | loss: 3.61 | perp.:  37.13\n",
      " b 600/720 >> 2286.8 ms/b | lr:  0.00017 | grad norm: 3.43 | max abs grad:   0.092 | loss: 3.60 | perp.:  36.77\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1642.21 sec | train_loss:  3.69 | train_perp:  40.07 | valid_loss:  3.62 | valid_perp.:  37.42\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  28/ 50) lr = 0.000167\n",
      " b 150/706 >> 2376.1 ms/b | lr:  0.00015 | grad norm: 2.10 | max abs grad:   0.103 | loss: 3.79 | perp.:  44.22\n",
      " b 300/706 >> 2346.7 ms/b | lr:  0.00016 | grad norm: 2.25 | max abs grad:   0.090 | loss: 3.63 | perp.:  37.65\n",
      " b 450/706 >> 2377.8 ms/b | lr:  0.00016 | grad norm: 2.80 | max abs grad:   0.222 | loss: 3.69 | perp.:  39.95\n",
      " b 600/706 >> 2369.9 ms/b | lr:  0.00016 | grad norm: 2.20 | max abs grad:   0.069 | loss: 3.53 | perp.:  33.99\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1673.31 sec | train_loss:  3.67 | train_perp:  39.37 | valid_loss:  3.53 | valid_perp.:  34.23\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  29/ 50) lr = 0.0001641\n",
      " b 150/717 >> 2368.7 ms/b | lr:  0.00016 | grad norm: 2.24 | max abs grad:   0.124 | loss: 3.59 | perp.:  36.07\n",
      " b 300/717 >> 2289.1 ms/b | lr:  0.00015 | grad norm: 2.42 | max abs grad:   0.090 | loss: 3.54 | perp.:  34.51\n",
      " b 450/717 >> 2343.8 ms/b | lr:  0.00016 | grad norm: 3.65 | max abs grad:   0.191 | loss: 3.51 | perp.:  33.58\n",
      " b 600/717 >> 2284.5 ms/b | lr:  0.00018 | grad norm: 2.18 | max abs grad:   0.054 | loss: 3.52 | perp.:  33.67\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1669.20 sec | train_loss:  3.54 | train_perp:  34.58 | valid_loss:  3.46 | valid_perp.:  31.79\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  30/ 50) lr = 0.0001614\n",
      " b 150/722 >> 2353.5 ms/b | lr:  0.00015 | grad norm: 2.38 | max abs grad:   0.080 | loss: 3.50 | perp.:  32.97\n",
      " b 300/722 >> 2282.1 ms/b | lr:  0.00016 | grad norm: 4.82 | max abs grad:   0.323 | loss: 3.44 | perp.:  31.05\n",
      " b 450/722 >> 2329.5 ms/b | lr:  0.00014 | grad norm: 2.45 | max abs grad:   0.146 | loss: 3.64 | perp.:  37.95\n",
      " b 600/722 >> 2263.2 ms/b | lr:  0.00017 | grad norm: 4.08 | max abs grad:   0.148 | loss: 3.51 | perp.:  33.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1667.31 sec | train_loss:  3.54 | train_perp:  34.52 | valid_loss:  3.42 | valid_perp.:  30.51\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  31/ 50) lr = 0.0001588\n",
      " b 150/720 >> 2248.0 ms/b | lr:  0.00016 | grad norm: 4.55 | max abs grad:   0.241 | loss: 3.47 | perp.:  32.15\n",
      " b 300/720 >> 2314.5 ms/b | lr:  0.00017 | grad norm: 2.44 | max abs grad:   0.061 | loss: 3.46 | perp.:  31.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 450/720 >> 2324.9 ms/b | lr:  0.00016 | grad norm: 2.33 | max abs grad:   0.094 | loss: 3.46 | perp.:  31.77\n",
      " b 600/720 >> 2304.3 ms/b | lr:  0.00017 | grad norm: 6.07 | max abs grad:   0.225 | loss: 3.48 | perp.:  32.50\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1644.21 sec | train_loss:  3.49 | train_perp:  32.62 | valid_loss:  5.91 | valid_perp.: 367.35\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  32/ 50) lr = 0.0001563\n",
      " b 150/717 >> 2315.8 ms/b | lr:  0.00013 | grad norm: 3.08 | max abs grad:   0.206 | loss: 3.74 | perp.:  42.09\n",
      " b 300/717 >> 2303.0 ms/b | lr:  0.00015 | grad norm: 3.35 | max abs grad:   0.146 | loss: 3.36 | perp.:  28.85\n",
      " b 450/717 >> 2201.2 ms/b | lr:  0.00017 | grad norm: 2.33 | max abs grad:   0.073 | loss: 3.36 | perp.:  28.82\n",
      " b 600/717 >> 2319.9 ms/b | lr:  0.00014 | grad norm: 1.98 | max abs grad:   0.090 | loss: 3.39 | perp.:  29.55\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1639.61 sec | train_loss:  3.49 | train_perp:  32.72 | valid_loss:  3.92 | valid_perp.:  50.32\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  33/ 50) lr = 0.0001539\n",
      " b 150/713 >> 2298.2 ms/b | lr:  0.00017 | grad norm: 2.49 | max abs grad:   0.105 | loss: 3.40 | perp.:  30.09\n",
      " b 300/713 >> 2336.9 ms/b | lr:  0.00013 | grad norm: 3.05 | max abs grad:   0.139 | loss: 3.41 | perp.:  30.28\n",
      " b 450/713 >> 2294.6 ms/b | lr:  0.00014 | grad norm: 2.27 | max abs grad:   0.109 | loss: 3.50 | perp.:  33.16\n",
      " b 600/713 >> 2357.1 ms/b | lr:  0.00016 | grad norm: 1.94 | max abs grad:   0.071 | loss: 3.39 | perp.:  29.76\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1647.93 sec | train_loss:  3.44 | train_perp:  31.11 | valid_loss:  3.49 | valid_perp.:  32.80\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  34/ 50) lr = 0.0001516\n",
      " b 150/717 >> 2312.7 ms/b | lr:  0.00015 | grad norm: 2.34 | max abs grad:   0.085 | loss: 3.42 | perp.:  30.57\n",
      " b 300/717 >> 2311.0 ms/b | lr:  0.00016 | grad norm: 8.59 | max abs grad:   0.373 | loss: 3.37 | perp.:  28.95\n",
      " b 450/717 >> 2387.6 ms/b | lr:  0.00016 | grad norm: 8.60 | max abs grad:   0.290 | loss: 3.31 | perp.:  27.27\n",
      " b 600/717 >> 2358.1 ms/b | lr:  0.00016 | grad norm: 4.54 | max abs grad:   0.155 | loss: 3.37 | perp.:  29.20\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1679.17 sec | train_loss:  3.39 | train_perp:  29.67 | valid_loss:  3.27 | valid_perp.:  26.33\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  35/ 50) lr = 0.0001494\n",
      " b 150/718 >> 2292.3 ms/b | lr:  0.00015 | grad norm: 3.06 | max abs grad:   0.085 | loss: 3.25 | perp.:  25.71\n",
      " b 300/718 >> 2284.9 ms/b | lr:  0.00015 | grad norm: 4.67 | max abs grad:   0.310 | loss: 3.35 | perp.:  28.51\n",
      " b 450/718 >> 2349.2 ms/b | lr:  0.00014 | grad norm: 3.20 | max abs grad:   0.254 | loss: 3.32 | perp.:  27.71\n",
      " b 600/718 >> 2229.3 ms/b | lr:  0.00015 | grad norm: 8.57 | max abs grad:   0.311 | loss: 3.24 | perp.:  25.43\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1648.09 sec | train_loss:  3.30 | train_perp:  27.22 | valid_loss:  3.55 | valid_perp.:  34.92\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  36/ 50) lr = 0.0001473\n",
      " b 150/716 >> 2304.5 ms/b | lr:  0.00016 | grad norm: 3.75 | max abs grad:   0.158 | loss: 3.26 | perp.:  25.98\n",
      " b 300/716 >> 2327.4 ms/b | lr:  0.00017 | grad norm: 2.37 | max abs grad:   0.070 | loss: 3.21 | perp.:  24.88\n",
      " b 450/716 >> 2370.8 ms/b | lr:  0.00015 | grad norm: 3.64 | max abs grad:   0.221 | loss: 3.27 | perp.:  26.32\n",
      " b 600/716 >> 2228.7 ms/b | lr:  0.00015 | grad norm: 2.26 | max abs grad:   0.063 | loss: 3.20 | perp.:  24.43\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1652.87 sec | train_loss:  3.25 | train_perp:  25.86 | valid_loss:  3.22 | valid_perp.:  25.10\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  37/ 50) lr = 0.0001453\n",
      " b 150/712 >> 2317.6 ms/b | lr:  0.00012 | grad norm: 2.78 | max abs grad:   0.155 | loss: 3.47 | perp.:  32.22\n",
      " b 300/712 >> 2305.7 ms/b | lr:  0.00015 | grad norm: 2.89 | max abs grad:   0.102 | loss: 3.26 | perp.:  26.05\n",
      " b 450/712 >> 2285.7 ms/b | lr:  0.00014 | grad norm: 2.43 | max abs grad:   0.103 | loss: 3.19 | perp.:  24.20\n",
      " b 600/712 >> 2333.0 ms/b | lr:  0.00015 | grad norm: 3.20 | max abs grad:   0.115 | loss: 3.19 | perp.:  24.34\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1649.00 sec | train_loss:  3.30 | train_perp:  27.12 | valid_loss:  3.17 | valid_perp.:  23.69\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  38/ 50) lr = 0.0001434\n",
      " b 150/722 >> 2271.6 ms/b | lr:  0.00015 | grad norm: 3.22 | max abs grad:   0.133 | loss: 3.11 | perp.:  22.33\n",
      " b 300/722 >> 2358.5 ms/b | lr:  0.00014 | grad norm: 7.69 | max abs grad:   0.532 | loss: 3.24 | perp.:  25.53\n",
      " b 450/722 >> 2423.3 ms/b | lr:  0.00015 | grad norm: 2.70 | max abs grad:   0.098 | loss: 3.16 | perp.:  23.58\n",
      " b 600/722 >> 2525.5 ms/b | lr:  0.00013 | grad norm: 2.66 | max abs grad:   0.153 | loss: 3.09 | perp.:  21.88\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1732.13 sec | train_loss:  3.17 | train_perp:  23.87 | valid_loss:  3.13 | valid_perp.:  22.77\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  39/ 50) lr = 0.0001415\n",
      " b 150/715 >> 2600.2 ms/b | lr:  0.00012 | grad norm: 1.98 | max abs grad:   0.054 | loss: 3.27 | perp.:  26.39\n",
      " b 300/715 >> 2528.6 ms/b | lr:  0.00012 | grad norm: 5.22 | max abs grad:   0.291 | loss: 3.06 | perp.:  21.39\n",
      " b 450/715 >> 2529.5 ms/b | lr:  0.00014 | grad norm: 3.13 | max abs grad:   0.116 | loss: 3.15 | perp.:  23.44\n",
      " b 600/715 >> 2498.0 ms/b | lr:  0.00014 | grad norm: 3.01 | max abs grad:   0.084 | loss: 3.19 | perp.:  24.25\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1807.35 sec | train_loss:  3.22 | train_perp:  24.92 | valid_loss:  3.06 | valid_perp.:  21.27\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  40/ 50) lr = 0.0001398\n",
      " b 150/720 >> 2337.2 ms/b | lr:  0.00014 | grad norm: 4.66 | max abs grad:   0.237 | loss: 3.17 | perp.:  23.80\n",
      " b 300/720 >> 2520.1 ms/b | lr:  0.00015 | grad norm: 2.15 | max abs grad:   0.069 | loss: 3.10 | perp.:  22.21\n",
      " b 450/720 >> 2481.9 ms/b | lr:  0.00016 | grad norm: 5.34 | max abs grad:   0.215 | loss: 3.08 | perp.:  21.73\n",
      " b 600/720 >> 2328.9 ms/b | lr:  0.00013 | grad norm: 5.88 | max abs grad:   0.363 | loss: 3.19 | perp.:  24.38\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1750.93 sec | train_loss:  3.14 | train_perp:  23.18 | valid_loss:  3.05 | valid_perp.:  21.12\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  41/ 50) lr = 0.000138\n",
      " b 150/722 >> 2477.8 ms/b | lr:  0.00013 | grad norm: 5.63 | max abs grad:   0.276 | loss: 3.05 | perp.:  21.07\n",
      " b 300/722 >> 2370.1 ms/b | lr:  0.00013 | grad norm: 3.99 | max abs grad:   0.192 | loss: 3.01 | perp.:  20.30\n",
      " b 450/722 >> 2287.3 ms/b | lr:  0.00012 | grad norm: 5.26 | max abs grad:   0.354 | loss: 3.05 | perp.:  21.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 600/722 >> 2274.5 ms/b | lr:  0.00012 | grad norm: 2.28 | max abs grad:   0.085 | loss: 3.06 | perp.:  21.40\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1685.76 sec | train_loss:  3.09 | train_perp:  21.99 | valid_loss:  2.98 | valid_perp.:  19.77\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  42/ 50) lr = 0.0001364\n",
      " b 150/717 >> 2302.7 ms/b | lr:  0.00014 | grad norm: 7.43 | max abs grad:   0.217 | loss: 3.13 | perp.:  22.77\n",
      " b 300/717 >> 2489.5 ms/b | lr:  0.00015 | grad norm: 5.84 | max abs grad:   0.201 | loss: 3.01 | perp.:  20.27\n",
      " b 450/717 >> 2567.6 ms/b | lr:  0.00012 | grad norm: 2.46 | max abs grad:   0.122 | loss: 2.99 | perp.:  19.92\n",
      " b 600/717 >> 2405.8 ms/b | lr:  0.00013 | grad norm: 2.63 | max abs grad:   0.079 | loss: 3.04 | perp.:  20.84\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1745.13 sec | train_loss:  3.07 | train_perp:  21.51 | valid_loss:  2.98 | valid_perp.:  19.74\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  43/ 50) lr = 0.0001348\n",
      " b 150/713 >> 2411.0 ms/b | lr:  0.00013 | grad norm: 3.74 | max abs grad:   0.188 | loss: 3.23 | perp.:  25.39\n",
      " b 300/713 >> 2237.1 ms/b | lr:  0.00011 | grad norm: 2.51 | max abs grad:   0.108 | loss: 2.92 | perp.:  18.45\n",
      " b 450/713 >> 2272.8 ms/b | lr:  0.00011 | grad norm: 5.63 | max abs grad:   0.344 | loss: 3.05 | perp.:  21.08\n",
      " b 600/713 >> 2311.0 ms/b | lr:  0.00014 | grad norm: 2.28 | max abs grad:   0.065 | loss: 3.47 | perp.:  32.10\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1642.10 sec | train_loss:  3.18 | train_perp:  23.99 | valid_loss:  2.95 | valid_perp.:  19.09\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  44/ 50) lr = 0.0001333\n",
      " b 150/713 >> 2293.3 ms/b | lr:  0.00013 | grad norm: 2.57 | max abs grad:   0.077 | loss: 3.13 | perp.:  22.99\n",
      " b 300/713 >> 2291.9 ms/b | lr:  0.00011 | grad norm: 5.27 | max abs grad:   0.377 | loss: 2.93 | perp.:  18.79\n",
      " b 450/713 >> 2369.2 ms/b | lr:  0.00014 | grad norm: 3.13 | max abs grad:   0.079 | loss: 3.13 | perp.:  22.95\n",
      " b 600/713 >> 2376.4 ms/b | lr:  0.00014 | grad norm: 2.65 | max abs grad:   0.102 | loss: 3.04 | perp.:  20.87\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1660.89 sec | train_loss:  3.08 | train_perp:  21.75 | valid_loss:  2.93 | valid_perp.:  18.80\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  45/ 50) lr = 0.0001318\n",
      " b 150/714 >> 2305.7 ms/b | lr:  0.00013 | grad norm: 2.98 | max abs grad:   0.165 | loss: 3.08 | perp.:  21.67\n",
      " b 300/714 >> 2272.7 ms/b | lr:  0.00012 | grad norm: 2.94 | max abs grad:   0.114 | loss: 2.90 | perp.:  18.13\n",
      " b 450/714 >> 2295.8 ms/b | lr:  0.00013 | grad norm: 2.80 | max abs grad:   0.151 | loss: 2.97 | perp.:  19.50\n",
      " b 600/714 >> 2293.2 ms/b | lr:  0.00013 | grad norm: 3.19 | max abs grad:   0.092 | loss: 2.94 | perp.:  19.00\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1637.71 sec | train_loss:  3.01 | train_perp:  20.20 | valid_loss:  3.24 | valid_perp.:  25.66\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  46/ 50) lr = 0.0001303\n",
      " b 150/716 >> 2342.6 ms/b | lr:  0.00013 | grad norm: 8.13 | max abs grad:   0.384 | loss: 3.04 | perp.:  20.98\n",
      " b 300/716 >> 2266.6 ms/b | lr:  0.00015 | grad norm: 5.52 | max abs grad:   0.140 | loss: 2.88 | perp.:  17.86\n",
      " b 450/716 >> 2329.9 ms/b | lr:  0.00014 | grad norm: 5.18 | max abs grad:   0.176 | loss: 2.93 | perp.:  18.70\n",
      " b 600/716 >> 2282.4 ms/b | lr:  0.00014 | grad norm: 2.57 | max abs grad:   0.078 | loss: 2.93 | perp.:  18.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1642.55 sec | train_loss:  2.96 | train_perp:  19.23 | valid_loss:  2.89 | valid_perp.:  18.08\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  47/ 50) lr = 0.0001289\n",
      " b 150/714 >> 2368.0 ms/b | lr:  0.00014 | grad norm: 3.96 | max abs grad:   0.130 | loss: 3.02 | perp.:  20.47\n",
      " b 300/714 >> 2321.3 ms/b | lr:  0.00011 | grad norm: 3.57 | max abs grad:   0.187 | loss: 2.99 | perp.:  19.91\n",
      " b 450/714 >> 2347.5 ms/b | lr:  0.00013 | grad norm: 2.28 | max abs grad:   0.070 | loss: 2.99 | perp.:  19.81\n",
      " b 600/714 >> 2211.8 ms/b | lr:  0.00014 | grad norm: 5.06 | max abs grad:   0.313 | loss: 2.80 | perp.:  16.51\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1644.85 sec | train_loss:  2.96 | train_perp:  19.28 | valid_loss:  2.85 | valid_perp.:  17.22\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  48/ 50) lr = 0.0001276\n",
      " b 150/727 >> 2276.8 ms/b | lr:  0.00013 | grad norm: 4.68 | max abs grad:   0.191 | loss: 2.90 | perp.:  18.22\n",
      " b 300/727 >> 2208.1 ms/b | lr:  0.00012 | grad norm: 3.83 | max abs grad:   0.130 | loss: 2.84 | perp.:  17.05\n",
      " b 450/727 >> 2300.9 ms/b | lr:  0.00012 | grad norm: 3.22 | max abs grad:   0.244 | loss: 2.87 | perp.:  17.59\n",
      " b 600/727 >> 2248.3 ms/b | lr:  0.00013 | grad norm: 4.20 | max abs grad:   0.137 | loss: 2.81 | perp.:  16.56\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1641.73 sec | train_loss:  2.87 | train_perp:  17.72 | valid_loss:  4.60 | valid_perp.:  99.19\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  49/ 50) lr = 0.0001263\n",
      " b 150/717 >> 2286.6 ms/b | lr:  0.00014 | grad norm: 9.18 | max abs grad:   0.340 | loss: 2.93 | perp.:  18.69\n",
      " b 300/717 >> 2298.1 ms/b | lr:  0.00013 | grad norm: 3.65 | max abs grad:   0.145 | loss: 2.87 | perp.:  17.69\n",
      " b 450/717 >> 2225.4 ms/b | lr:  0.00014 | grad norm: 6.73 | max abs grad:   0.284 | loss: 2.85 | perp.:  17.25\n",
      " b 600/717 >> 2375.2 ms/b | lr:  0.00013 | grad norm: 3.22 | max abs grad:   0.119 | loss: 2.85 | perp.:  17.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1642.89 sec | train_loss:  2.89 | train_perp:  17.97 | valid_loss:  2.80 | valid_perp.:  16.43\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  50/ 50) lr = 0.000125\n",
      " b 150/719 >> 2296.7 ms/b | lr:  0.00012 | grad norm: 6.23 | max abs grad:   0.366 | loss: 2.88 | perp.:  17.77\n",
      " b 300/719 >> 2292.2 ms/b | lr:  0.00012 | grad norm: 3.25 | max abs grad:   0.110 | loss: 2.85 | perp.:  17.36\n",
      " b 450/719 >> 2258.5 ms/b | lr:  0.00012 | grad norm: 2.32 | max abs grad:   0.076 | loss: 2.81 | perp.:  16.67\n",
      " b 600/719 >> 2299.2 ms/b | lr:  0.00012 | grad norm: 3.09 | max abs grad:   0.175 | loss: 2.85 | perp.:  17.36\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1641.21 sec | train_loss:  2.92 | train_perp:  18.61 | valid_loss:  2.92 | valid_perp.:  18.47\n",
      "================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data, save_wts = False)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  51/100) lr = 0.0001238\n",
      " b 150/724 >> 3601.0 ms/b | lr:  0.00013 | grad norm: 5.01 | max abs grad:   0.124 | loss: 2.77 | perp.:  15.99\n",
      " b 300/724 >> 3016.5 ms/b | lr:  0.00011 | grad norm: 3.78 | max abs grad:   0.274 | loss: 2.77 | perp.:  15.94\n",
      " b 450/724 >> 2900.0 ms/b | lr:  0.00013 | grad norm: 4.25 | max abs grad:   0.118 | loss: 2.73 | perp.:  15.39\n",
      " b 600/724 >> 2954.8 ms/b | lr:  0.00012 | grad norm: 3.35 | max abs grad:   0.086 | loss: 2.75 | perp.:  15.70\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2237.45 sec | train_loss:  2.78 | train_perp:  16.19 | valid_loss:  3.13 | valid_perp.:  22.79\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  52/100) lr = 0.0001226\n",
      " b 150/732 >> 2837.8 ms/b | lr:  0.00012 | grad norm: 3.22 | max abs grad:   0.157 | loss: 2.68 | perp.:  14.64\n",
      " b 300/732 >> 2853.8 ms/b | lr:   0.0001 | grad norm: 2.83 | max abs grad:   0.128 | loss: 2.73 | perp.:  15.38\n",
      " b 450/732 >> 2863.8 ms/b | lr:  0.00012 | grad norm: 3.34 | max abs grad:   0.116 | loss: 2.79 | perp.:  16.31\n",
      " b 600/732 >> 2945.0 ms/b | lr:  0.00012 | grad norm: 3.24 | max abs grad:   0.108 | loss: 2.86 | perp.:  17.42\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2106.59 sec | train_loss:  2.80 | train_perp:  16.38 | valid_loss:  4.76 | valid_perp.: 117.18\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  53/100) lr = 0.0001214\n",
      " b 150/707 >> 3055.2 ms/b | lr:  0.00012 | grad norm: 2.70 | max abs grad:   0.100 | loss: 2.84 | perp.:  17.13\n",
      " b 300/707 >> 3086.9 ms/b | lr:  0.00011 | grad norm: 2.96 | max abs grad:   0.096 | loss: 2.85 | perp.:  17.31\n",
      " b 450/707 >> 3056.5 ms/b | lr:  0.00011 | grad norm: 3.88 | max abs grad:   0.190 | loss: 2.75 | perp.:  15.63\n",
      " b 600/707 >> 2973.3 ms/b | lr:  0.00011 | grad norm: 3.23 | max abs grad:   0.226 | loss: 2.79 | perp.:  16.23\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2142.72 sec | train_loss:  2.84 | train_perp:  17.18 | valid_loss:  2.75 | valid_perp.:  15.58\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  54/100) lr = 0.0001203\n",
      " b 150/716 >> 2943.9 ms/b | lr:  0.00011 | grad norm: 4.07 | max abs grad:   0.190 | loss: 2.82 | perp.:  16.80\n",
      " b 300/716 >> 2967.9 ms/b | lr:  0.00012 | grad norm: 5.12 | max abs grad:   0.173 | loss: 2.80 | perp.:  16.37\n",
      " b 450/716 >> 2939.2 ms/b | lr:  0.00011 | grad norm: 4.59 | max abs grad:   0.269 | loss: 2.74 | perp.:  15.53\n",
      " b 600/716 >> 2937.9 ms/b | lr:  0.00013 | grad norm: 6.87 | max abs grad:   0.203 | loss: 2.75 | perp.:  15.64\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2109.79 sec | train_loss:  2.80 | train_perp:  16.38 | valid_loss:  2.64 | valid_perp.:  14.04\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  55/100) lr = 0.0001192\n",
      " b 150/725 >> 2902.5 ms/b | lr:  0.00012 | grad norm: 7.38 | max abs grad:   0.248 | loss: 2.75 | perp.:  15.63\n",
      " b 300/725 >> 2936.1 ms/b | lr:  0.00012 | grad norm: 3.96 | max abs grad:   0.163 | loss: 2.76 | perp.:  15.77\n",
      " b 450/725 >> 2867.5 ms/b | lr:  0.00013 | grad norm: 2.48 | max abs grad:   0.083 | loss: 2.70 | perp.:  14.81\n",
      " b 600/725 >> 3133.1 ms/b | lr:  0.00011 | grad norm: 4.70 | max abs grad:   0.481 | loss: 2.78 | perp.:  16.11\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2166.43 sec | train_loss:  2.77 | train_perp:  15.88 | valid_loss:  2.72 | valid_perp.:  15.24\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  56/100) lr = 0.0001181\n",
      " b 150/713 >> 3016.0 ms/b | lr:  9.2e-05 | grad norm: 5.21 | max abs grad:   0.322 | loss: 2.78 | perp.:  16.10\n",
      " b 300/713 >> 2978.1 ms/b | lr:  0.00011 | grad norm: 4.68 | max abs grad:   0.287 | loss: 2.76 | perp.:  15.82\n",
      " b 450/713 >> 2990.9 ms/b | lr:  0.00012 | grad norm: 4.58 | max abs grad:   0.157 | loss: 2.73 | perp.:  15.26\n",
      " b 600/713 >> 3009.9 ms/b | lr:  0.00011 | grad norm: 3.83 | max abs grad:   0.357 | loss: 2.65 | perp.:  14.08\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2177.68 sec | train_loss:  2.76 | train_perp:  15.73 | valid_loss:  2.63 | valid_perp.:  13.85\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  57/100) lr = 0.0001171\n",
      " b 150/721 >> 3132.3 ms/b | lr:  0.00012 | grad norm: 2.85 | max abs grad:   0.095 | loss: 2.62 | perp.:  13.78\n",
      " b 300/721 >> 3116.6 ms/b | lr:  0.00013 | grad norm: 7.18 | max abs grad:   0.378 | loss: 2.77 | perp.:  15.91\n",
      " b 450/721 >> 3030.1 ms/b | lr:  0.00012 | grad norm: 3.36 | max abs grad:   0.117 | loss: 2.78 | perp.:  16.08\n",
      " b 600/721 >> 2877.2 ms/b | lr:  0.00011 | grad norm: 8.90 | max abs grad:   0.515 | loss: 2.62 | perp.:  13.75\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2177.60 sec | train_loss:  2.72 | train_perp:  15.16 | valid_loss:  2.62 | valid_perp.:  13.76\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  58/100) lr = 0.0001161\n",
      " b 150/714 >> 2924.1 ms/b | lr:   0.0001 | grad norm: 2.86 | max abs grad:   0.162 | loss: 2.72 | perp.:  15.18\n",
      " b 300/714 >> 3040.9 ms/b | lr:  0.00012 | grad norm: 5.46 | max abs grad:   0.175 | loss: 2.66 | perp.:  14.34\n",
      " b 450/714 >> 3032.3 ms/b | lr:  0.00012 | grad norm: 3.47 | max abs grad:   0.097 | loss: 2.60 | perp.:  13.47\n",
      " b 600/714 >> 3025.6 ms/b | lr:  0.00012 | grad norm: 5.64 | max abs grad:   0.223 | loss: 2.75 | perp.:  15.67\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2132.16 sec | train_loss:  2.70 | train_perp:  14.94 | valid_loss:  2.61 | valid_perp.:  13.57\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  59/100) lr = 0.0001151\n",
      " b 150/717 >> 2914.8 ms/b | lr:  0.00012 | grad norm: 2.87 | max abs grad:   0.093 | loss: 2.69 | perp.:  14.75\n",
      " b 300/717 >> 3066.7 ms/b | lr:  9.8e-05 | grad norm: 4.05 | max abs grad:   0.281 | loss: 2.69 | perp.:  14.70\n",
      " b 450/717 >> 3139.7 ms/b | lr:  0.00012 | grad norm: 6.75 | max abs grad:   0.213 | loss: 2.67 | perp.:  14.38\n",
      " b 600/717 >> 3138.7 ms/b | lr:  0.00011 | grad norm: 4.04 | max abs grad:   0.170 | loss: 2.63 | perp.:  13.84\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2242.81 sec | train_loss:  2.68 | train_perp:  14.61 | valid_loss:  2.55 | valid_perp.:  12.79\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  60/100) lr = 0.0001141\n",
      " b 150/718 >> 3195.0 ms/b | lr:  0.00012 | grad norm: 3.31 | max abs grad:   0.165 | loss: 2.53 | perp.:  12.56\n",
      " b 300/718 >> 3165.0 ms/b | lr:  0.00012 | grad norm: 2.91 | max abs grad:   0.097 | loss: 2.54 | perp.:  12.70\n",
      " b 450/718 >> 2952.3 ms/b | lr:  0.00011 | grad norm: 3.08 | max abs grad:   0.074 | loss: 2.53 | perp.:  12.54\n",
      " b 600/718 >> 3045.7 ms/b | lr:  0.00012 | grad norm: 4.84 | max abs grad:   0.164 | loss: 2.69 | perp.:  14.80\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2212.10 sec | train_loss:  2.61 | train_perp:  13.55 | valid_loss:  2.69 | valid_perp.:  14.77\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  61/100) lr = 0.0001132\n",
      " b 150/721 >> 3071.0 ms/b | lr:  0.00011 | grad norm: 6.21 | max abs grad:   0.627 | loss: 2.61 | perp.:  13.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/721 >> 2973.1 ms/b | lr:  0.00012 | grad norm: 5.01 | max abs grad:   0.185 | loss: 2.59 | perp.:  13.38\n",
      " b 450/721 >> 2938.4 ms/b | lr:   0.0001 | grad norm: 3.25 | max abs grad:   0.116 | loss: 2.49 | perp.:  12.02\n",
      " b 600/721 >> 2997.1 ms/b | lr:   0.0001 | grad norm: 4.49 | max abs grad:   0.291 | loss: 2.59 | perp.:  13.28\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2157.32 sec | train_loss:  2.58 | train_perp:  13.21 | valid_loss:  2.66 | valid_perp.:  14.28\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  62/100) lr = 0.0001123\n",
      " b 150/711 >> 3051.2 ms/b | lr:  0.00012 | grad norm: 5.52 | max abs grad:   0.172 | loss: 2.74 | perp.:  15.54\n",
      " b 300/711 >> 3021.8 ms/b | lr:  9.9e-05 | grad norm: 3.50 | max abs grad:   0.148 | loss: 2.55 | perp.:  12.86\n",
      " b 450/711 >> 2972.5 ms/b | lr:  0.00011 | grad norm: 2.58 | max abs grad:   0.082 | loss: 2.52 | perp.:  12.39\n",
      " b 600/711 >> 3082.8 ms/b | lr:  0.00012 | grad norm: 3.57 | max abs grad:   0.083 | loss: 2.57 | perp.:  13.05\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2158.03 sec | train_loss:  2.64 | train_perp:  13.95 | valid_loss:  2.54 | valid_perp.:  12.64\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  63/100) lr = 0.0001114\n",
      " b 150/716 >> 3011.8 ms/b | lr:   0.0001 | grad norm: 2.64 | max abs grad:   0.165 | loss: 2.54 | perp.:  12.64\n",
      " b 300/716 >> 3065.5 ms/b | lr:  0.00012 | grad norm: 3.96 | max abs grad:   0.110 | loss: 2.63 | perp.:  13.87\n",
      " b 450/716 >> 3005.0 ms/b | lr:   0.0001 | grad norm: 6.49 | max abs grad:   0.315 | loss: 2.53 | perp.:  12.50\n",
      " b 600/716 >> 2953.5 ms/b | lr:  0.00011 | grad norm: 8.01 | max abs grad:   0.523 | loss: 2.56 | perp.:  12.93\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2161.03 sec | train_loss:  2.60 | train_perp:  13.43 | valid_loss:  2.72 | valid_perp.:  15.20\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  64/100) lr = 0.0001105\n",
      " b 150/719 >> 3038.7 ms/b | lr:  0.00011 | grad norm: 5.48 | max abs grad:   0.220 | loss: 2.52 | perp.:  12.47\n",
      " b 300/719 >> 2958.4 ms/b | lr:  0.00011 | grad norm: 4.20 | max abs grad:   0.217 | loss: 2.44 | perp.:  11.46\n",
      " b 450/719 >> 3012.8 ms/b | lr:  0.00012 | grad norm: 2.74 | max abs grad:   0.081 | loss: 2.60 | perp.:  13.47\n",
      " b 600/719 >> 3007.9 ms/b | lr:  0.00011 | grad norm: 5.86 | max abs grad:   0.451 | loss: 2.45 | perp.:  11.64\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2166.96 sec | train_loss:  2.54 | train_perp:  12.66 | valid_loss:  2.43 | valid_perp.:  11.39\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  65/100) lr = 0.0001096\n",
      " b 150/726 >> 2994.2 ms/b | lr:  0.00012 | grad norm: 4.71 | max abs grad:   0.128 | loss: 2.55 | perp.:  12.76\n",
      " b 300/726 >> 2986.8 ms/b | lr:  0.00011 | grad norm: 2.97 | max abs grad:   0.086 | loss: 2.42 | perp.:  11.29\n",
      " b 450/726 >> 2986.7 ms/b | lr:   0.0001 | grad norm: 3.28 | max abs grad:   0.124 | loss: 2.43 | perp.:  11.32\n",
      " b 600/726 >> 2990.7 ms/b | lr:  0.00013 | grad norm: 2.80 | max abs grad:   0.087 | loss: 2.43 | perp.:  11.31\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2163.48 sec | train_loss:  2.48 | train_perp:  11.99 | valid_loss:  2.51 | valid_perp.:  12.28\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  66/100) lr = 0.0001088\n",
      " b 150/722 >> 3021.7 ms/b | lr:  0.00011 | grad norm: 3.20 | max abs grad:   0.161 | loss: 2.60 | perp.:  13.40\n",
      " b 300/722 >> 3009.0 ms/b | lr:  0.00011 | grad norm: 4.88 | max abs grad:   0.132 | loss: 2.40 | perp.:  11.00\n",
      " b 450/722 >> 3199.2 ms/b | lr:   0.0001 | grad norm: 3.81 | max abs grad:   0.174 | loss: 2.40 | perp.:  11.00\n",
      " b 600/722 >> 3088.3 ms/b | lr:  0.00011 | grad norm: 3.95 | max abs grad:   0.181 | loss: 2.41 | perp.:  11.13\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2212.76 sec | train_loss:  2.46 | train_perp:  11.76 | valid_loss:  2.52 | valid_perp.:  12.41\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  67/100) lr = 0.000108\n",
      " b 150/713 >> 2956.8 ms/b | lr:   0.0001 | grad norm: 4.30 | max abs grad:   0.199 | loss: 2.49 | perp.:  12.09\n",
      " b 300/713 >> 2989.0 ms/b | lr:  7.2e-05 | grad norm: 5.60 | max abs grad:   0.434 | loss: 2.50 | perp.:  12.16\n",
      " b 450/713 >> 2965.3 ms/b | lr:  0.00011 | grad norm: 3.71 | max abs grad:   0.105 | loss: 2.44 | perp.:  11.48\n",
      " b 600/713 >> 3052.2 ms/b | lr:  9.5e-05 | grad norm: 3.88 | max abs grad:   0.204 | loss: 2.49 | perp.:  12.07\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2122.38 sec | train_loss:  2.50 | train_perp:  12.21 | valid_loss:  2.40 | valid_perp.:  11.05\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  68/100) lr = 0.0001072\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs, 2*epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, 2*epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data, save_wts = False)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    enc_states, attn_states, dec_states = states\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoder(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoder(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, save_wts = True)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 4\n",
    "model.eval(save_wts = True)\n",
    "# Get some data from a random point in the test_data set\n",
    "states = model.init_states(nb)\n",
    "data, targets = get_batch(test_data, 120, seq_len, evaluate = True)\n",
    "data = data[:,:nb].contiguous()\n",
    "targets = targets.view(seq_len, -1)[:,:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output, states = model(data, states)\n",
    "# Convert the output log probabilities to normal probabilities\n",
    "output = output.exp()\n",
    "# Get the argmax of each step in the output\n",
    "output_p, output_idx = output.max(dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted output word indices to the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.t()\n",
    "output_idx = output_idx.t()\n",
    "for i in range(nb):\n",
    "    # Print the output with the targets\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "List of modules in the model for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modules = list(model.modules())\n",
    "list(enumerate(modules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic weight heat maps to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_wts = np.array(modules[3].weight.data)\n",
    "embed_norm = (embed_wts - embed_wts.mean()) / (embed_wts.max() - embed_wts.min())\n",
    "plt.imshow(embed_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('dim'); plt.ylabel('word index');\n",
    "plt.title('Embedding layer')\n",
    "plt.colorbar()\n",
    "embed_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = modules[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_wts = np.array(attn.attention.weight.data)\n",
    "attn_norm = (attn_wts - attn_wts.mean()) / (attn_wts.max() - attn_wts.min())\n",
    "plt.imshow(attn_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_input+d_state+d_output'); plt.ylabel('d_output')\n",
    "plt.title('Output attention sublayer (in attention mechanism)')\n",
    "plt.colorbar()\n",
    "attn_wts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence attention visualization by mapping the alignment weights (in the attention mechanism) at each step of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = nb//cols\n",
    "fig, axs = plt.subplots(rows, cols, figsize = (30, 20))\n",
    "for b in range(nb):\n",
    "    wts = attn.attn_wts[:,b,:]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    r = b // cols\n",
    "    c = b % cols\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(wts, aspect = 'auto', cmap = 'jet')\n",
    "    # Fix labels\n",
    "    xlabels = list(targets[b].data)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_xlabel('Targets')\n",
    "    ylabels = list(data[:,b].data)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_ylabel('Inputs')\n",
    "    ax.set_title('Example %d' % b)\n",
    "    fig.colorbar(im, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
