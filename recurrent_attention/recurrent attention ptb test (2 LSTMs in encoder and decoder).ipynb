{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "from recurrent_attention import RecurrentAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead stuff\n",
    "\n",
    "Helper functions for batching, resetting hidden states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eval_batch_size = 10\n",
    "batch_size = 74\n",
    "seq_len = 18\n",
    "dropout = 0.1\n",
    "clip = 1.5\n",
    "lr = 0.02\n",
    "warmup_steps = 10\n",
    "decay_factor = 0.5  # Higher => faster learning rate decay\n",
    "smoothing = 0.05\n",
    "\n",
    "epochs = 50\n",
    "log_interval = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embed_size = 512\n",
    "encode_size = 256\n",
    "h_size = 256\n",
    "decode_size = 256\n",
    "decode_out_size = 512\n",
    "n_enc_layers = 2\n",
    "attn_rnn_layers = 1\n",
    "n_dec_layers = 2\n",
    "smooth_align = True\n",
    "\n",
    "bidirectional_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, `batchify` arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into batches\n",
    "    nbatches = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data.narrow(0, 0, nbatches * batch_size)\n",
    "    # Evenly divide the data across the batches\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "# Wraps hidden states into new Variables to detach them from their history\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "# `get_batch` subdivides the source data into chunks of the specified length.\n",
    "# E.g., using the example for the `batchify` function above and a length of 2,\n",
    "# we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the `batchify` function. The chunks are along dimension 0, corresponding\n",
    "# to the `seq_len` dimension in the LSTM.\n",
    "def get_batch(source, i, seq_len, evaluate = False):\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = Variable(source[i : i+seq_len], volatile = evaluate)\n",
    "    target = Variable(source[i+1 : i+1+seq_len].view(-1), volatile = evaluate)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing class for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx = None, smoothing = 0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = torch.zeros_like(x.data)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist.add_(self.smoothing / self.size)\n",
    "        if self.padding_idx is not None:\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad = False)) * ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduler that sets the learning rate factor according to:\n",
    "\n",
    "$$\\text{lr} = d_{\\text{model}}^{-0.5}\\cdot\\min{(\\text{epoch}^{-0.5}, \\text{epoch}\\cdot\\text{warmup}^{-1.5})}$$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first $\\text{warmup}$ epochs, then decreasing it proportionally to the inverse square root of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(h_size, warmup, optimizer):\n",
    "    lrate = lambda e: h_size**(-0.5) * min((e+1)**(-decay_factor), (e+1) * warmup**(-(decay_factor+1)))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x229021b2d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2290012e908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = lambda factor, h_size, warmup: lambda e: factor*(h_size**(-0.5) * min(e**(-decay_factor), e * warmup**(-(decay_factor+1))))\n",
    "opts = [\n",
    "    lrate(2*lr, embed_size, warmup_steps), \n",
    "    lrate(lr, embed_size*2, warmup_steps),\n",
    "    lrate(lr, embed_size, warmup_steps//2),\n",
    "    lrate(lr, embed_size, warmup_steps*2),\n",
    "    lrate(lr, embed_size, warmup_steps),\n",
    "]\n",
    "plt.plot(np.arange(1, epochs+1), [[opt(i) for opt in opts] for i in range(1, epochs+1)])\n",
    "plt.legend([\n",
    "    \"%.4g:%d:%d\" % (2*lr, embed_size, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size*2, warmup_steps),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps//2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps*2),\n",
    "    \"%.4g:%d:%d\" % (lr, embed_size, warmup_steps),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/ptb')\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, embed_size, encode_size, h_size,\n",
    "                 decode_size, decode_out_size, n_enc_layers, attn_rnn_layers,\n",
    "                 n_dec_layers, align_location = False, loc_align_size = 1,\n",
    "                 loc_align_kernel = 1,  smooth_align = False, bidirectional_attn = False,\n",
    "                 tie_wts = True, dropout = 0.1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.embed_size = embed_size\n",
    "        self.encode_size = encode_size\n",
    "        self.h_size = h_size\n",
    "        self.decode_size = decode_size\n",
    "        self.decode_out_size = decode_out_size\n",
    "        self.n_enc_layers = n_enc_layers\n",
    "        self.attn_rnn_layers = attn_rnn_layers\n",
    "        self.n_dec_layers = n_dec_layers\n",
    "        self.align_location = align_location\n",
    "        self.loc_align_size = loc_align_size\n",
    "        self.loc_align_kernel = loc_align_kernel\n",
    "        self.smooth_align = smooth_align\n",
    "        self.bidirectional_attn = bidirectional_attn\n",
    "        self.tie_wts = tie_wts\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab, embed_size)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size = embed_size if i == 0 else encode_size,\n",
    "                hidden_size = encode_size, dropout = dropout\n",
    "            ) for i in range(n_enc_layers)\n",
    "        ])\n",
    "        self.attn = RecurrentAttention(\n",
    "            in_size = encode_size, h_size = h_size, out_size = decode_size,\n",
    "            align_location = align_location, loc_align_size = loc_align_size,\n",
    "            loc_align_kernel = loc_align_kernel, smooth_align = smooth_align,\n",
    "            num_rnn_layers = attn_rnn_layers, attn_act_fn = 'ReLU',\n",
    "            dropout = dropout, bidirectional = bidirectional_attn\n",
    "        )\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size = decode_size, dropout = dropout,\n",
    "                hidden_size = decode_size if i < n_dec_layers-1 else decode_out_size,\n",
    "            ) for i in range(n_dec_layers)\n",
    "        ])\n",
    "        self.projection = nn.Linear(decode_out_size, tgt_vocab)\n",
    "        if tie_wts and src_vocab == tgt_vocab and embed_size == decode_out_size:\n",
    "            self.embedding.weight = self.projection.weight\n",
    "        self.log_softmax = nn.LogSoftmax(dim = -1)\n",
    "            \n",
    "        # For visualizations\n",
    "        self.save_wts = False\n",
    "        self.enc_out = None\n",
    "        self.dec_out = None\n",
    "        \n",
    "    def init(self):\n",
    "        for subnet in [self.encoders, self.decoders]:\n",
    "            for layer in subnet:\n",
    "                for p in layer.parameters():\n",
    "                    if p.dim() > 1:\n",
    "                        nn.init.xavier_normal(p)\n",
    "                    else:\n",
    "                        p.data.fill_(0)\n",
    "        for p in self.projection.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "            else:\n",
    "                p.data.fill_(0)\n",
    "        self.attn.init()\n",
    "        \n",
    "    def init_states(self, batch_size):\n",
    "        encoder_states = [\n",
    "            (\n",
    "                Variable(torch.zeros(1, batch_size, self.encode_size)),\n",
    "                Variable(torch.zeros(1, batch_size, self.encode_size))\n",
    "            ) for _ in range(self.n_enc_layers)\n",
    "        ]\n",
    "        attn_states = self.attn.init_rnn_states(batch_size)\n",
    "        decoder_states = [\n",
    "            (\n",
    "                Variable(torch.zeros(\n",
    "                    1, batch_size, self.decode_size if i < self.n_dec_layers-1 else self.decode_out_size\n",
    "                )),\n",
    "                Variable(torch.zeros(\n",
    "                    1, batch_size, self.decode_size if i < self.n_dec_layers-1 else self.decode_out_size\n",
    "                ))\n",
    "            ) for i in range(self.n_dec_layers)\n",
    "        ]\n",
    "        return encoder_states, attn_states, decoder_states\n",
    "    \n",
    "    def forward(self, inputs, states):\n",
    "        enc_states, attn_states, dec_states = states\n",
    "        if self.save_wts:\n",
    "            self.enc_out = []\n",
    "            self.dec_out = []\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = self.embedding(inputs) * np.sqrt(self.embed_size)\n",
    "        \n",
    "        # Encoder stack\n",
    "        new_enc_states = []\n",
    "        enc_in = self.drop(self.relu(embeddings))\n",
    "        for states, encoder in zip(enc_states, self.encoders):\n",
    "            enc_out, new_enc_state = encoder(enc_in, states)\n",
    "            new_enc_states.append(new_enc_state)\n",
    "            if self.save_wts:\n",
    "                self.enc_out.append(enc_out.data.clone())\n",
    "            enc_in = enc_out\n",
    "                \n",
    "        # Attention mechanism\n",
    "        attn_out, new_attn_states = self.attn(enc_out, attn_states)\n",
    "        \n",
    "        # Decoder stack\n",
    "        new_dec_states = []\n",
    "        dec_in = attn_out\n",
    "        for states, decoder in zip(dec_states, self.decoders):\n",
    "            dec_out, new_dec_state = decoder(dec_in, states)\n",
    "            new_dec_states.append(new_dec_state)\n",
    "            if self.save_wts:\n",
    "                self.dec_out.append(dec_out.data.clone())\n",
    "            dec_in = dec_out\n",
    "        \n",
    "        # Projection layer\n",
    "        logits = self.projection(dec_out)\n",
    "        output = self.log_softmax(logits)\n",
    "        \n",
    "        return output, (new_enc_states, new_attn_states, new_dec_states)\n",
    "    \n",
    "    def train(self, mode = True, save_wts = False):\n",
    "        super(RNNModel, self).train(mode)\n",
    "        self.attn.save_attn_wts = save_wts\n",
    "        self.save_wts = save_wts\n",
    "        \n",
    "    def eval(self, save_wts = True):\n",
    "        super(RNNModel, self).eval()\n",
    "        self.attn.save_attn_wts = save_wts\n",
    "        self.save_wts = save_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, criterion, optimizer, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 9533969\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    src_vocab = ntokens, tgt_vocab = ntokens, embed_size = embed_size,\n",
    "    encode_size = encode_size, h_size = h_size, decode_size = decode_size,\n",
    "    decode_out_size = decode_out_size, n_enc_layers = n_enc_layers,\n",
    "    attn_rnn_layers = attn_rnn_layers, n_dec_layers = n_dec_layers,\n",
    "    dropout = dropout, smooth_align = smooth_align\n",
    ")\n",
    "model.init()\n",
    "criterion = LabelSmoothing(ntokens, smoothing = smoothing)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr = lr, betas = (0.9, 0.98), eps = 1e-9\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(embed_size, warmup_steps, optimizer)\n",
    "# Reference\n",
    "nparams = sum([p.numel() for p in model.parameters()])\n",
    "print('Model parameters: %d' % nparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Ready the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12562, 74]), torch.Size([7376, 10]), torch.Size([8243, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "train_data.size(), val_data.size(), test_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Use random length sequences\n",
    "    seq_lens = []\n",
    "    tot_len = 0\n",
    "    jitter = 0.15 * seq_len\n",
    "    num_data = train_data.size(0)\n",
    "    while tot_len < num_data - 2:\n",
    "        if num_data - tot_len - 2 <= seq_len + jitter:\n",
    "            slen = num_data - tot_len - 2\n",
    "        else:\n",
    "            slen = int(np.random.normal(seq_len, jitter))\n",
    "            if slen <= 0:\n",
    "                slen = seq_len    # eh\n",
    "            if tot_len + slen >= num_data - jitter - 2:\n",
    "                slen = num_data - tot_len - 2\n",
    "        seq_lens.append(slen)\n",
    "        tot_len += slen\n",
    "    i_cumseq = [0] + list(np.cumsum(seq_lens)[:-1])\n",
    "    idx = np.arange(len(seq_lens))\n",
    "    np.random.shuffle(idx)\n",
    "    # Turn on training mode\n",
    "    model.train(save_wts = False)\n",
    "    # Initialize RNN states\n",
    "    states = model.init_states(batch_size)\n",
    "    # Prep metainfo\n",
    "    total_loss = 0\n",
    "    total_epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(idx):\n",
    "        # Get training data\n",
    "        data, targets = get_batch(train_data, i_cumseq[i], seq_lens[i])\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states)\n",
    "        # Zero out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run the model forward\n",
    "        output, _states = model(data, states)\n",
    "        if np.isnan(output.data).any():\n",
    "            return 0, total_epoch_loss[0], data, targets, states, _states\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        if np.isnan(loss.data[0]):\n",
    "            return 1, total_epoch_loss[0], data, targets, states, _states\n",
    "        states = _states\n",
    "        # Propagate loss gradient backwards\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # Save gradient statistics before they're changed cuz we'll be logging this batch\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            # Calculate the largest (absolute) gradient of all elements in the model parameters\n",
    "            max_grad = max([p.grad.data.abs().max() for p in parameters])\n",
    "        total_norm = nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        # Scale the batch learning rate so that shorter sequences aren't \"stronger\"\n",
    "        scaled_lr = lr_scheduler.get_lr()[0] * np.sqrt(seq_lens[i] / seq_len)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = scaled_lr\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get some metainfo\n",
    "        total_loss += loss.data\n",
    "        total_epoch_loss += loss.data * data.size(0)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            print(' b {:3d}/{:3d} >> {:6.1f} ms/b | lr: {:8.2g} | grad norm: {:4.2f} | max abs grad: {:7.3f} | loss: {:4.2f} | perp.: {:6.2f}'.format(\n",
    "                batch, len(seq_lens), elapsed * 1000/log_interval, scaled_lr, total_norm, max_grad, cur_loss, np.exp(cur_loss)\n",
    "            ))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return -1, total_epoch_loss[0] / num_data, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_src, save_wts = True):\n",
    "    model.eval(save_wts = save_wts)\n",
    "    total_loss = 0\n",
    "    states = model.init_states(eval_batch_size)\n",
    "    for i in range(0, data_src.size(0) - 1, seq_len):\n",
    "        # Get data\n",
    "        data, targets = get_batch(data_src, i, seq_len, evaluate = True)\n",
    "        # Repackage the hidden states\n",
    "        states = repackage_hidden(states)\n",
    "        # Evaluate\n",
    "        output, states = model(data, states)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        total_loss += loss.data * data.size(0)\n",
    "    return total_loss[0] / data_src.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/ 50) lr = 2.795e-05 (warmup)\n",
      " b 150/727 >> 2826.7 ms/b | lr:  2.7e-05 | grad norm: 1.86 | max abs grad:   0.056 | loss: 7.86 | perp.: 2593.44\n",
      " b 300/727 >> 2654.5 ms/b | lr:  2.8e-05 | grad norm: 0.76 | max abs grad:   0.023 | loss: 6.30 | perp.: 543.79\n",
      " b 450/727 >> 2634.9 ms/b | lr:  2.9e-05 | grad norm: 0.69 | max abs grad:   0.016 | loss: 6.18 | perp.: 484.89\n",
      " b 600/727 >> 2720.8 ms/b | lr:  2.6e-05 | grad norm: 0.66 | max abs grad:   0.011 | loss: 6.16 | perp.: 475.71\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1960.42 sec | train_loss:  6.53 | train_perp: 685.59 | valid_loss:  6.13 | valid_perp.: 459.32\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   2/ 50) lr = 5.59e-05 (warmup)\n",
      " b 150/712 >> 2943.3 ms/b | lr:  4.9e-05 | grad norm: 0.66 | max abs grad:   0.015 | loss: 6.21 | perp.: 497.78\n",
      " b 300/712 >> 3207.3 ms/b | lr:  5.7e-05 | grad norm: 0.56 | max abs grad:   0.010 | loss: 6.17 | perp.: 478.74\n",
      " b 450/712 >> 3093.4 ms/b | lr:  5.3e-05 | grad norm: 0.49 | max abs grad:   0.009 | loss: 6.17 | perp.: 479.98\n",
      " b 600/712 >> 2950.6 ms/b | lr:  5.6e-05 | grad norm: 0.46 | max abs grad:   0.008 | loss: 6.17 | perp.: 476.78\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2159.83 sec | train_loss:  6.17 | train_perp: 477.43 | valid_loss:  6.13 | valid_perp.: 459.65\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   3/ 50) lr = 8.385e-05 (warmup)\n",
      " b 150/720 >> 2759.3 ms/b | lr:  8.1e-05 | grad norm: 0.46 | max abs grad:   0.011 | loss: 6.20 | perp.: 491.09\n",
      " b 300/720 >> 2851.2 ms/b | lr:  8.4e-05 | grad norm: 0.58 | max abs grad:   0.016 | loss: 6.13 | perp.: 461.21\n",
      " b 450/720 >> 2684.4 ms/b | lr:  8.1e-05 | grad norm: 0.42 | max abs grad:   0.009 | loss: 6.09 | perp.: 442.70\n",
      " b 600/720 >> 2749.9 ms/b | lr:  8.1e-05 | grad norm: 0.74 | max abs grad:   0.018 | loss: 6.08 | perp.: 434.87\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1975.78 sec | train_loss:  6.10 | train_perp: 447.58 | valid_loss:  6.00 | valid_perp.: 403.34\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   4/ 50) lr = 0.0001118 (warmup)\n",
      " b 150/724 >> 2662.2 ms/b | lr:  9.1e-05 | grad norm: 0.61 | max abs grad:   0.014 | loss: 6.07 | perp.: 431.76\n",
      " b 300/724 >> 2616.3 ms/b | lr:  0.00011 | grad norm: 0.42 | max abs grad:   0.008 | loss: 5.99 | perp.: 400.40\n",
      " b 450/724 >> 2674.6 ms/b | lr:   0.0001 | grad norm: 0.51 | max abs grad:   0.011 | loss: 5.99 | perp.: 397.61\n",
      " b 600/724 >> 2733.1 ms/b | lr:  0.00011 | grad norm: 0.81 | max abs grad:   0.016 | loss: 5.97 | perp.: 393.09\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1933.13 sec | train_loss:  5.99 | train_perp: 400.37 | valid_loss:  5.93 | valid_perp.: 375.10\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   5/ 50) lr = 0.0001398 (warmup)\n",
      " b 150/722 >> 2569.9 ms/b | lr:  0.00014 | grad norm: 0.71 | max abs grad:   0.020 | loss: 5.98 | perp.: 395.06\n",
      " b 300/722 >> 2622.1 ms/b | lr:  0.00016 | grad norm: 0.73 | max abs grad:   0.018 | loss: 5.94 | perp.: 381.68\n",
      " b 450/722 >> 2691.5 ms/b | lr:  0.00014 | grad norm: 0.51 | max abs grad:   0.012 | loss: 5.92 | perp.: 373.25\n",
      " b 600/722 >> 2624.9 ms/b | lr:  0.00014 | grad norm: 0.54 | max abs grad:   0.013 | loss: 5.91 | perp.: 369.09\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1897.36 sec | train_loss:  5.92 | train_perp: 373.13 | valid_loss:  5.84 | valid_perp.: 344.17\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   6/ 50) lr = 0.0001677 (warmup)\n",
      " b 150/720 >> 2734.3 ms/b | lr:  0.00016 | grad norm: 0.70 | max abs grad:   0.014 | loss: 5.90 | perp.: 365.59\n",
      " b 300/720 >> 2632.0 ms/b | lr:  0.00019 | grad norm: 0.63 | max abs grad:   0.013 | loss: 5.83 | perp.: 341.96\n",
      " b 450/720 >> 2636.0 ms/b | lr:  0.00014 | grad norm: 0.70 | max abs grad:   0.017 | loss: 5.82 | perp.: 337.06\n",
      " b 600/720 >> 2668.0 ms/b | lr:  0.00018 | grad norm: 0.63 | max abs grad:   0.016 | loss: 5.79 | perp.: 326.23\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1901.03 sec | train_loss:  5.82 | train_perp: 336.44 | valid_loss:  5.75 | valid_perp.: 314.17\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   7/ 50) lr = 0.0001957 (warmup)\n",
      " b 150/723 >> 2579.6 ms/b | lr:  0.00016 | grad norm: 0.75 | max abs grad:   0.019 | loss: 5.78 | perp.: 324.57\n",
      " b 300/723 >> 2654.0 ms/b | lr:  0.00022 | grad norm: 0.59 | max abs grad:   0.010 | loss: 5.72 | perp.: 304.25\n",
      " b 450/723 >> 2625.6 ms/b | lr:  0.00019 | grad norm: 0.58 | max abs grad:   0.010 | loss: 5.72 | perp.: 305.60\n",
      " b 600/723 >> 2699.7 ms/b | lr:   0.0002 | grad norm: 0.62 | max abs grad:   0.013 | loss: 5.70 | perp.: 300.00\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1903.03 sec | train_loss:  5.72 | train_perp: 304.40 | valid_loss:  5.63 | valid_perp.: 277.30\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   8/ 50) lr = 0.0002236 (warmup)\n",
      " b 150/721 >> 2688.0 ms/b | lr:  0.00025 | grad norm: 0.67 | max abs grad:   0.019 | loss: 5.67 | perp.: 290.97\n",
      " b 300/721 >> 2631.8 ms/b | lr:  0.00023 | grad norm: 0.63 | max abs grad:   0.012 | loss: 5.60 | perp.: 271.76\n",
      " b 450/721 >> 2633.5 ms/b | lr:  0.00022 | grad norm: 0.84 | max abs grad:   0.016 | loss: 5.58 | perp.: 265.47\n",
      " b 600/721 >> 2539.0 ms/b | lr:   0.0002 | grad norm: 0.83 | max abs grad:   0.016 | loss: 5.52 | perp.: 248.61\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1895.37 sec | train_loss:  5.58 | train_perp: 264.78 | valid_loss:  5.49 | valid_perp.: 242.53\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch   9/ 50) lr = 0.0002516 (warmup)\n",
      " b 150/719 >> 2656.2 ms/b | lr:  0.00023 | grad norm: 1.09 | max abs grad:   0.038 | loss: 5.47 | perp.: 238.41\n",
      " b 300/719 >> 2596.2 ms/b | lr:  0.00023 | grad norm: 1.06 | max abs grad:   0.024 | loss: 5.38 | perp.: 217.67\n",
      " b 450/719 >> 2762.1 ms/b | lr:  0.00024 | grad norm: 1.46 | max abs grad:   0.039 | loss: 5.35 | perp.: 210.30\n",
      " b 600/719 >> 2800.8 ms/b | lr:  0.00027 | grad norm: 2.44 | max abs grad:   0.067 | loss: 5.34 | perp.: 207.82\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1940.99 sec | train_loss:  5.36 | train_perp: 213.31 | valid_loss:  5.51 | valid_perp.: 246.03\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  10/ 50) lr = 0.0002795 (warmup)\n",
      " b 150/720 >> 2756.7 ms/b | lr:  0.00026 | grad norm: 1.31 | max abs grad:   0.036 | loss: 5.27 | perp.: 195.37\n",
      " b 300/720 >> 2693.1 ms/b | lr:  0.00026 | grad norm: 1.18 | max abs grad:   0.026 | loss: 5.16 | perp.: 174.61\n",
      " b 450/720 >> 2641.8 ms/b | lr:  0.00025 | grad norm: 1.88 | max abs grad:   0.048 | loss: 5.13 | perp.: 169.25\n",
      " b 600/720 >> 2574.5 ms/b | lr:  0.00027 | grad norm: 1.61 | max abs grad:   0.042 | loss: 5.09 | perp.: 162.12\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1912.19 sec | train_loss:  5.16 | train_perp: 174.55 | valid_loss:  4.91 | valid_perp.: 136.09\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  11/ 50) lr = 0.0002665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 150/717 >> 2665.0 ms/b | lr:  0.00027 | grad norm: 1.44 | max abs grad:   0.036 | loss: 5.03 | perp.: 152.88\n",
      " b 300/717 >> 2598.5 ms/b | lr:  0.00029 | grad norm: 1.51 | max abs grad:   0.032 | loss: 4.98 | perp.: 145.03\n",
      " b 450/717 >> 2706.0 ms/b | lr:  0.00024 | grad norm: 2.58 | max abs grad:   0.062 | loss: 4.94 | perp.: 140.09\n",
      " b 600/717 >> 2752.1 ms/b | lr:  0.00024 | grad norm: 2.10 | max abs grad:   0.048 | loss: 4.98 | perp.: 146.13\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1934.84 sec | train_loss:  4.97 | train_perp: 143.82 | valid_loss:  5.75 | valid_perp.: 314.47\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  12/ 50) lr = 0.0002552\n",
      " b 150/711 >> 2656.8 ms/b | lr:  0.00026 | grad norm: 2.16 | max abs grad:   0.055 | loss: 4.96 | perp.: 143.28\n",
      " b 300/711 >> 2611.9 ms/b | lr:  0.00024 | grad norm: 3.58 | max abs grad:   0.106 | loss: 4.81 | perp.: 122.16\n",
      " b 450/711 >> 2696.6 ms/b | lr:  0.00028 | grad norm: 6.23 | max abs grad:   0.183 | loss: 4.74 | perp.: 114.57\n",
      " b 600/711 >> 2725.2 ms/b | lr:  0.00023 | grad norm: 1.89 | max abs grad:   0.085 | loss: 4.85 | perp.: 127.34\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1894.04 sec | train_loss:  4.83 | train_perp: 125.34 | valid_loss:  4.46 | valid_perp.:  86.52\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  13/ 50) lr = 0.0002451\n",
      " b 150/723 >> 2623.6 ms/b | lr:  0.00025 | grad norm: 3.39 | max abs grad:   0.095 | loss: 4.82 | perp.: 123.67\n",
      " b 300/723 >> 2638.4 ms/b | lr:  0.00024 | grad norm: 3.34 | max abs grad:   0.104 | loss: 4.65 | perp.: 104.61\n",
      " b 450/723 >> 2580.8 ms/b | lr:   0.0002 | grad norm: 4.20 | max abs grad:   0.154 | loss: 4.65 | perp.: 104.62\n",
      " b 600/723 >> 2640.1 ms/b | lr:  0.00025 | grad norm: 6.24 | max abs grad:   0.189 | loss: 4.65 | perp.: 104.91\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1897.77 sec | train_loss:  4.70 | train_perp: 109.85 | valid_loss:  4.33 | valid_perp.:  75.92\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  14/ 50) lr = 0.0002362\n",
      " b 150/709 >> 2694.9 ms/b | lr:  0.00024 | grad norm: 2.87 | max abs grad:   0.094 | loss: 4.65 | perp.: 104.18\n",
      " b 300/709 >> 2667.9 ms/b | lr:  0.00026 | grad norm: 6.04 | max abs grad:   0.145 | loss: 4.56 | perp.:  95.63\n",
      " b 450/709 >> 2625.2 ms/b | lr:  0.00022 | grad norm: 2.04 | max abs grad:   0.062 | loss: 4.42 | perp.:  83.35\n",
      " b 600/709 >> 2726.3 ms/b | lr:  0.00026 | grad norm: 1.80 | max abs grad:   0.045 | loss: 4.48 | perp.:  88.06\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1901.17 sec | train_loss:  4.52 | train_perp:  91.46 | valid_loss:  4.17 | valid_perp.:  64.81\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  15/ 50) lr = 0.0002282\n",
      " b 150/719 >> 2660.4 ms/b | lr:  0.00022 | grad norm: 2.39 | max abs grad:   0.064 | loss: 4.38 | perp.:  80.20\n",
      " b 300/719 >> 2607.9 ms/b | lr:  0.00023 | grad norm: 2.97 | max abs grad:   0.080 | loss: 4.38 | perp.:  80.00\n",
      " b 450/719 >> 2702.0 ms/b | lr:  0.00021 | grad norm: 2.55 | max abs grad:   0.094 | loss: 4.49 | perp.:  89.02\n",
      " b 600/719 >> 2618.9 ms/b | lr:  0.00025 | grad norm: 9.08 | max abs grad:   0.223 | loss: 4.26 | perp.:  70.59\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1898.32 sec | train_loss:  4.37 | train_perp:  79.07 | valid_loss:  3.97 | valid_perp.:  53.13\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  16/ 50) lr = 0.000221\n",
      " b 150/714 >> 2721.8 ms/b | lr:  0.00019 | grad norm: 4.76 | max abs grad:   0.251 | loss: 4.43 | perp.:  83.85\n",
      " b 300/714 >> 2705.4 ms/b | lr:   0.0002 | grad norm: 5.23 | max abs grad:   0.244 | loss: 4.19 | perp.:  65.77\n",
      " b 450/714 >> 2659.5 ms/b | lr:  0.00019 | grad norm: 2.78 | max abs grad:   0.090 | loss: 4.21 | perp.:  67.25\n",
      " b 600/714 >> 2592.2 ms/b | lr:  0.00026 | grad norm: 8.70 | max abs grad:   0.251 | loss: 4.10 | perp.:  60.18\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1899.96 sec | train_loss:  4.25 | train_perp:  70.11 | valid_loss:  3.87 | valid_perp.:  48.17\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  17/ 50) lr = 0.0002144\n",
      " b 150/715 >> 2709.1 ms/b | lr:   0.0002 | grad norm: 3.90 | max abs grad:   0.147 | loss: 4.55 | perp.:  94.19\n",
      " b 300/715 >> 2688.8 ms/b | lr:  0.00021 | grad norm: 4.98 | max abs grad:   0.164 | loss: 4.05 | perp.:  57.44\n",
      " b 450/715 >> 2609.9 ms/b | lr:  0.00022 | grad norm: 3.26 | max abs grad:   0.113 | loss: 4.09 | perp.:  59.78\n",
      " b 600/715 >> 2680.4 ms/b | lr:  0.00017 | grad norm: 4.90 | max abs grad:   0.197 | loss: 4.25 | perp.:  70.06\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1900.33 sec | train_loss:  4.22 | train_perp:  68.16 | valid_loss:  4.33 | valid_perp.:  76.01\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  18/ 50) lr = 0.0002083\n",
      " b 150/714 >> 2677.9 ms/b | lr:   0.0002 | grad norm: 3.99 | max abs grad:   0.141 | loss: 4.44 | perp.:  84.43\n",
      " b 300/714 >> 2691.5 ms/b | lr:  0.00021 | grad norm: 2.66 | max abs grad:   0.079 | loss: 3.96 | perp.:  52.69\n",
      " b 450/714 >> 2705.7 ms/b | lr:  0.00021 | grad norm: 6.49 | max abs grad:   0.182 | loss: 3.96 | perp.:  52.41\n",
      " b 600/714 >> 2681.6 ms/b | lr:   0.0002 | grad norm: 5.67 | max abs grad:   0.135 | loss: 4.01 | perp.:  55.27\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1907.24 sec | train_loss:  4.09 | train_perp:  59.51 | valid_loss:  6.15 | valid_perp.: 466.96\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  19/ 50) lr = 0.0002028\n",
      " b 150/723 >> 2641.2 ms/b | lr:  0.00019 | grad norm: 2.79 | max abs grad:   0.105 | loss: 4.14 | perp.:  62.98\n",
      " b 300/723 >> 2584.6 ms/b | lr:  0.00017 | grad norm: 4.71 | max abs grad:   0.241 | loss: 3.97 | perp.:  52.74\n",
      " b 450/723 >> 2700.5 ms/b | lr:   0.0002 | grad norm: 5.77 | max abs grad:   0.177 | loss: 3.84 | perp.:  46.46\n",
      " b 600/723 >> 2636.4 ms/b | lr:  0.00021 | grad norm: 2.51 | max abs grad:   0.069 | loss: 3.74 | perp.:  42.19\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1899.44 sec | train_loss:  3.91 | train_perp:  50.03 | valid_loss:  3.48 | valid_perp.:  32.60\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  20/ 50) lr = 0.0001976\n",
      " b 150/716 >> 2675.0 ms/b | lr:  0.00019 | grad norm: 5.55 | max abs grad:   0.243 | loss: 4.27 | perp.:  71.60\n",
      " b 300/716 >> 2613.2 ms/b | lr:  0.00019 | grad norm: 5.04 | max abs grad:   0.129 | loss: 3.75 | perp.:  42.35\n",
      " b 450/716 >> 2697.1 ms/b | lr:   0.0002 | grad norm: 2.73 | max abs grad:   0.078 | loss: 3.85 | perp.:  46.77\n",
      " b 600/716 >> 2658.3 ms/b | lr:   0.0002 | grad norm: 2.71 | max abs grad:   0.070 | loss: 3.74 | perp.:  41.99\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1899.11 sec | train_loss:  3.93 | train_perp:  50.75 | valid_loss:  3.66 | valid_perp.:  39.04\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  21/ 50) lr = 0.0001929\n",
      " b 150/718 >> 2730.0 ms/b | lr:   0.0002 | grad norm: 4.69 | max abs grad:   0.141 | loss: 4.01 | perp.:  54.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/718 >> 2711.6 ms/b | lr:  0.00018 | grad norm: 2.80 | max abs grad:   0.146 | loss: 3.78 | perp.:  43.63\n",
      " b 450/718 >> 2579.4 ms/b | lr:   0.0002 | grad norm: 2.34 | max abs grad:   0.048 | loss: 3.64 | perp.:  38.03\n",
      " b 600/718 >> 2570.9 ms/b | lr:  0.00019 | grad norm: 2.56 | max abs grad:   0.082 | loss: 3.66 | perp.:  39.04\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1899.63 sec | train_loss:  3.79 | train_perp:  44.39 | valid_loss:  3.44 | valid_perp.:  31.21\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  22/ 50) lr = 0.0001884\n",
      " b 150/716 >> 2635.6 ms/b | lr:  0.00017 | grad norm: 2.47 | max abs grad:   0.063 | loss: 3.89 | perp.:  48.91\n",
      " b 300/716 >> 2587.4 ms/b | lr:   0.0002 | grad norm: 3.62 | max abs grad:   0.148 | loss: 3.69 | perp.:  40.22\n",
      " b 450/716 >> 2654.2 ms/b | lr:  0.00018 | grad norm: 8.57 | max abs grad:   0.255 | loss: 3.82 | perp.:  45.54\n",
      " b 600/716 >> 2757.4 ms/b | lr:  0.00021 | grad norm: 2.34 | max abs grad:   0.045 | loss: 3.64 | perp.:  38.25\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1932.98 sec | train_loss:  3.74 | train_perp:  42.17 | valid_loss:  3.25 | valid_perp.:  25.77\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  23/ 50) lr = 0.0001843\n",
      " b 150/719 >> 2919.1 ms/b | lr:  0.00018 | grad norm: 2.61 | max abs grad:   0.065 | loss: 3.62 | perp.:  37.19\n",
      " b 300/719 >> 3013.6 ms/b | lr:  0.00014 | grad norm: 3.50 | max abs grad:   0.164 | loss: 3.72 | perp.:  41.14\n",
      " b 450/719 >> 2992.8 ms/b | lr:  0.00018 | grad norm: 6.17 | max abs grad:   0.186 | loss: 3.52 | perp.:  33.65\n",
      " b 600/719 >> 2958.5 ms/b | lr:  0.00018 | grad norm: 4.11 | max abs grad:   0.104 | loss: 3.44 | perp.:  31.19\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2125.28 sec | train_loss:  3.58 | train_perp:  35.79 | valid_loss:  5.46 | valid_perp.: 235.89\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  24/ 50) lr = 0.0001804\n",
      " b 150/721 >> 2923.5 ms/b | lr:  0.00019 | grad norm: 5.74 | max abs grad:   0.170 | loss: 3.88 | perp.:  48.38\n",
      " b 300/721 >> 2751.1 ms/b | lr:  0.00018 | grad norm: 2.77 | max abs grad:   0.086 | loss: 3.37 | perp.:  29.13\n",
      " b 450/721 >> 2678.6 ms/b | lr:   0.0002 | grad norm: 6.09 | max abs grad:   0.177 | loss: 3.58 | perp.:  35.73\n",
      " b 600/721 >> 2575.3 ms/b | lr:  0.00019 | grad norm: 5.64 | max abs grad:   0.155 | loss: 3.52 | perp.:  33.73\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1950.53 sec | train_loss:  3.60 | train_perp:  36.56 | valid_loss:  3.86 | valid_perp.:  47.60\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  25/ 50) lr = 0.0001768\n",
      " b 150/727 >> 2555.0 ms/b | lr:  0.00016 | grad norm: 11.52 | max abs grad:   0.525 | loss: 3.59 | perp.:  36.30\n",
      " b 300/727 >> 2614.4 ms/b | lr:  0.00018 | grad norm: 4.02 | max abs grad:   0.086 | loss: 3.47 | perp.:  32.04\n",
      " b 450/727 >> 2618.6 ms/b | lr:  0.00013 | grad norm: 3.25 | max abs grad:   0.187 | loss: 3.29 | perp.:  26.76\n",
      " b 600/727 >> 2604.1 ms/b | lr:  0.00017 | grad norm: 2.72 | max abs grad:   0.137 | loss: 3.39 | perp.:  29.77\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1891.49 sec | train_loss:  3.44 | train_perp:  31.10 | valid_loss:  2.92 | valid_perp.:  18.48\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  26/ 50) lr = 0.0001733\n",
      " b 150/720 >> 2587.9 ms/b | lr:  0.00016 | grad norm: 3.24 | max abs grad:   0.128 | loss: 3.53 | perp.:  34.02\n",
      " b 300/720 >> 2640.3 ms/b | lr:  0.00017 | grad norm: 6.53 | max abs grad:   0.222 | loss: 3.38 | perp.:  29.41\n",
      " b 450/720 >> 2748.0 ms/b | lr:  0.00018 | grad norm: 5.93 | max abs grad:   0.169 | loss: 3.37 | perp.:  29.15\n",
      " b 600/720 >> 2952.0 ms/b | lr:  0.00017 | grad norm: 3.86 | max abs grad:   0.146 | loss: 3.39 | perp.:  29.74\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1997.80 sec | train_loss:  3.42 | train_perp:  30.52 | valid_loss:  2.94 | valid_perp.:  18.89\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  27/ 50) lr = 0.0001701\n",
      " b 150/710 >> 2936.6 ms/b | lr:  0.00017 | grad norm: 7.74 | max abs grad:   0.315 | loss: 3.38 | perp.:  29.29\n",
      " b 300/710 >> 2896.1 ms/b | lr:  0.00018 | grad norm: 11.27 | max abs grad:   0.230 | loss: 3.19 | perp.:  24.34\n",
      " b 450/710 >> 2856.9 ms/b | lr:  0.00017 | grad norm: 7.50 | max abs grad:   0.269 | loss: 3.40 | perp.:  30.03\n",
      " b 600/710 >> 2692.3 ms/b | lr:  0.00017 | grad norm: 7.37 | max abs grad:   0.164 | loss: 3.20 | perp.:  24.57\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1997.64 sec | train_loss:  3.29 | train_perp:  26.82 | valid_loss:  2.68 | valid_perp.:  14.58\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  28/ 50) lr = 0.000167\n",
      " b 150/718 >> 2921.9 ms/b | lr:  0.00018 | grad norm: 3.23 | max abs grad:   0.097 | loss: 3.42 | perp.:  30.45\n",
      " b 300/718 >> 2916.5 ms/b | lr:  0.00017 | grad norm: 4.42 | max abs grad:   0.104 | loss: 3.06 | perp.:  21.26\n",
      " b 450/718 >> 2687.8 ms/b | lr:  0.00017 | grad norm: 9.16 | max abs grad:   0.199 | loss: 3.16 | perp.:  23.56\n",
      " b 600/718 >> 2655.5 ms/b | lr:  0.00017 | grad norm: 7.04 | max abs grad:   0.239 | loss: 3.26 | perp.:  25.92\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1982.93 sec | train_loss:  3.23 | train_perp:  25.28 | valid_loss:  2.73 | valid_perp.:  15.33\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  29/ 50) lr = 0.0001641\n",
      " b 150/722 >> 2898.9 ms/b | lr:  0.00012 | grad norm: 7.68 | max abs grad:   0.400 | loss: 3.50 | perp.:  33.18\n",
      " b 300/722 >> 2910.4 ms/b | lr:  0.00017 | grad norm: 6.29 | max abs grad:   0.223 | loss: 3.25 | perp.:  25.77\n",
      " b 450/722 >> 2745.0 ms/b | lr:  0.00014 | grad norm: 2.50 | max abs grad:   0.057 | loss: 3.00 | perp.:  20.17\n",
      " b 600/722 >> 2763.2 ms/b | lr:  0.00016 | grad norm: 6.46 | max abs grad:   0.204 | loss: 3.21 | perp.:  24.69\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2033.00 sec | train_loss:  3.26 | train_perp:  26.05 | valid_loss:  2.60 | valid_perp.:  13.47\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  30/ 50) lr = 0.0001614\n",
      " b 150/722 >> 2722.5 ms/b | lr:  0.00016 | grad norm: 12.31 | max abs grad:   0.326 | loss: 3.23 | perp.:  25.18\n",
      " b 300/722 >> 2667.5 ms/b | lr:  0.00017 | grad norm: 4.70 | max abs grad:   0.140 | loss: 3.13 | perp.:  22.95\n",
      " b 450/722 >> 2670.3 ms/b | lr:  0.00016 | grad norm: 2.42 | max abs grad:   0.062 | loss: 3.02 | perp.:  20.49\n",
      " b 600/722 >> 2692.8 ms/b | lr:  0.00016 | grad norm: 3.35 | max abs grad:   0.109 | loss: 3.13 | perp.:  22.92\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1939.50 sec | train_loss:  3.14 | train_perp:  23.19 | valid_loss:  2.61 | valid_perp.:  13.63\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  31/ 50) lr = 0.0001588\n",
      " b 150/714 >> 2867.7 ms/b | lr:  0.00018 | grad norm: 12.44 | max abs grad:   0.310 | loss: 3.10 | perp.:  22.25\n",
      " b 300/714 >> 2734.0 ms/b | lr:  0.00014 | grad norm: 2.92 | max abs grad:   0.134 | loss: 2.89 | perp.:  17.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 450/714 >> 2624.7 ms/b | lr:  0.00016 | grad norm: 3.96 | max abs grad:   0.118 | loss: 3.26 | perp.:  26.15\n",
      " b 600/714 >> 2697.2 ms/b | lr:  0.00016 | grad norm: 8.71 | max abs grad:   0.304 | loss: 3.01 | perp.:  20.38\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1956.95 sec | train_loss:  3.08 | train_perp:  21.81 | valid_loss:  3.39 | valid_perp.:  29.65\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  32/ 50) lr = 0.0001563\n",
      " b 150/719 >> 2877.3 ms/b | lr:  0.00015 | grad norm: 4.04 | max abs grad:   0.123 | loss: 3.11 | perp.:  22.53\n",
      " b 300/719 >> 2757.6 ms/b | lr:  0.00016 | grad norm: 6.90 | max abs grad:   0.189 | loss: 2.88 | perp.:  17.86\n",
      " b 450/719 >> 2650.0 ms/b | lr:  0.00015 | grad norm: 3.51 | max abs grad:   0.139 | loss: 2.80 | perp.:  16.37\n",
      " b 600/719 >> 2631.6 ms/b | lr:  0.00017 | grad norm: 7.35 | max abs grad:   0.239 | loss: 2.92 | perp.:  18.56\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1958.57 sec | train_loss:  2.98 | train_perp:  19.78 | valid_loss:  2.47 | valid_perp.:  11.83\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  33/ 50) lr = 0.0001539\n",
      " b 150/713 >> 2666.6 ms/b | lr:  0.00016 | grad norm: 20.66 | max abs grad:   0.802 | loss: 3.05 | perp.:  21.17\n",
      " b 300/713 >> 2661.3 ms/b | lr:  0.00015 | grad norm: 6.70 | max abs grad:   0.489 | loss: 2.99 | perp.:  19.86\n",
      " b 450/713 >> 2766.3 ms/b | lr:  0.00016 | grad norm: 3.75 | max abs grad:   0.086 | loss: 2.87 | perp.:  17.67\n",
      " b 600/713 >> 2755.4 ms/b | lr:  0.00017 | grad norm: 7.37 | max abs grad:   0.238 | loss: 2.84 | perp.:  17.08\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1942.59 sec | train_loss:  2.98 | train_perp:  19.64 | valid_loss:  2.36 | valid_perp.:  10.63\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  34/ 50) lr = 0.0001516\n",
      " b 150/716 >> 2879.4 ms/b | lr:  0.00014 | grad norm: 6.90 | max abs grad:   0.598 | loss: 3.12 | perp.:  22.67\n",
      " b 300/716 >> 2656.3 ms/b | lr:  0.00016 | grad norm: 4.26 | max abs grad:   0.113 | loss: 2.69 | perp.:  14.66\n",
      " b 450/716 >> 2921.8 ms/b | lr:  0.00017 | grad norm: 7.91 | max abs grad:   0.226 | loss: 3.08 | perp.:  21.86\n",
      " b 600/716 >> 2823.9 ms/b | lr:  0.00016 | grad norm: 9.25 | max abs grad:   0.265 | loss: 2.83 | perp.:  16.93\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2029.81 sec | train_loss:  2.95 | train_perp:  19.03 | valid_loss:  6.37 | valid_perp.: 585.67\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  35/ 50) lr = 0.0001494\n",
      " b 150/714 >> 2845.2 ms/b | lr:  0.00017 | grad norm: 3.50 | max abs grad:   0.125 | loss: 2.90 | perp.:  18.16\n",
      " b 300/714 >> 2712.7 ms/b | lr:  0.00014 | grad norm: 7.69 | max abs grad:   0.278 | loss: 2.83 | perp.:  16.96\n",
      " b 450/714 >> 2731.7 ms/b | lr:  0.00014 | grad norm: 4.05 | max abs grad:   0.100 | loss: 2.72 | perp.:  15.19\n",
      " b 600/714 >> 2737.2 ms/b | lr:  0.00015 | grad norm: 2.94 | max abs grad:   0.073 | loss: 2.93 | perp.:  18.71\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1956.68 sec | train_loss:  2.86 | train_perp:  17.46 | valid_loss:  2.26 | valid_perp.:   9.54\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  36/ 50) lr = 0.0001473\n",
      " b 150/716 >> 2677.1 ms/b | lr:  0.00014 | grad norm: 7.04 | max abs grad:   0.413 | loss: 2.79 | perp.:  16.30\n",
      " b 300/716 >> 2690.9 ms/b | lr:  0.00015 | grad norm: 10.10 | max abs grad:   0.265 | loss: 2.95 | perp.:  19.15\n",
      " b 450/716 >> 2604.7 ms/b | lr:  0.00014 | grad norm: 2.75 | max abs grad:   0.090 | loss: 2.61 | perp.:  13.56\n",
      " b 600/716 >> 2677.1 ms/b | lr:  0.00017 | grad norm: 9.74 | max abs grad:   0.335 | loss: 2.74 | perp.:  15.54\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1908.34 sec | train_loss:  2.80 | train_perp:  16.40 | valid_loss:  2.22 | valid_perp.:   9.19\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  37/ 50) lr = 0.0001453\n",
      " b 150/715 >> 2675.4 ms/b | lr:  0.00015 | grad norm: 4.72 | max abs grad:   0.180 | loss: 2.74 | perp.:  15.46\n",
      " b 300/715 >> 2677.6 ms/b | lr:  0.00012 | grad norm: 4.13 | max abs grad:   0.145 | loss: 2.67 | perp.:  14.43\n",
      " b 450/715 >> 2732.1 ms/b | lr:  0.00014 | grad norm: 10.36 | max abs grad:   0.276 | loss: 2.65 | perp.:  14.09\n",
      " b 600/715 >> 2542.6 ms/b | lr:  0.00015 | grad norm: 8.85 | max abs grad:   0.287 | loss: 2.62 | perp.:  13.78\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1907.48 sec | train_loss:  2.72 | train_perp:  15.18 | valid_loss:  2.20 | valid_perp.:   9.02\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  38/ 50) lr = 0.0001434\n",
      " b 150/714 >> 2658.7 ms/b | lr:  0.00014 | grad norm: 5.30 | max abs grad:   0.170 | loss: 2.74 | perp.:  15.47\n",
      " b 300/714 >> 2691.1 ms/b | lr:  0.00015 | grad norm: 4.93 | max abs grad:   0.178 | loss: 2.94 | perp.:  18.90\n",
      " b 450/714 >> 2647.5 ms/b | lr:  0.00014 | grad norm: 3.85 | max abs grad:   0.111 | loss: 2.61 | perp.:  13.64\n",
      " b 600/714 >> 2673.4 ms/b | lr:  0.00013 | grad norm: 4.12 | max abs grad:   0.102 | loss: 2.61 | perp.:  13.62\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1903.37 sec | train_loss:  2.74 | train_perp:  15.50 | valid_loss:  2.44 | valid_perp.:  11.44\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  39/ 50) lr = 0.0001415\n",
      " b 150/723 >> 2614.5 ms/b | lr:  0.00014 | grad norm: 6.59 | max abs grad:   0.274 | loss: 2.64 | perp.:  13.98\n",
      " b 300/723 >> 2644.1 ms/b | lr:  0.00015 | grad norm: 9.17 | max abs grad:   0.261 | loss: 2.70 | perp.:  14.85\n",
      " b 450/723 >> 2644.7 ms/b | lr:  0.00015 | grad norm: 6.28 | max abs grad:   0.190 | loss: 2.62 | perp.:  13.77\n",
      " b 600/723 >> 2644.4 ms/b | lr:  0.00016 | grad norm: 10.93 | max abs grad:   0.298 | loss: 2.43 | perp.:  11.38\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1901.98 sec | train_loss:  2.60 | train_perp:  13.41 | valid_loss:  2.29 | valid_perp.:   9.86\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  40/ 50) lr = 0.0001398\n",
      " b 150/719 >> 2626.3 ms/b | lr:  0.00014 | grad norm: 7.13 | max abs grad:   0.235 | loss: 2.42 | perp.:  11.24\n",
      " b 300/719 >> 2724.5 ms/b | lr:  0.00015 | grad norm: 20.44 | max abs grad:   0.701 | loss: 2.79 | perp.:  16.26\n",
      " b 450/719 >> 2692.0 ms/b | lr:  0.00014 | grad norm: 9.44 | max abs grad:   0.330 | loss: 2.65 | perp.:  14.12\n",
      " b 600/719 >> 2614.6 ms/b | lr:  0.00014 | grad norm: 5.75 | max abs grad:   0.122 | loss: 2.59 | perp.:  13.31\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1903.36 sec | train_loss:  2.62 | train_perp:  13.69 | valid_loss:  2.13 | valid_perp.:   8.38\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  41/ 50) lr = 0.000138\n",
      " b 150/719 >> 2696.6 ms/b | lr:  0.00013 | grad norm: 15.14 | max abs grad:   1.411 | loss: 2.80 | perp.:  16.47\n",
      " b 300/719 >> 2645.1 ms/b | lr:  0.00014 | grad norm: 5.71 | max abs grad:   0.282 | loss: 2.60 | perp.:  13.40\n",
      " b 450/719 >> 2603.6 ms/b | lr:  0.00012 | grad norm: 4.67 | max abs grad:   0.150 | loss: 2.40 | perp.:  10.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 600/719 >> 2680.4 ms/b | lr:  0.00014 | grad norm: 8.22 | max abs grad:   0.192 | loss: 2.63 | perp.:  13.92\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1907.91 sec | train_loss:  2.60 | train_perp:  13.49 | valid_loss:  2.14 | valid_perp.:   8.53\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  42/ 50) lr = 0.0001364\n",
      " b 150/713 >> 2703.9 ms/b | lr:  0.00015 | grad norm: 6.93 | max abs grad:   0.238 | loss: 2.92 | perp.:  18.52\n",
      " b 300/713 >> 2582.4 ms/b | lr:  0.00012 | grad norm: 2.78 | max abs grad:   0.081 | loss: 2.48 | perp.:  11.97\n",
      " b 450/713 >> 2715.9 ms/b | lr:  0.00012 | grad norm: 12.25 | max abs grad:   0.760 | loss: 2.52 | perp.:  12.41\n",
      " b 600/713 >> 2710.8 ms/b | lr:  0.00013 | grad norm: 3.12 | max abs grad:   0.100 | loss: 2.44 | perp.:  11.51\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1905.87 sec | train_loss:  2.58 | train_perp:  13.20 | valid_loss:  2.02 | valid_perp.:   7.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  43/ 50) lr = 0.0001348\n",
      " b 150/720 >> 2625.9 ms/b | lr:  0.00014 | grad norm: 9.09 | max abs grad:   0.180 | loss: 2.36 | perp.:  10.63\n",
      " b 300/720 >> 2644.6 ms/b | lr:  0.00014 | grad norm: 5.52 | max abs grad:   0.197 | loss: 2.80 | perp.:  16.38\n",
      " b 450/720 >> 2621.1 ms/b | lr:  0.00013 | grad norm: 4.41 | max abs grad:   0.158 | loss: 2.30 | perp.:   9.99\n",
      " b 600/720 >> 2678.3 ms/b | lr:  0.00013 | grad norm: 5.38 | max abs grad:   0.189 | loss: 2.51 | perp.:  12.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1898.64 sec | train_loss:  2.50 | train_perp:  12.13 | valid_loss:  1.99 | valid_perp.:   7.31\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  44/ 50) lr = 0.0001333\n",
      " b 150/719 >> 2720.2 ms/b | lr:  0.00013 | grad norm: 5.77 | max abs grad:   0.240 | loss: 2.66 | perp.:  14.27\n",
      " b 300/719 >> 2594.6 ms/b | lr:  0.00013 | grad norm: 10.13 | max abs grad:   0.462 | loss: 2.31 | perp.:  10.04\n",
      " b 450/719 >> 2632.2 ms/b | lr:  0.00013 | grad norm: 10.00 | max abs grad:   0.411 | loss: 2.32 | perp.:  10.15\n",
      " b 600/719 >> 2714.0 ms/b | lr:  0.00013 | grad norm: 7.32 | max abs grad:   0.230 | loss: 2.42 | perp.:  11.27\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1927.22 sec | train_loss:  2.44 | train_perp:  11.46 | valid_loss:  1.93 | valid_perp.:   6.91\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  45/ 50) lr = 0.0001318\n",
      " b 150/723 >> 2685.4 ms/b | lr:  0.00014 | grad norm: 11.76 | max abs grad:   0.352 | loss: 2.63 | perp.:  13.93\n",
      " b 300/723 >> 2760.7 ms/b | lr:  0.00013 | grad norm: 4.45 | max abs grad:   0.179 | loss: 2.35 | perp.:  10.51\n",
      " b 450/723 >> 2738.2 ms/b | lr:  0.00016 | grad norm: 4.84 | max abs grad:   0.135 | loss: 2.65 | perp.:  14.11\n",
      " b 600/723 >> 2706.8 ms/b | lr:  0.00014 | grad norm: 4.34 | max abs grad:   0.103 | loss: 2.36 | perp.:  10.60\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1951.99 sec | train_loss:  2.52 | train_perp:  12.44 | valid_loss:  2.01 | valid_perp.:   7.48\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  46/ 50) lr = 0.0001303\n",
      " b 150/720 >> 2604.9 ms/b | lr:  0.00015 | grad norm: 5.27 | max abs grad:   0.143 | loss: 2.45 | perp.:  11.62\n",
      " b 300/720 >> 2690.2 ms/b | lr:  0.00015 | grad norm: 4.42 | max abs grad:   0.099 | loss: 2.59 | perp.:  13.29\n",
      " b 450/720 >> 2607.3 ms/b | lr:  0.00013 | grad norm: 6.05 | max abs grad:   0.309 | loss: 2.24 | perp.:   9.41\n",
      " b 600/720 >> 2672.0 ms/b | lr:  0.00013 | grad norm: 3.35 | max abs grad:   0.189 | loss: 2.22 | perp.:   9.19\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1915.45 sec | train_loss:  2.46 | train_perp:  11.71 | valid_loss:  1.85 | valid_perp.:   6.36\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  47/ 50) lr = 0.0001289\n",
      " b 150/722 >> 2691.1 ms/b | lr:  0.00015 | grad norm: 14.91 | max abs grad:   0.499 | loss: 2.38 | perp.:  10.82\n",
      " b 300/722 >> 2648.5 ms/b | lr:  0.00013 | grad norm: 4.00 | max abs grad:   0.120 | loss: 2.22 | perp.:   9.19\n",
      " b 450/722 >> 2764.3 ms/b | lr:  0.00013 | grad norm: 6.57 | max abs grad:   0.244 | loss: 2.28 | perp.:   9.74\n",
      " b 600/722 >> 2619.0 ms/b | lr:  0.00012 | grad norm: 4.51 | max abs grad:   0.148 | loss: 2.15 | perp.:   8.59\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1926.27 sec | train_loss:  2.25 | train_perp:   9.47 | valid_loss:  2.43 | valid_perp.:  11.32\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  48/ 50) lr = 0.0001276\n",
      " b 150/721 >> 2591.3 ms/b | lr:  0.00012 | grad norm: 5.87 | max abs grad:   0.405 | loss: 2.45 | perp.:  11.59\n",
      " b 300/721 >> 2579.1 ms/b | lr:  0.00012 | grad norm: 5.16 | max abs grad:   0.403 | loss: 2.43 | perp.:  11.41\n",
      " b 450/721 >> 2610.6 ms/b | lr:  0.00014 | grad norm: 7.82 | max abs grad:   0.348 | loss: 2.25 | perp.:   9.52\n",
      " b 600/721 >> 2677.6 ms/b | lr:  0.00013 | grad norm: 4.21 | max abs grad:   0.195 | loss: 2.28 | perp.:   9.80\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1892.18 sec | train_loss:  2.39 | train_perp:  10.92 | valid_loss:  1.96 | valid_perp.:   7.10\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  49/ 50) lr = 0.0001263\n",
      " b 150/721 >> 2604.0 ms/b | lr:  0.00013 | grad norm: 8.83 | max abs grad:   0.479 | loss: 2.20 | perp.:   9.03\n",
      " b 300/721 >> 2595.2 ms/b | lr:  0.00013 | grad norm: 3.95 | max abs grad:   0.098 | loss: 2.19 | perp.:   8.92\n",
      " b 450/721 >> 2684.9 ms/b | lr:  0.00013 | grad norm: 19.52 | max abs grad:   0.849 | loss: 2.19 | perp.:   8.94\n",
      " b 600/721 >> 2634.6 ms/b | lr:  0.00013 | grad norm: 4.72 | max abs grad:   0.160 | loss: 2.20 | perp.:   9.04\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1894.01 sec | train_loss:  2.23 | train_perp:   9.30 | valid_loss:  2.09 | valid_perp.:   8.09\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  50/ 50) lr = 0.000125\n",
      " b 150/716 >> 2619.2 ms/b | lr:  0.00011 | grad norm: 14.98 | max abs grad:   0.953 | loss: 2.38 | perp.:  10.78\n",
      " b 300/716 >> 2668.2 ms/b | lr:  0.00013 | grad norm: 4.46 | max abs grad:   0.170 | loss: 2.27 | perp.:   9.68\n",
      " b 450/716 >> 2633.4 ms/b | lr:  0.00011 | grad norm: 2.90 | max abs grad:   0.097 | loss: 2.18 | perp.:   8.80\n",
      " b 600/716 >> 2660.3 ms/b | lr:  0.00013 | grad norm: 11.88 | max abs grad:   0.467 | loss: 2.07 | perp.:   7.93\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 1893.80 sec | train_loss:  2.24 | train_perp:   9.38 | valid_loss:  2.08 | valid_perp.:   8.01\n",
      "================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data, save_wts = False)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  51/100) lr = 0.0001238\n",
      " b 150/714 >> 4085.1 ms/b | lr:  0.00013 | grad norm: 14.93 | max abs grad:   0.701 | loss: 2.45 | perp.:  11.60\n",
      " b 300/714 >> 3442.0 ms/b | lr:  0.00014 | grad norm: 3.59 | max abs grad:   0.117 | loss: 2.06 | perp.:   7.86\n",
      " b 450/714 >> 3377.8 ms/b | lr:  0.00012 | grad norm: 3.71 | max abs grad:   0.125 | loss: 2.04 | perp.:   7.68\n",
      " b 600/714 >> 3419.3 ms/b | lr:  0.00014 | grad norm: 14.27 | max abs grad:   0.565 | loss: 2.05 | perp.:   7.77\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2543.86 sec | train_loss:  2.19 | train_perp:   8.90 | valid_loss:  3.17 | valid_perp.:  23.86\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  52/100) lr = 0.0001226\n",
      " b 150/720 >> 3346.7 ms/b | lr:  0.00012 | grad norm: 5.19 | max abs grad:   0.117 | loss: 2.21 | perp.:   9.15\n",
      " b 300/720 >> 3354.1 ms/b | lr:  0.00012 | grad norm: 6.72 | max abs grad:   0.318 | loss: 2.18 | perp.:   8.85\n",
      " b 450/720 >> 3329.8 ms/b | lr:  0.00013 | grad norm: 5.81 | max abs grad:   0.206 | loss: 1.99 | perp.:   7.31\n",
      " b 600/720 >> 3496.6 ms/b | lr:  0.00012 | grad norm: 8.46 | max abs grad:   0.351 | loss: 2.08 | perp.:   7.98\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2458.46 sec | train_loss:  2.13 | train_perp:   8.44 | valid_loss:  1.71 | valid_perp.:   5.51\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  53/100) lr = 0.0001214\n",
      " b 150/714 >> 3519.7 ms/b | lr:  0.00011 | grad norm: 7.24 | max abs grad:   0.541 | loss: 2.28 | perp.:   9.82\n",
      " b 300/714 >> 3404.9 ms/b | lr:  0.00011 | grad norm: 3.76 | max abs grad:   0.169 | loss: 2.29 | perp.:   9.84\n",
      " b 450/714 >> 3427.0 ms/b | lr:  0.00012 | grad norm: 18.06 | max abs grad:   0.653 | loss: 2.07 | perp.:   7.96\n",
      " b 600/714 >> 3289.6 ms/b | lr:  0.00011 | grad norm: 2.92 | max abs grad:   0.146 | loss: 1.86 | perp.:   6.41\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2443.46 sec | train_loss:  2.14 | train_perp:   8.53 | valid_loss:  1.72 | valid_perp.:   5.56\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  54/100) lr = 0.0001203\n",
      " b 150/720 >> 3443.3 ms/b | lr:  0.00012 | grad norm: 17.51 | max abs grad:   0.755 | loss: 2.32 | perp.:  10.17\n",
      " b 300/720 >> 3410.2 ms/b | lr:  0.00012 | grad norm: 16.98 | max abs grad:   0.484 | loss: 2.14 | perp.:   8.48\n",
      " b 450/720 >> 3411.9 ms/b | lr:  0.00012 | grad norm: 3.78 | max abs grad:   0.164 | loss: 1.93 | perp.:   6.92\n",
      " b 600/720 >> 3368.5 ms/b | lr:  0.00011 | grad norm: 4.35 | max abs grad:   0.286 | loss: 1.91 | perp.:   6.74\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2436.40 sec | train_loss:  2.07 | train_perp:   7.96 | valid_loss:  1.64 | valid_perp.:   5.14\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  55/100) lr = 0.0001192\n",
      " b 150/717 >> 3690.7 ms/b | lr:  0.00012 | grad norm: 10.25 | max abs grad:   0.253 | loss: 2.22 | perp.:   9.24\n",
      " b 300/717 >> 3541.8 ms/b | lr:  0.00011 | grad norm: 3.23 | max abs grad:   0.156 | loss: 2.08 | perp.:   8.01\n",
      " b 450/717 >> 3393.0 ms/b | lr:  0.00013 | grad norm: 3.72 | max abs grad:   0.176 | loss: 1.85 | perp.:   6.34\n",
      " b 600/717 >> 3412.7 ms/b | lr:  0.00013 | grad norm: 2.84 | max abs grad:   0.082 | loss: 1.94 | perp.:   6.93\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2512.27 sec | train_loss:  2.01 | train_perp:   7.44 | valid_loss:  1.66 | valid_perp.:   5.24\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  56/100) lr = 0.0001181\n",
      " b 150/713 >> 3730.1 ms/b | lr:  0.00011 | grad norm: 18.90 | max abs grad:   0.962 | loss: 1.92 | perp.:   6.80\n",
      " b 300/713 >> 3631.0 ms/b | lr:  0.00013 | grad norm: 8.39 | max abs grad:   0.377 | loss: 1.84 | perp.:   6.31\n",
      " b 450/713 >> 3612.0 ms/b | lr:  0.00011 | grad norm: 5.45 | max abs grad:   0.163 | loss: 1.94 | perp.:   6.97\n",
      " b 600/713 >> 3387.8 ms/b | lr:  0.00013 | grad norm: 3.78 | max abs grad:   0.159 | loss: 1.75 | perp.:   5.76\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2556.18 sec | train_loss:  1.97 | train_perp:   7.15 | valid_loss:  1.56 | valid_perp.:   4.78\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  57/100) lr = 0.0001171\n",
      " b 150/720 >> 3403.8 ms/b | lr:  0.00012 | grad norm: 14.55 | max abs grad:   0.360 | loss: 1.80 | perp.:   6.06\n",
      " b 300/720 >> 3414.2 ms/b | lr:  0.00012 | grad norm: 3.02 | max abs grad:   0.129 | loss: 1.79 | perp.:   6.01\n",
      " b 450/720 >> 3460.4 ms/b | lr:  0.00011 | grad norm: 3.38 | max abs grad:   0.181 | loss: 1.91 | perp.:   6.78\n",
      " b 600/720 >> 3376.8 ms/b | lr:  9.2e-05 | grad norm: 5.09 | max abs grad:   0.272 | loss: 1.67 | perp.:   5.34\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2456.06 sec | train_loss:  1.81 | train_perp:   6.11 | valid_loss:  1.76 | valid_perp.:   5.81\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  58/100) lr = 0.0001161\n",
      " b 150/717 >> 3369.1 ms/b | lr:  0.00012 | grad norm: 3.83 | max abs grad:   0.120 | loss: 1.85 | perp.:   6.33\n",
      " b 300/717 >> 3592.7 ms/b | lr:  0.00011 | grad norm: 4.17 | max abs grad:   0.151 | loss: 1.80 | perp.:   6.03\n",
      " b 450/717 >> 3575.5 ms/b | lr:  0.00012 | grad norm: 4.61 | max abs grad:   0.216 | loss: 1.70 | perp.:   5.50\n",
      " b 600/717 >> 3959.5 ms/b | lr:  0.00011 | grad norm: 3.03 | max abs grad:   0.125 | loss: 1.79 | perp.:   6.02\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2609.83 sec | train_loss:  1.81 | train_perp:   6.10 | valid_loss:  1.50 | valid_perp.:   4.50\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  59/100) lr = 0.0001151\n",
      " b 150/714 >> 3610.1 ms/b | lr:  0.00012 | grad norm: 15.32 | max abs grad:   0.653 | loss: 1.74 | perp.:   5.68\n",
      " b 300/714 >> 3513.4 ms/b | lr:  0.00012 | grad norm: 35.39 | max abs grad:   2.835 | loss: 1.77 | perp.:   5.85\n",
      " b 450/714 >> 3466.1 ms/b | lr:  0.00011 | grad norm: 3.49 | max abs grad:   0.153 | loss: 1.69 | perp.:   5.43\n",
      " b 600/714 >> 3467.9 ms/b | lr:  0.00011 | grad norm: 3.08 | max abs grad:   0.079 | loss: 1.69 | perp.:   5.41\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2505.55 sec | train_loss:  1.73 | train_perp:   5.64 | valid_loss:  1.58 | valid_perp.:   4.88\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  60/100) lr = 0.0001141\n",
      " b 150/714 >> 3436.7 ms/b | lr:  0.00012 | grad norm: 4.19 | max abs grad:   0.178 | loss: 1.73 | perp.:   5.66\n",
      " b 300/714 >> 3557.6 ms/b | lr:  0.00011 | grad norm: 5.28 | max abs grad:   0.226 | loss: 2.28 | perp.:   9.79\n",
      " b 450/714 >> 3552.6 ms/b | lr:  0.00012 | grad norm: 4.23 | max abs grad:   0.206 | loss: 1.72 | perp.:   5.59\n",
      " b 600/714 >> 3422.5 ms/b | lr:   0.0001 | grad norm: 2.75 | max abs grad:   0.087 | loss: 1.67 | perp.:   5.29\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2486.04 sec | train_loss:  1.86 | train_perp:   6.43 | valid_loss:  1.45 | valid_perp.:   4.25\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  61/100) lr = 0.0001132\n",
      " b 150/717 >> 3395.0 ms/b | lr:  0.00012 | grad norm: 3.22 | max abs grad:   0.131 | loss: 1.59 | perp.:   4.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b 300/717 >> 3455.7 ms/b | lr:  0.00012 | grad norm: 4.85 | max abs grad:   0.243 | loss: 1.60 | perp.:   4.96\n",
      " b 450/717 >> 3482.0 ms/b | lr:  0.00011 | grad norm: 5.70 | max abs grad:   0.234 | loss: 1.65 | perp.:   5.22\n",
      " b 600/717 >> 3496.6 ms/b | lr:  0.00011 | grad norm: 5.93 | max abs grad:   0.230 | loss: 2.41 | perp.:  11.17\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2484.43 sec | train_loss:  1.82 | train_perp:   6.14 | valid_loss:  1.44 | valid_perp.:   4.21\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  62/100) lr = 0.0001123\n",
      " b 150/725 >> 3555.5 ms/b | lr:  0.00011 | grad norm: 2.71 | max abs grad:   0.138 | loss: 1.58 | perp.:   4.86\n",
      " b 300/725 >> 3346.0 ms/b | lr:  0.00012 | grad norm: 5.44 | max abs grad:   0.185 | loss: 1.57 | perp.:   4.79\n",
      " b 450/725 >> 3370.8 ms/b | lr:  0.00011 | grad norm: 2.90 | max abs grad:   0.153 | loss: 1.59 | perp.:   4.93\n",
      " b 600/725 >> 3453.6 ms/b | lr:  0.00011 | grad norm: 3.44 | max abs grad:   0.141 | loss: 1.63 | perp.:   5.09\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2486.44 sec | train_loss:  1.60 | train_perp:   4.94 | valid_loss:  1.39 | valid_perp.:   4.02\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  63/100) lr = 0.0001114\n",
      " b 150/719 >> 3539.5 ms/b | lr:  0.00011 | grad norm: 4.34 | max abs grad:   0.160 | loss: 1.70 | perp.:   5.50\n",
      " b 300/719 >> 3398.7 ms/b | lr:  9.5e-05 | grad norm: 6.63 | max abs grad:   0.277 | loss: 1.54 | perp.:   4.69\n",
      " b 450/719 >> 3443.9 ms/b | lr:  0.00011 | grad norm: 21.61 | max abs grad:   1.346 | loss: 1.64 | perp.:   5.15\n",
      " b 600/719 >> 3520.3 ms/b | lr:   0.0001 | grad norm: 3.92 | max abs grad:   0.149 | loss: 1.56 | perp.:   4.74\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2491.65 sec | train_loss:  1.61 | train_perp:   5.02 | valid_loss:  1.38 | valid_perp.:   3.96\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  64/100) lr = 0.0001105\n",
      " b 150/716 >> 3480.0 ms/b | lr:   0.0001 | grad norm: 2.69 | max abs grad:   0.080 | loss: 1.60 | perp.:   4.95\n",
      " b 300/716 >> 3496.7 ms/b | lr:  0.00011 | grad norm: 39.29 | max abs grad:   1.533 | loss: 1.99 | perp.:   7.30\n",
      " b 450/716 >> 3731.4 ms/b | lr:  0.00011 | grad norm: 9.32 | max abs grad:   0.535 | loss: 2.20 | perp.:   8.99\n",
      " b 600/716 >> 3573.4 ms/b | lr:  0.00012 | grad norm: 25.96 | max abs grad:   1.696 | loss: 1.82 | perp.:   6.19\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 2539.32 sec | train_loss:  2.02 | train_perp:   7.50 | valid_loss:  1.35 | valid_perp.:   3.87\n",
      "================================================================================================================\n",
      "\n",
      "\n",
      "Epoch  65/100) lr = 0.0001096\n",
      " b 150/722 >> 3424.8 ms/b | lr:  0.00011 | grad norm: 3.68 | max abs grad:   0.143 | loss: 1.55 | perp.:   4.71\n",
      " b 300/722 >> 3399.0 ms/b | lr:   0.0001 | grad norm: 5.93 | max abs grad:   0.405 | loss: 1.96 | perp.:   7.10\n",
      " b 450/722 >> 3329.1 ms/b | lr:  0.00011 | grad norm: 2.88 | max abs grad:   0.104 | loss: 1.50 | perp.:   4.48\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 112\n",
    "CAUSES = ['output', 'grad']\n",
    "for epoch in range(epochs, 2*epochs):\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch {:3d}/{:3d}) lr = {:0.4g}{}'.format(epoch+1, 2*epochs, np.mean(lr_scheduler.get_lr()[0]), ' (warmup)' if epoch < warmup_steps else ''))\n",
    "    start_time = time.time()\n",
    "    stat, train_loss, data, targets, states, nstates = train()\n",
    "    if stat in list(range(len(CAUSES))):\n",
    "        c = CAUSES[stat]\n",
    "        n = (WIDTH - len(c) - 4) // 2\n",
    "        print('\\n' + (' '*n) + 'NaN ' + c)\n",
    "        break\n",
    "    elapsed = time.time() - start_time\n",
    "    val_loss = evaluate(val_data, save_wts = False)\n",
    "    max_param = max([p.data.abs().max() for p in model.parameters() if p.grad is not None])\n",
    "    print('-' * WIDTH)\n",
    "    print('Elapsed time: {:6.2f} sec | train_loss: {:5.2f} | train_perp: {:6.2f} | valid_loss: {:5.2f} | valid_perp.: {:6.2f}'.format(\n",
    "        elapsed, train_loss, np.exp(train_loss), val_loss, np.exp(val_loss)\n",
    "    ))\n",
    "    print('=' * WIDTH)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat in list(range(len(CAUSES))):\n",
    "    params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(any([np.isnan(p.data).any() for p in params]), any([np.isnan(p.grad.data).any() for p in params]))\n",
    "    \n",
    "    enc_states, attn_states, dec_states = states\n",
    "    relu = nn.ReLU()\n",
    "    log_softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    embeddings = model.embedding(data)\n",
    "    enc_out, new_enc_states = model.encoder(model.drop(embeddings))\n",
    "    attn_out, new_attn_states = model.attn(enc_out, attn_states)\n",
    "    dec_out, new_dec_states = model.decoder(relu(attn_out))\n",
    "    output = model.projection(dec_out)\n",
    "    \n",
    "    print([\n",
    "        np.isnan(p.data).any() for p in [embeddings, enc_out, attn_out, dec_out, output]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, save_wts = True)\n",
    "print('test_loss: {:5.2f} | test_perplexity: {:5.2f}'.format(\n",
    "    test_loss, np.exp(test_loss)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 4\n",
    "model.eval(save_wts = True)\n",
    "# Get some data from a random point in the test_data set\n",
    "states = model.init_states(nb)\n",
    "data, targets = get_batch(test_data, 120, seq_len, evaluate = True)\n",
    "data = data[:,:nb].contiguous()\n",
    "targets = targets.view(seq_len, -1)[:,:nb].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model forward\n",
    "output, states = model(data, states)\n",
    "# Convert the output log probabilities to normal probabilities\n",
    "output = output.exp()\n",
    "# Get the argmax of each step in the output\n",
    "output_p, output_idx = output.max(dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted output word indices to the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.t()\n",
    "output_idx = output_idx.t()\n",
    "for i in range(nb):\n",
    "    # Print the output with the targets\n",
    "    seqs = torch.cat([targets[i].unsqueeze(0), output_idx[i].unsqueeze(0)], 0)\n",
    "    # Number incorrectly predicted\n",
    "    num_incorrect = (targets[i] != output_idx[i]).sum()\n",
    "    print('%d incorrectly predicted\\n' % num_incorrect[0], seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, some of the predicted subsequences are correct, but temporally shifted. Some other subsequences duplicate a predicted word two or more times, resulting in either incorrect predictions for those duplicated words, or the immediately following subsequence being \"correct\", but actually incorrect because it's now in the wrong temporal position.\n",
    "\n",
    "These errors feel like they'd be caused by the attention mechanism failing to properly attend to the \"important\" parts of the sequence at each step during. Not sure if this is due to the attention mechanism failing, or if something about the encoder and/or decoder messing with it.\n",
    "\n",
    "## Visualizations\n",
    "List of modules in the model for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modules = list(model.modules())\n",
    "list(enumerate(modules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic weight heat maps to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embed_wts = np.array(modules[3].weight.data)\n",
    "embed_norm = (embed_wts - embed_wts.mean()) / (embed_wts.max() - embed_wts.min())\n",
    "plt.imshow(embed_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('dim'); plt.ylabel('word index');\n",
    "plt.title('Embedding layer')\n",
    "plt.colorbar()\n",
    "embed_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = modules[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_wts = np.array(attn.attention.weight.data)\n",
    "attn_norm = (attn_wts - attn_wts.mean()) / (attn_wts.max() - attn_wts.min())\n",
    "plt.imshow(attn_norm, aspect = 'auto', cmap = 'jet')\n",
    "plt.xlabel('d_input+d_state+d_output'); plt.ylabel('d_output')\n",
    "plt.title('Output attention sublayer (in attention mechanism)')\n",
    "plt.colorbar()\n",
    "attn_wts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence attention visualization by mapping the alignment weights (in the attention mechanism) at each step of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = nb//cols\n",
    "fig, axs = plt.subplots(rows, cols, figsize = (30, 20))\n",
    "for b in range(nb):\n",
    "    wts = attn.attn_wts[:,b,:]\n",
    "    wts_mean = wts.mean()\n",
    "    wts_max = wts.max()\n",
    "    wts_min = wts.min()\n",
    "    norm = (wts - wts_mean) / (wts_max - wts_min)\n",
    "    r = b // cols\n",
    "    c = b % cols\n",
    "    ax = axs[r, c]\n",
    "    im = ax.imshow(wts, aspect = 'auto', cmap = 'jet')\n",
    "    # Fix labels\n",
    "    xlabels = list(targets[b].data)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_xlabel('Targets')\n",
    "    ylabels = list(data[:,b].data)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_ylabel('Inputs')\n",
    "    ax.set_title('Example %d' % b)\n",
    "    fig.colorbar(im, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if the fact that the encoder and decoder both are 2-stacks of LSTMs is causing the odd shapes of these attention heatmaps. Since the later layers of an LSTM stack tend to encode longer-term temporal dependencies, feeding the attention mechanism the outputs to the last layer of the encoder stack might be causing the attention mechanism to look at longer-term dependencies and ignoring shorter-term ones. Need to test out a model that has skip connections from the previous layer's output to the next layer's output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
